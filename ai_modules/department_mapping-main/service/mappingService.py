import os
import re
import pickle
from typing import List, Dict, Any, Optional

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import logging
logger = logging.getLogger(__name__)

class MappingService:
    def __init__(self):
        """í•™ê³¼ëª… ë§¤í•‘ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.departments_data: List[Dict[str, Any]] = []
        self.embeddings: Optional[np.ndarray] = None
        self.keyword_mapping: Dict[str, str] = {}
        self._name_index: Dict[str, Dict[str, Any]] = {}
        self.load_embedding_data()
        self.setup_keyword_mapping()
        self._build_name_index()
    
    def load_embedding_data(self):
        """ìž„ë² ë”© ë°ì´í„° ë¡œë“œ"""
        try:
            pkl_path = "/app/data/goal_Dataset.pkl"
            if os.path.exists(pkl_path):
                with open(pkl_path, 'rb') as f:
                    data = pickle.load(f)
                self.embeddings = data.get('embeddings')
                self.departments_data = data.get('lookup_index', []) or []
                logger.info(f"âœ… ìž„ë² ë”© ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.departments_data)}ê°œ í•™ê³¼")
            else:
                logger.warning(f"âŒ ìž„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
                self.embeddings = None
                self.departments_data = []
        except Exception as e:
            logger.error(f"âŒ ìž„ë² ë”© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embeddings = None
            self.departments_data = []
    
    def setup_keyword_mapping(self):
        """í‚¤ì›Œë“œ ë§¤í•‘ ì„¤ì •(ì†Œë¬¸ìž/ê³µë°±ì œê±° í‚¤ ê¸°ì¤€)"""
        mapping = {
            # ì»´í“¨í„° ê´€ë ¨ - ì‹¤ì œ í•™ê³¼ëª…ì— ë§žì¶° ìˆ˜ì •
            "ì»´ê³µ": "ì»´í“¨í„°ì¸ê³µì§€ëŠ¥í•™ë¶€",
            "ì»´í“¨í„°ê³µí•™ê³¼": "ì»´í“¨í„°ì¸ê³µì§€ëŠ¥í•™ë¶€",
            "ì»´í“¨í„°ê³µí•™": "ì»´í“¨í„°ì¸ê³µì§€ëŠ¥í•™ë¶€",
            "ì»´í“¨í„°": "ì»´í“¨í„°ì¸ê³µì§€ëŠ¥í•™ë¶€",
            "ì»´í“¨í„°í•™ê³¼": "ì»´í“¨í„°ì¸ê³µì§€ëŠ¥í•™ë¶€",
            "ai": "ì»´í“¨í„°ì¸ê³µì§€ëŠ¥í•™ë¶€",
            "ì¸ê³µì§€ëŠ¥": "ì»´í“¨í„°ì¸ê³µì§€ëŠ¥í•™ë¶€",
            
            # ì „ìžê³µí•™ ê´€ë ¨
            "ì „ìžê³µí•™ê³¼": "ì „ìžê³µí•™ë¶€",
            "ì „ìžê³µí•™": "ì „ìžê³µí•™ë¶€",
            "ì „ìž": "ì „ìžê³µí•™ë¶€",
            "ì „ì „": "ì „ìžê³µí•™ë¶€",
            
            # ê¸°ê³„ê³µí•™ ê´€ë ¨
            "ê¸°ê³„ê³µí•™ê³¼": "ê¸°ê³„ì‹œìŠ¤í…œê³µí•™ë¶€",
            "ê¸°ê³„ê³µí•™": "ê¸°ê³„ì‹œìŠ¤í…œê³µí•™ë¶€",
            "ê¸°ê³„": "ê¸°ê³„ì‹œìŠ¤í…œê³µí•™ë¶€",
            "ê¸°ê³„ì„¤ê³„": "ê¸°ê³„ì„¤ê³„ê³µí•™ë¶€",
            
            # ê²½ì˜/ê²½ì œ ê´€ë ¨
            "ê²½ì˜í•™ê³¼": "ê²½ì˜í•™ê³¼",
            "ê²½ì˜í•™": "ê²½ì˜í•™ê³¼",
            "ê²½ì œí•™ê³¼": "ê²½ì œí•™ë¶€",
            "ê²½ì œí•™": "ê²½ì œí•™ë¶€",
            
            # ê±´ì¶• ê´€ë ¨
            "ê±´ì¶•ê³µí•™ê³¼": "ê±´ì¶•ê³µí•™ê³¼",
            "ê±´ì¶•": "ê±´ì¶•ê³µí•™ê³¼",
            
            # ë¬¸í—Œì •ë³´í•™ê³¼ ê´€ë ¨
            "ë¬¸ì •ê³¼": "ë¬¸í—Œì •ë³´í•™ê³¼",
            "ë¬¸í—Œì •ë³´í•™ê³¼": "ë¬¸í—Œì •ë³´í•™ê³¼",
            "ë¬¸í—Œì •ë³´í•™": "ë¬¸í—Œì •ë³´í•™ê³¼",
            "ë¬¸í—Œì •ë³´": "ë¬¸í—Œì •ë³´í•™ê³¼",
            "ë„ì„œê´€í•™": "ë¬¸í—Œì •ë³´í•™ê³¼",
            "ì •ë³´í•™": "ë¬¸í—Œì •ë³´í•™ê³¼",

            # ê¸°íƒ€
            "ê³ ë¶„ìžë‚˜ë…¸ê³µí•™ê³¼": "ê³ ë¶„ìžë‚˜ë…¸ê³µí•™ê³¼",
            "ê³ ë¶„ìž": "ê³ ë¶„ìžë‚˜ë…¸ê³µí•™ê³¼",
            "ë‚˜ë…¸": "ê³ ë¶„ìžë‚˜ë…¸ê³µí•™ê³¼",
        }
        self.keyword_mapping = {self._norm_key(k): v for k, v in mapping.items()}

    def _build_name_index(self):
        self._name_index = {}
        for dept in self.departments_data or []:
            name = dept.get("department_name")
            if name:
                self._name_index[name] = dept
    
    def find_similar_departments(self, query: str, top_k: int = 3) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ í•™ê³¼ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰í•  í•™ê³¼ëª… ë˜ëŠ” ì „ì²´ ë¬¸ìž¥ (ì˜ˆ: "ì»´ê³µ", "ì»´ê³µì—ì„œ ì¸ê³µì§€ëŠ¥ ìˆ˜ì—…ì„ ì°¾ì•„ì¤˜")
            top_k: ë°˜í™˜í•  ìƒìœ„ kê°œ ê²°ê³¼

        Returns:
            ìœ ì‚¬í•œ í•™ê³¼ë“¤ì˜ ì •ë³´ì™€ ì‹ ë¢°ë„ ì ìˆ˜
        """
        logger.info(f"ðŸ« DEPT MAPPING START (í•™ê³¼ ë§¤í•‘ ì‹œìž‘)")
        logger.info(f"  ì˜ë¯¸: ì‚¬ìš©ìž ìž…ë ¥ì—ì„œ í•™ê³¼ëª…ì„ ì¶”ì¶œí•˜ì—¬ í‘œì¤€ í•™ê³¼ëª…ìœ¼ë¡œ ì •ê·œí™”")
        logger.info(f"  ìž…ë ¥ ì¿¼ë¦¬: '{query}'")
        logger.info(f"  ìš”ì²­ ê²°ê³¼ ìˆ˜: ìƒìœ„ {top_k}ê°œ")

        # ë¬¸ìž¥ì—ì„œ í•™ê³¼ëª… í‚¤ì›Œë“œ ì¶”ì¶œ
        extracted_keywords = self._extract_department_keywords(query)
        logger.info(f"  ðŸ” ì¶”ì¶œëœ í•™ê³¼ í‚¤ì›Œë“œ: {extracted_keywords}")

        if not self.departments_data:
            logger.warning("  âš ï¸ í•™ê³¼ ë°ì´í„° ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {"departments": [], "scores": []}
        
        try:
            # 1. ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ë¡œ í‚¤ì›Œë“œ ë§¤í•‘ ì‹œë„
            for keyword in extracted_keywords:
                normalized_keyword = self._norm_key(keyword)
                logger.info(f"  ðŸ”„ í‚¤ì›Œë“œ ë§¤í•‘ ì‹œë„: '{keyword}' â†’ '{normalized_keyword}'")

                if normalized_keyword in self.keyword_mapping:
                    target_dept = self.keyword_mapping[normalized_keyword]
                    logger.info(f"  âœ… í‚¤ì›Œë“œ ë§¤í•‘ ì„±ê³µ: '{normalized_keyword}' â†’ '{target_dept}'")
                    hit = self.get_department_by_name(target_dept)
                    if hit:
                        result = {
                            "departments": [self._format_dept(hit)],
                            "scores": [0.95]
                        }
                        logger.info(f"  ðŸ“Š í‚¤ì›Œë“œ ë§¤í•‘ ê²°ê³¼: {hit.get('department_name', 'Unknown')} (ì‹ ë¢°ë„: 0.95)")
                        return result
                    else:
                        logger.warning(f"  âš ï¸ ë§¤í•‘ëœ í•™ê³¼ '{target_dept}' ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            logger.info(f"  âž¡ï¸ ëª¨ë“  í‚¤ì›Œë“œ ë§¤í•‘ ì‹¤íŒ¨ - ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
            
            # 2. ë¶€ë¶„ ë¬¸ìžì—´ ë§¤ì¹­
            logger.info(f"  ðŸ” ë¶€ë¶„ ë¬¸ìžì—´ ë§¤ì¹­ ì‹œìž‘ ({len(self.departments_data)}ê°œ í•™ê³¼ ëŒ€ìƒ)")
            departments = []
            scores = []

            for dept in self.departments_data:
                dept_name = dept["department_name"].lower()
                query_lower = query.lower()

                # ì •í™•í•œ ë§¤ì¹­
                if query_lower in dept_name or dept_name in query_lower:
                    departments.append(self._format_dept(dept))
                    scores.append(0.9)
                    logger.debug(f"    ì •í™• ë§¤ì¹­: {dept['department_name']} (ì ìˆ˜: 0.9)")
                # í‚¤ì›Œë“œ í¬í•¨ ë§¤ì¹­
                elif any(keyword in dept_name for keyword in query_lower.split()):
                    departments.append(self._format_dept(dept))
                    scores.append(0.7)
                    logger.debug(f"    í‚¤ì›Œë“œ ë§¤ì¹­: {dept['department_name']} (ì ìˆ˜: 0.7)")

            # ìƒìœ„ kê°œ ë°˜í™˜
            if departments:
                sorted_results = sorted(zip(departments, scores), key=lambda x: x[1], reverse=True)
                top_results = sorted_results[:top_k]
                result = {
                    "departments": [r[0] for r in top_results],
                    "scores": [r[1] for r in top_results]
                }
                logger.info(f"  ðŸ“Š ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼: {len(top_results)}ê°œ í›„ë³´ ë°œê²¬")
                for i, (dept, score) in enumerate(top_results, 1):
                    logger.info(f"    í›„ë³´ {i}: {dept.get('department_name', 'Unknown')} (ì‹ ë¢°ë„: {score:.3f})")
                return result

            logger.info(f"  âŒ ë§¤ì¹­ ê²°ê³¼ ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {"departments": [], "scores": []}
            
        except Exception as e:
            logger.error(f"  âŒ í•™ê³¼ ë§¤í•‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"departments": [], "scores": []}
    
    def get_all_departments(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  í•™ê³¼ ëª©ë¡ ë°˜í™˜"""
        if not self.departments_data:
            return []
        
        return [
            {
                "department_id": dept["department_id"],
                "department_name": dept["department_name"],
                "description": dept["text"][:100] + "..." if len(dept["text"]) > 100 else dept["text"]
            }
            for dept in self.departments_data
        ]
    
    def normalize_department_name(self, name: str) -> str:
        """
        í•™ê³¼ëª… ì •ê·œí™”
        - ê³µë°± ì œê±°
        - ì ‘ë¯¸ì–´ í†µì¼ (ê³¼/í•™ë¶€)
        - ë™ì˜ì–´ ë§¤í•‘
        """
        # ê³µë°± ì œê±°
        normalized = name.replace(" ", "")
        
        # ë™ì˜ì–´/ë³„ì¹­ ë§¤í•‘(í‚¤ì›Œë“œ ë§¤í•‘ê³¼ ë™ì¼ ê·œì¹™ ì‚¬ìš©)
        mapped = self.keyword_mapping.get(self._norm_key(normalized))
        return mapped or normalized
    
    def get_department_by_name(self, name: str) -> Dict[str, Any]:
        """ì •í™•í•œ í•™ê³¼ëª…ìœ¼ë¡œ í•™ê³¼ ì •ë³´ ì¡°íšŒ"""
        normalized_name = self.normalize_department_name(name)
        logger.info(f"  ðŸ” í•™ê³¼ëª… ì¡°íšŒ: '{name}' â†’ ì •ê·œí™”: '{normalized_name}'")

        result = self._name_index.get(normalized_name)
        if result:
            logger.info(f"  âœ… í•™ê³¼ ì •ë³´ ì°¾ìŒ: {result.get('department_name')}")
        else:
            logger.warning(f"  âŒ í•™ê³¼ ì •ë³´ ì°¾ì§€ ëª»í•¨. ì¸ë±ìŠ¤ í‚¤ë“¤: {list(self._name_index.keys())[:5]}...")

        return result

    def _extract_department_keywords(self, query: str) -> List[str]:
        """ë¬¸ìž¥ì—ì„œ í•™ê³¼ëª… ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        query_lower = query.lower()

        # setup_keyword_mapping()ì—ì„œ ì„¤ì •í•œ ë§¤í•‘ì„ ì—­ìœ¼ë¡œ í™œìš©
        # self.keyword_mappingì˜ í‚¤ë“¤ì„ denormalizeí•´ì„œ ì›ë³¸ í‚¤ì›Œë“œë“¤ ë³µì›
        original_keys = [
            "ì»´ê³µ", "ì»´í“¨í„°ê³µí•™ê³¼", "ì»´í“¨í„°ê³µí•™", "ì»´í“¨í„°", "ì»´í“¨í„°í•™ê³¼", "ai", "ì¸ê³µì§€ëŠ¥",
            "ì „ìžê³µí•™ê³¼", "ì „ìžê³µí•™", "ì „ìž", "ì „ì „",
            "ê¸°ê³„ê³µí•™ê³¼", "ê¸°ê³„ê³µí•™", "ê¸°ê³„", "ê¸°ê³„ì„¤ê³„",
            "ê²½ì˜í•™ê³¼", "ê²½ì˜í•™", "ê²½ì œí•™ê³¼", "ê²½ì œí•™",
            "ê±´ì¶•ê³µí•™ê³¼", "ê±´ì¶•",
            "ë¬¸ì •ê³¼", "ë¬¸í—Œì •ë³´í•™ê³¼", "ë¬¸í—Œì •ë³´í•™", "ë¬¸í—Œì •ë³´", "ë„ì„œê´€í•™", "ì •ë³´í•™",
            "ê³ ë¶„ìžë‚˜ë…¸ê³µí•™ê³¼", "ê³ ë¶„ìž", "ë‚˜ë…¸"
        ]

        # ë¬¸ìž¥ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        for key in original_keys:
            if key.lower() in query_lower and key not in keywords:
                keywords.append(key)
                logger.debug(f"    í‚¤ì›Œë“œ ë°œê²¬: '{key}' in '{query}'")

        return keywords if keywords else [query]  # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ë°˜í™˜

    # ===== ë‚´ë¶€ ìœ í‹¸ =====
    def _norm_key(self, s: str) -> str:
        return (s or '').replace(' ', '').lower()

    def _format_dept(self, dept: Dict[str, Any]) -> Dict[str, Any]:
        text = dept.get('text', '') or ''
        return {
            "department_id": dept.get("department_id"),
            "department_name": dept.get("department_name"),
            "description": text[:200] + ("..." if len(text) > 200 else "")
        }
