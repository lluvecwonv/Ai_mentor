"""
AI ë©˜í†  ì—ì´ì „íŠ¸ ì»¨íŠ¸ë¡¤ëŸ¬ - ë¦¬íŒ©í† ë§ëœ ë²„ì „
"""

from fastapi import APIRouter
from fastapi.responses import JSONResponse, StreamingResponse
import logging
import json

from service.core.mentor_service import HybridMentorService
import os
from models.validation import RequestBody, ChainRequest, AgentRequest, ErrorResponse
from utils.response_formatters import strip_markdown, format_sse
from utils.course_parser import parse_course_sections_with_preamble
from utils.graph_generator import generate_graph_base64
from exceptions import AIMentorException, ValidationError

router = APIRouter()

# LangGraph ì „ìš© ëª¨ë“œë¡œ ê°„ì†Œí™”
hybrid_service = HybridMentorService(use_unified_langgraph=True)
logger = logging.getLogger(__name__)


@router.post("/agent", response_model=dict)
async def agent_api(request_body: RequestBody):
    """ë©”ì¸ ì—ì´ì „íŠ¸ API (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""
    session_id = request_body.session_id or "default"
    stream = getattr(request_body, 'stream', False)
    logger.info(f"ğŸš€ /agent endpoint called for session: {session_id}, stream: {stream}")

    # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì¸ ê²½ìš°
    if stream:
        return await _handle_streaming_request(request_body, session_id)

    # ì¼ë°˜ ìš”ì²­ ì²˜ë¦¬
    try:
        # ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ
        user_message = ""
        messages = [{"role": msg.role, "content": msg.content} for msg in request_body.messages]

        for msg in messages:
            if msg.get("role") == "user":
                user_message = msg.get("content", "")

        # AI ë©˜í†  ì„œë¹„ìŠ¤ ì‹¤í–‰
        history = await hybrid_service.run_agent(user_message, session_id)

        # ì‘ë‹µ ì²˜ë¦¬
        content = _process_response(history, request_body)

        return JSONResponse({
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": content},
                "finish_reason": "stop"
            }]
        })

    except ValidationError as e:
        logger.error(f"ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜: {e}")
        return _error_response(400, "VALIDATION_ERROR", str(e))

    except AIMentorException as e:
        logger.error(f"AI ë©˜í†  ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        return _error_response(500, "SERVICE_ERROR", str(e))

    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return _error_response(500, "INTERNAL_ERROR", str(e))


async def _handle_streaming_request(request_body: RequestBody, session_id: str):
    """ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì²˜ë¦¬"""
    logger.info(f"ğŸŒŠ Streaming request for session: {session_id}")

    async def generate_stream():
        try:
            # ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ
            user_message = ""
            messages = [{"role": msg.role, "content": msg.content} for msg in request_body.messages]

            for msg in messages:
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")

            # ì¦‰ì‹œ "ìƒê°ì¤‘ì…ë‹ˆë‹¤" ë©”ì‹œì§€ ì „ì†¡
            thinking_chunk = {
                "choices": [{
                    "index": 0,
                    "delta": {"content": "ğŸ¤” ìƒê°ì¤‘ì…ë‹ˆë‹¤..."},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(thinking_chunk, ensure_ascii=False)}\n\n"

            # ê³„íš ìˆ˜ë¦½ ë©”ì‹œì§€
            planning_chunk = {
                "choices": [{
                    "index": 0,
                    "delta": {"content": "\n\nğŸ“‹ ë¶„ì„í•˜ê³  ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(planning_chunk, ensure_ascii=False)}\n\n"

            # AI ë©˜í†  ì„œë¹„ìŠ¤ ì‹¤í–‰
            history = await hybrid_service.run_agent(user_message, session_id)

            # ì‘ë‹µ ì²˜ë¦¬
            content = _process_response(history, request_body)

            # ì™„ë£Œ ë©”ì‹œì§€
            ready_chunk = {
                "choices": [{
                    "index": 0,
                    "delta": {"content": "\n\nâœ… ì²˜ë¦¬ ì™„ë£Œ! ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(ready_chunk, ensure_ascii=False)}\n\n"

            # ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ì‘ë‹µ ì „ì†¡ (ë‹¨ì–´ë³„ë¡œ ë¶„í• )
            words = content.split()
            for i, word in enumerate(words):
                chunk_data = {
                    "choices": [{
                        "index": 0,
                        "delta": {"content": word + " "},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

                # ë§ˆì§€ë§‰ ì²­í¬ì—ëŠ” finish_reason ì¶”ê°€
                if i == len(words) - 1:
                    final_chunk = {
                        "choices": [{
                            "index": 0,
                            "delta": {},
                            "finish_reason": "stop"
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            error_chunk = {
                "error": {
                    "message": str(e),
                    "type": "internal_error"
                }
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


def _process_response(history: dict, request_body: RequestBody) -> str:
    """ì‘ë‹µ ì²˜ë¦¬ ë¡œì§"""
    # OpenAI í˜¸í™˜ ì‘ë‹µ ì²˜ë¦¬
    if "choices" in history and len(history["choices"]) > 0:
        content = history["choices"][0]["message"]["content"]
        step = {"tool_name": "LANGGRAPH_RESPONSE"}
    else:
        step = history.get("steps", [{}])[-1] if "steps" in history else {}
        content = step.get("tool_response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì»¤ë¦¬í˜ëŸ¼ ê·¸ë˜í”„ ì²˜ë¦¬
    if step.get("tool_name") == "CURRICULUM_RECOMMEND":
        content = _process_curriculum_graph(content)

    # í¬ë§· ì²˜ë¦¬
    if request_body.format == "plain" or step.get("tool_name") == "JBNU_SQL":
        content = strip_markdown(content)

    return content


def _process_curriculum_graph(raw_content: str) -> str:
    """ì»¤ë¦¬í˜ëŸ¼ ê·¸ë˜í”„ ì²˜ë¦¬"""
    try:
        parsed = parse_course_sections_with_preamble(raw_content)
        preamble = parsed.pop("preamble", "")
        graph_b64 = generate_graph_base64(parsed)
        return f"{preamble}\n\n![Curriculum Graph](data:image/png;base64,{graph_b64})"
    except Exception as e:
        logger.error(f"ì»¤ë¦¬í˜ëŸ¼ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
        return raw_content  # ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜


def _error_response(status_code: int, error_code: str, message: str) -> JSONResponse:
    """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
    return JSONResponse(
        status_code=status_code,
        content=ErrorResponse(
            error=message,
            error_code=error_code
        ).model_dump()
    )


# === í—¬ìŠ¤ì²´í¬ ===
@router.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    try:
        health_status = hybrid_service.get_health_status()
        status_code = 200 if health_status["status"] == "healthy" else 503
        return JSONResponse(status_code=status_code, content=health_status)
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        return _error_response(503, "HEALTH_CHECK_ERROR", str(e))


# === ì„¸ì…˜ ê´€ë¦¬ ===
@router.get("/session/{session_id}/summary")
async def get_session_summary(session_id: str):
    """ì„¸ì…˜ ìš”ì•½ ì¡°íšŒ"""
    try:
        stats = hybrid_service.get_session_info(session_id)
        session_info = stats.get('session_info', {})
        summary = f"ëŒ€í™” ìš”ì•½: {session_info.get('total_messages', 0)}ê°œ ë©”ì‹œì§€, ì£¼ì œ: {session_info.get('current_topic', 'None')}"
        return JSONResponse({
            "session_id": session_id,
            "summary": summary,
            "stats": stats
        })
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return _error_response(500, "SESSION_SUMMARY_ERROR", str(e))


@router.delete("/session/{session_id}")
async def clear_session(session_id: str):
    """ì„¸ì…˜ ì´ˆê¸°í™”"""
    try:
        success = hybrid_service.clear_session_history(session_id)
        if success:
            return JSONResponse({
                "session_id": session_id,
                "message": "Session history cleared successfully"
            })
        else:
            return _error_response(500, "SESSION_CLEAR_ERROR", "ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨")
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return _error_response(500, "SESSION_CLEAR_ERROR", str(e))


# === LangChain ê¸°ëŠ¥ ===
@router.post("/langchain/chain")
async def run_langchain_chain(request: ChainRequest):
    """LangChain ì²´ì¸ ì‹¤í–‰"""
    try:
        logger.info(f"LangChain ì²´ì¸ ì‹¤í–‰: {request.chain_type} - {request.user_input}")

        # ì²´ì¸ íƒ€ì…ë³„ ì²˜ë¦¬
        if request.chain_type == "basic":
            result = await hybrid_service.llm_handler.run_chain("basic", request.user_input)
        elif request.chain_type == "context":
            result = await hybrid_service.llm_handler.run_chain("context", request.user_input, context=request.context)
        elif request.chain_type == "analysis":
            result = await hybrid_service.llm_handler.run_chain("analysis", request.user_input)
        else:
            return _error_response(400, "INVALID_CHAIN_TYPE", f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì²´ì¸ íƒ€ì…: {request.chain_type}")

        return JSONResponse({
            "chain_type": request.chain_type,
            "result": result,
            "session_id": request.session_id
        })

    except Exception as e:
        logger.error(f"LangChain ì²´ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return _error_response(500, "CHAIN_EXECUTION_ERROR", str(e))


@router.post("/langchain/agent")
async def run_langchain_agent(request: AgentRequest):
    """LangChain ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    try:
        logger.info(f"LangChain ì—ì´ì „íŠ¸ ì‹¤í–‰: {request.user_input}")
        result = await hybrid_service.llm_handler.run_agent(request.user_input)

        return JSONResponse({
            "agent_result": result,
            "session_id": request.session_id
        })

    except Exception as e:
        logger.error(f"LangChain ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return _error_response(500, "AGENT_EXECUTION_ERROR", str(e))


# === ìŠ¤íŠ¸ë¦¬ë° API ===
@router.post("/agent-stream")
async def agent_streaming_api(request_body: RequestBody):
    """ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì—ì´ì „íŠ¸ API"""
    session_id = request_body.session_id or "default"
    logger.info(f"ğŸŒŠ /v2/agent endpoint called for session: {session_id}")

    # stream íŒŒë¼ë¯¸í„° í™•ì¸
    stream = getattr(request_body, 'stream', False)
    logger.info(f"Stream parameter: {stream}, request_body.stream: {request_body.stream}")

    if not stream:
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ agent_api í˜¸ì¶œ
        return await agent_api(request_body)

    async def generate_stream():
        try:
            # ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ
            user_message = ""
            messages = [{"role": msg.role, "content": msg.content} for msg in request_body.messages]

            for msg in messages:
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")

            # AI ë©˜í†  ì„œë¹„ìŠ¤ ì‹¤í–‰
            history = await hybrid_service.run_agent(user_message, session_id)

            # ì‘ë‹µ ì²˜ë¦¬
            content = _process_response(history, request_body)

            # ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ì‘ë‹µ ì „ì†¡ (ë‹¨ì–´ë³„ë¡œ ë¶„í• )
            words = content.split()
            for i, word in enumerate(words):
                chunk_data = {
                    "choices": [{
                        "index": 0,
                        "delta": {"content": word + " "},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

                # ë§ˆì§€ë§‰ ì²­í¬ì—ëŠ” finish_reason ì¶”ê°€
                if i == len(words) - 1:
                    final_chunk = {
                        "choices": [{
                            "index": 0,
                            "delta": {},
                            "finish_reason": "stop"
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            error_chunk = {
                "error": {
                    "message": str(e),
                    "type": "internal_error"
                }
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )
