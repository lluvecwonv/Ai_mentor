from fastapi import APIRouter
from fastapi.responses import JSONResponse, StreamingResponse
import logging
import json

from service.core.mentor_service import HybridMentorService
from models.validation import RequestBody, ErrorResponse
from utils.controller_utils import strip_markdown, process_curriculum_graph
from exceptions import AIMentorException, ValidationError

router = APIRouter()

# LangGraph ì „ìš© ëª¨ë“œë¡œ ê°„ì†Œí™”
hybrid_service = HybridMentorService(use_unified_langgraph=True)
logger = logging.getLogger(__name__)


@router.post("/agent", response_model=dict)
async def agent_api(request_body: RequestBody):
    """ë©”ì¸ ì—ì´ì „íŠ¸ API (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""
    session_id = request_body.session_id or "default"
    stream = getattr(request_body, 'stream', False)
    logger.info(f"ğŸš€ /agent endpoint called for session: {session_id}, stream: {stream}")

    try:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = _extract_user_message(request_body.messages)

        # AI ë©˜í†  ì„œë¹„ìŠ¤ ì‹¤í–‰
        history = await hybrid_service.run_agent(user_message, session_id)

        # ì‘ë‹µ ì²˜ë¦¬
        content = _process_response(history, request_body)

        # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì¸ ê²½ìš°
        if stream:
            return _create_streaming_response(content)

        # ì¼ë°˜ ì‘ë‹µ
        return JSONResponse({
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": content},
                "finish_reason": "stop"
            }]
        })

    except ValidationError as e:
        logger.error(f"ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜: {e}")
        return _error_response(400, "VALIDATION_ERROR", str(e))
    except AIMentorException as e:
        logger.error(f"AI ë©˜í†  ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        return _error_response(500, "SERVICE_ERROR", str(e))
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return _error_response(500, "INTERNAL_ERROR", str(e))


def _extract_user_message(messages):
    """ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ"""
    for msg in messages:
        if msg.role == "user":
            return msg.content
    return ""


def _process_response(history: dict, request_body: RequestBody) -> str:
    """ì‘ë‹µ ì²˜ë¦¬ ë¡œì§"""
    # OpenAI í˜¸í™˜ ì‘ë‹µ ì²˜ë¦¬
    if "choices" in history and len(history["choices"]) > 0:
        content = history["choices"][0]["message"]["content"]
        step = {"tool_name": "LANGGRAPH_RESPONSE"}
    else:
        step = history.get("steps", [{}])[-1] if "steps" in history else {}
        content = step.get("tool_response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì»¤ë¦¬í˜ëŸ¼ ê·¸ë˜í”„ ì²˜ë¦¬
    if step.get("tool_name") == "CURRICULUM_RECOMMEND":
        content = process_curriculum_graph(content)

    # í¬ë§· ì²˜ë¦¬
    if request_body.format == "plain" or step.get("tool_name") == "JBNU_SQL":
        content = strip_markdown(content)

    return content


def _create_streaming_response(content: str):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    async def generate_stream():
        # ì§„í–‰ ìƒí™© ë©”ì‹œì§€ë“¤
        progress_messages = [
            "ğŸ¤” ìƒê°ì¤‘ì…ë‹ˆë‹¤...",
            "\n\nğŸ“‹ ë¶„ì„í•˜ê³  ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            "\n\nâœ… ì²˜ë¦¬ ì™„ë£Œ! ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"
        ]

        for msg in progress_messages:
            chunk = {
                "choices": [{
                    "index": 0,
                    "delta": {"content": msg},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

        # ì‹¤ì œ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
        words = content.split()
        for i, word in enumerate(words):
            chunk = {
                "choices": [{
                    "index": 0,
                    "delta": {"content": word + " "},
                    "finish_reason": "stop" if i == len(words) - 1 else None
                }]
            }
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


def _error_response(status_code: int, error_code: str, message: str) -> JSONResponse:
    """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
    return JSONResponse(
        status_code=status_code,
        content=ErrorResponse(
            error=message,
            error_code=error_code
        ).model_dump()
    )


# === ì„¸ì…˜ ê´€ë¦¬ ===
@router.get("/session/{session_id}/summary")
async def get_session_summary(session_id: str):
    """ì„¸ì…˜ ìš”ì•½ ì¡°íšŒ"""
    try:
        stats = hybrid_service.get_session_info(session_id)
        session_info = stats.get('session_info', {})
        summary = f"ëŒ€í™” ìš”ì•½: {session_info.get('total_messages', 0)}ê°œ ë©”ì‹œì§€, ì£¼ì œ: {session_info.get('current_topic', 'None')}"
        return JSONResponse({
            "session_id": session_id,
            "summary": summary,
            "stats": stats
        })
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return _error_response(500, "SESSION_SUMMARY_ERROR", str(e))


@router.delete("/session/{session_id}")
async def clear_session(session_id: str):
    """ì„¸ì…˜ ì´ˆê¸°í™”"""
    try:
        success = hybrid_service.clear_session_history(session_id)
        if success:
            return JSONResponse({
                "session_id": session_id,
                "message": "Session history cleared successfully"
            })
        else:
            return _error_response(500, "SESSION_CLEAR_ERROR", "ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨")
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return _error_response(500, "SESSION_CLEAR_ERROR", str(e))


