"""
í†µí•© LangGraph ì„œë¹„ìŠ¤ ë˜í¼
Light/Medium/Heavy ëª¨ë“  ë³µì¡ë„ë¥¼ ì²˜ë¦¬í•˜ëŠ” í†µí•© ì„œë¹„ìŠ¤
"""

from __future__ import annotations
import logging
import asyncio
from typing import Dict, Any, AsyncGenerator, Optional, List
from datetime import datetime

from .unified_langgraph_app import create_unified_langgraph_app, UnifiedLangGraphApp
from .langgraph_state import UnifiedMentorState
from service.core.memory import ConversationMemory
from exceptions import AIMentorException

logger = logging.getLogger(__name__)

class UnifiedLangGraphService:
    """
    í†µí•© LangGraph ì„œë¹„ìŠ¤ ë˜í¼
    Light/Medium/Heavy ë³µì¡ë„ë¥¼ ë‹¨ì¼ ê·¸ë˜í”„ë¡œ ì²˜ë¦¬
    """

    def __init__(self, conversation_memory: ConversationMemory = None):
        """
        í†µí•© LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            conversation_memory: ê¸°ì¡´ ëŒ€í™” ë©”ëª¨ë¦¬ì™€ ì—°ë™
        """
        logger.info("ğŸš€ í†µí•© LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")

        try:
            self.conversation_memory = conversation_memory
            self.app = create_unified_langgraph_app(conversation_memory)
            self._is_healthy = True
            self._stats = {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "light_requests": 0,
                "medium_requests": 0,
                "heavy_requests": 0,
                "avg_response_time": 0.0,
                "start_time": datetime.now()
            }

            logger.info("âœ… í†µí•© LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ í†µí•© LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._is_healthy = False
            raise AIMentorException(f"í†µí•© LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ==================== ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œë“¤ ====================

    async def run_agent(
        self,
        user_message: str,
        session_id: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        í†µí•© ì—ì´ì „íŠ¸ ì‹¤í–‰ (ëª¨ë“  ë³µì¡ë„ ì²˜ë¦¬)

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°

        Returns:
            OpenAI í˜¸í™˜ ì‘ë‹µ í˜•ì‹
        """
        start_time = datetime.now()
        self._stats["total_requests"] += 1

        logger.info(f"ğŸ¤– í†µí•© LangGraph ì—ì´ì „íŠ¸ ì‹¤í–‰: session={session_id}")
        logger.info(f"ğŸ“ ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}")

        try:
            # í†µí•© LangGraph ì‹¤í–‰
            final_state = await self.app.process_query(
                user_message=user_message,
                session_id=session_id
            )

            # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            duration = (datetime.now() - start_time).total_seconds()

            # ë³µì¡ë„ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            complexity = final_state.get("hybrid_info", {}).get("query_analysis", {}).get("complexity", "medium")
            self._update_stats(duration, complexity, success=True)

            # OpenAI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            response = self._convert_to_openai_format(final_state, duration)

            # ëŒ€í™” ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë™ì¼)
            if self.conversation_memory:
                await self._update_conversation_memory(
                    session_id, user_message, response, final_state
                )

            logger.info(f"âœ… í†µí•© LangGraph ì‹¤í–‰ ì™„ë£Œ: {duration:.2f}ì´ˆ ({complexity})")
            return response

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            self._update_stats(duration, "unknown", success=False)

            logger.error(f"âŒ í†µí•© LangGraph ì‹¤í–‰ ì‹¤íŒ¨: {e}")

            # ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜
            return self._create_error_response(str(e))

    async def run_agent_streaming(
        self,
        user_message: str,
        session_id: str = "default",
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì—ì´ì „íŠ¸ ì‹¤í–‰

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°

        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ë“¤
        """
        logger.info(f"ğŸŒŠ í†µí•© LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰: session={session_id}")

        try:
            # ì‹œì‘ ë©”ì‹œì§€
            yield self._create_streaming_chunk("ğŸ¤” ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")

            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ - ë¹„ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ (ì¶”í›„ ê°œì„ )
            result = await self.app.process_query(user_message, session_id)

            # ê²°ê³¼ë¥¼ ë‹¨ì–´ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            response_text = result.get("response", "")
            words = response_text.split()

            for word in words:
                yield self._create_streaming_chunk(word + " ")
                await asyncio.sleep(0.1)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼

            # ì™„ë£Œ ë©”ì‹œì§€
            yield self._create_streaming_chunk("", finish_reason="stop")

        except Exception as e:
            logger.error(f"âŒ í†µí•© LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield self._create_error_chunk(str(e))

    # ==================== ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ====================

    def _convert_to_openai_format(
        self,
        final_state: UnifiedMentorState,
        duration: float
    ) -> Dict[str, Any]:
        """
        í†µí•© LangGraph ê²°ê³¼ë¥¼ OpenAI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            final_state: í†µí•© LangGraph ìµœì¢… ìƒíƒœ
            duration: ì‹¤í–‰ ì‹œê°„

        Returns:
            OpenAI í˜¸í™˜ ì‘ë‹µ
        """
        # ìµœì¢… ê²°ê³¼ ì¶”ì¶œ - ìƒˆë¡œìš´ ì‘ë‹µ í˜•ì‹ ì‚¬ìš©
        final_result = final_state.get("response", "")
        if not final_result:
            # ì´ì „ í˜•ì‹ë„ í™•ì¸
            final_result = final_state.get("final_result", "")
            if not final_result:
                # ë©”ì‹œì§€ì—ì„œ ë§ˆì§€ë§‰ AI ì‘ë‹µ ì°¾ê¸°
                messages = final_state.get("messages", [])
                for message in reversed(messages):
                    if hasattr(message, 'content') and message.content:
                        final_result = message.content
                        break

        if not final_result:
            final_result = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        processing_type = final_state.get("processing_type", "unknown")
        complexity = final_state.get("hybrid_info", {}).get("query_analysis", {}).get("complexity", "medium")

        # OpenAI í˜•ì‹ ì‘ë‹µ
        return {
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": final_result
                },
                "finish_reason": "stop"
            }],
            # í†µí•© ë©”íƒ€ë°ì´í„°
            "unified_langgraph_metadata": {
                "execution_time": duration,
                "complexity": complexity,
                "processing_type": processing_type,
                "steps_executed": len(final_state.get("step_times", {})),
                "retry_count": final_state.get("retry_count", 0),
                "route": final_state.get("route", "unknown"),
                "confidence": final_state.get("result_confidence", 0.0),
                "parallel_execution": complexity == "heavy"
            },
            # ê¸°ì¡´ í˜¸í™˜ì„±
            "processing_type": f"unified_{processing_type}",
            "final_result": final_result,
            "steps": [
                {
                    "tool_name": f"UNIFIED_LANGGRAPH_{complexity.upper()}",
                    "tool_response": final_result,
                    "execution_time": duration
                }
            ]
        }

    def _create_streaming_chunk(
        self,
        content: str,
        finish_reason: Optional[str] = None
    ) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìƒì„±"""
        import json

        chunk_data = {
            "choices": [{
                "index": 0,
                "delta": {"content": content} if content else {},
                "finish_reason": finish_reason
            }]
        }

        return f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

    def _create_error_chunk(self, error_message: str) -> str:
        """ì—ëŸ¬ ì²­í¬ ìƒì„±"""
        import json

        error_chunk = {
            "error": {
                "message": error_message,
                "type": "unified_langgraph_error"
            }
        }

        return f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        fallback_message = (
            "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
            "ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜ ì§ˆë¬¸ì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”."
        )

        return {
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": fallback_message
                },
                "finish_reason": "error"
            }],
            "error": {
                "message": error_message,
                "type": "unified_langgraph_error"
            },
            "processing_type": "unified_error",
            "final_result": fallback_message
        }

    async def _update_conversation_memory(
        self,
        session_id: str,
        user_message: str,
        response: Dict[str, Any],
        final_state: UnifiedMentorState
    ):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        if not self.conversation_memory:
            return

        try:
            # ê¸°ì¡´ ConversationMemory ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
            ai_response = response["choices"][0]["message"]["content"]

            # ëŒ€í™” êµí™˜ ì¶”ê°€
            self.conversation_memory.add_exchange(
                session_id=session_id,
                user_message=user_message,
                assistant_response=ai_response,
                query_analysis={
                    "execution_time": final_state.get("step_times", {}),
                    "complexity": final_state.get("complexity"),
                    "processing_type": final_state.get("processing_type"),
                    "unified_langgraph": True
                }
            )

            logger.debug(f"ğŸ’¾ í†µí•© ëŒ€í™” ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ: session={session_id}")

        except Exception as e:
            logger.error(f"âŒ í†µí•© ëŒ€í™” ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _update_stats(self, duration: float, complexity: str, success: bool):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        if success:
            self._stats["successful_requests"] += 1
        else:
            self._stats["failed_requests"] += 1

        # ë³µì¡ë„ë³„ í†µê³„
        if complexity == "light":
            self._stats["light_requests"] += 1
        elif complexity == "medium":
            self._stats["medium_requests"] += 1
        elif complexity == "heavy":
            self._stats["heavy_requests"] += 1

        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        total_successful = self._stats["successful_requests"]
        if total_successful > 0:
            current_avg = self._stats["avg_response_time"]
            self._stats["avg_response_time"] = (
                (current_avg * (total_successful - 1) + duration) / total_successful
            )

    # ==================== í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ====================

    def get_health_status(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
        uptime = (datetime.now() - self._stats["start_time"]).total_seconds()

        return {
            "status": "healthy" if self._is_healthy else "unhealthy",
            "service_type": "unified_langgraph",
            "uptime_seconds": uptime,
            "statistics": {
                **self._stats,
                "success_rate": (
                    self._stats["successful_requests"] / max(1, self._stats["total_requests"])
                )
            },
            "complexity_distribution": {
                "light": self._stats["light_requests"],
                "medium": self._stats["medium_requests"],
                "heavy": self._stats["heavy_requests"]
            },
            "components": {
                "unified_app": "healthy" if self.app else "unhealthy",
                "conversation_memory": "healthy" if self.conversation_memory else "not_configured"
            }
        }

    def get_session_info(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            return {
                "session_id": session_id,
                "service_type": "unified_langgraph",
                "session_info": {
                    "current_topic": "Unified LangGraph Session",
                    "supports_complexity": ["light", "medium", "heavy"],
                    "architecture": "single_graph"
                }
            }
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "session_id": session_id,
                "error": str(e)
            }

    def clear_session_history(self, session_id: str) -> bool:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        try:
            # ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            if self.conversation_memory:
                # ConversationMemoryì˜ ì´ˆê¸°í™” ë©”ì„œë“œ í˜¸ì¶œ
                pass

            logger.info(f"ğŸ—‘ï¸ í†µí•© ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”: {session_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ í†µí•© ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    # ==================== í†µí•© ì „ìš© ë©”ì„œë“œë“¤ ====================

    def get_graph_visualization(self) -> str:
        """ê·¸ë˜í”„ ì‹œê°í™”"""
        return self.app.get_graph_visualization()

    async def debug_execution(
        self,
        user_message: str,
        session_id: str = "default"
    ) -> Dict[str, Any]:
        """ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰"""
        final_state = await self.app.ainvoke(
            user_message=user_message,
            session_id=session_id,
            debug_mode=True
        )

        return {
            "final_state": final_state,
            "execution_trace": final_state.get("step_times", {}),
            "complexity": final_state.get("complexity"),
            "processing_type": final_state.get("processing_type"),
            "route_taken": final_state.get("route"),
            "errors_encountered": final_state.get("failed_steps", [])
        }

    def get_complexity_stats(self) -> Dict[str, Any]:
        """ë³µì¡ë„ë³„ í†µê³„"""
        total = self._stats["total_requests"]
        if total == 0:
            return {"message": "No requests processed yet"}

        return {
            "total_requests": total,
            "complexity_breakdown": {
                "light": {
                    "count": self._stats["light_requests"],
                    "percentage": (self._stats["light_requests"] / total) * 100
                },
                "medium": {
                    "count": self._stats["medium_requests"],
                    "percentage": (self._stats["medium_requests"] / total) * 100
                },
                "heavy": {
                    "count": self._stats["heavy_requests"],
                    "percentage": (self._stats["heavy_requests"] / total) * 100
                }
            }
        }


# ==================== íŒ©í† ë¦¬ í•¨ìˆ˜ ====================

def create_unified_langgraph_service(conversation_memory: ConversationMemory = None) -> UnifiedLangGraphService:
    """
    í†µí•© LangGraph ì„œë¹„ìŠ¤ ìƒì„± íŒ©í† ë¦¬

    Args:
        conversation_memory: ê¸°ì¡´ ëŒ€í™” ë©”ëª¨ë¦¬

    Returns:
        ì´ˆê¸°í™”ëœ í†µí•© LangGraph ì„œë¹„ìŠ¤
    """
    logger.info("ğŸ­ í†µí•© LangGraph ì„œë¹„ìŠ¤ íŒ©í† ë¦¬ì—ì„œ ìƒì„±")
    return UnifiedLangGraphService(conversation_memory)


# í†µí•© LangGraph ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
UNIFIED_LANGGRAPH_AVAILABLE = True