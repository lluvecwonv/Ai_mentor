"""
ê°„ì†Œí™”ëœ í•˜ì´ë¸Œë¦¬ë“œ AI Mentor ì„œë¹„ìŠ¤
í•µì‹¬ ê¸°ëŠ¥ì—ë§Œ ì§‘ì¤‘í•œ ê¹”ë”í•œ ë²„ì „
"""

import logging
import asyncio
from typing import Dict, Any, AsyncGenerator, List
from datetime import datetime

# í•µì‹¬ importsë§Œ ìœ ì§€
from config.settings import settings
from exceptions import AIMentorException, ConversationMemoryError
from service.core.memory import ConversationMemory
from service.analysis.query_analyzer import QueryAnalyzer
from handlers import VectorSearchHandler, SqlQueryHandler, DepartmentMappingHandler, CurriculumHandler
from processors import ChainProcessor
from service.analysis.result_synthesizer import ResultSynthesizer
from utils.logging import QueryLogger, set_context, clear_context, new_request_id
from utils.mentor_service import ChatModelHelper, StreamingUtils
from utils.context_builder import ContextBuilder

logger = logging.getLogger(__name__)

try:
    from utils.llm_client_langchain import LlmClientLangChain as LlmAgentService
except ImportError:
    LlmAgentService = None

# LangGraph í†µí•© - ê¸°ì¡´ LangGraph ì„œë¹„ìŠ¤ (ì œê±°ë¨)
LANGGRAPH_AVAILABLE = False
LangGraphService = None

# í†µí•© LangGraph ì„œë¹„ìŠ¤ (ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬)
try:
    from .unified_langgraph_service import create_unified_langgraph_service, UnifiedLangGraphService
    UNIFIED_LANGGRAPH_AVAILABLE = True
except ImportError as e:
    # loggerëŠ” ì•„ë˜ì—ì„œ ì •ì˜ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” print ì‚¬ìš©
    print(f"LangGraph ì„œë¹„ìŠ¤ import ì‹¤íŒ¨: {e}")
    UNIFIED_LANGGRAPH_AVAILABLE = False
    UnifiedLangGraphService = None
except Exception as e:
    print(f"LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    UNIFIED_LANGGRAPH_AVAILABLE = False
    UnifiedLangGraphService = None

class HybridMentorService:
    """ê°„ì†Œí™”ëœ AI Mentor ì„œë¹„ìŠ¤ - LangGraphì™€ ToT í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›"""

    def __init__(self, use_langgraph: bool = False, use_unified_langgraph: bool = False):
        """
        HybridMentorService ì´ˆê¸°í™”

        Args:
            use_langgraph: ê¸°ì¡´ LangGraph ì‚¬ìš© ì—¬ë¶€
            use_unified_langgraph: í†µí•© LangGraph ì‚¬ìš© ì—¬ë¶€ (Light/Medium/Heavy í†µí•©)
        """
        self.logger = logger

        # í†µí•© LangGraphê°€ ìš°ì„ ìˆœìœ„
        if use_unified_langgraph and UNIFIED_LANGGRAPH_AVAILABLE:
            self.use_unified_langgraph = True
            self.use_langgraph = False
            logger.info("ğŸ”§ í†µí•© LangGraph ëª¨ë“œë¡œ ì´ˆê¸°í™”")
        elif use_langgraph and LANGGRAPH_AVAILABLE:
            self.use_unified_langgraph = False
            self.use_langgraph = True
            logger.info("ğŸ”§ ê¸°ì¡´ LangGraph ëª¨ë“œë¡œ ì´ˆê¸°í™”")
        else:
            self.use_unified_langgraph = False
            self.use_langgraph = False
            logger.info("ğŸ”§ ToT ëª¨ë“œë¡œ ì´ˆê¸°í™”")

        if (use_unified_langgraph or use_langgraph) and not (UNIFIED_LANGGRAPH_AVAILABLE or LANGGRAPH_AVAILABLE):
            logger.warning("LangGraphê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ. ToT ëª¨ë“œë¡œ í´ë°±")

        self._initialize_components()

    def _initialize_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë§Œ ì´ˆê¸°í™”"""
        try:
            # LLM í•¸ë“¤ëŸ¬
            if LlmAgentService:
                self.llm_handler = LlmAgentService()
                logger.debug("LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                raise AIMentorException("LLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ëŒ€í™” ë©”ëª¨ë¦¬
            self.conversation_memory = ConversationMemory(
                max_history_length=settings.max_history_length,
                llm_handler=self.llm_handler
            )

            # ì¿¼ë¦¬ ë¶„ì„ê¸°
            self.query_analyzer = QueryAnalyzer(conversation_memory=self.conversation_memory)

            # í•¸ë“¤ëŸ¬ë“¤
            self.vector_handler = VectorSearchHandler()
            self.sql_handler = SqlQueryHandler()
            self.mapping_handler = DepartmentMappingHandler()
            self.curriculum_handler = CurriculumHandler()

            # ì²´ì¸ í”„ë¡œì„¸ì„œ
            self.chain_processor = ChainProcessor(
                self.query_analyzer, self.conversation_memory, self.llm_handler,
                self.vector_handler, self.sql_handler, self.mapping_handler, settings
            )

            # ê²°ê³¼ ì¢…í•©ê¸°
            self.result_synthesizer = ResultSynthesizer(self.llm_handler)

            # LangGraph ì„œë¹„ìŠ¤ë“¤ (ì˜µì…˜)
            self.langgraph_service = None
            self.unified_langgraph_service = None

            # í†µí•© LangGraph ìš°ì„  ì´ˆê¸°í™”
            if self.use_unified_langgraph:
                try:
                    self.unified_langgraph_service = create_unified_langgraph_service(self.conversation_memory)
                    logger.info("âœ… í†µí•© LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ í†µí•© LangGraph ì´ˆê¸°í™” ì‹¤íŒ¨, ToTë¡œ í´ë°±: {e}")
                    self.use_unified_langgraph = False
                    self.unified_langgraph_service = None

            # ê¸°ì¡´ LangGraph ì´ˆê¸°í™”
            elif self.use_langgraph:
                try:
                    self.langgraph_service = create_langgraph_service(self.conversation_memory)
                    logger.info("âœ… ê¸°ì¡´ LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ ê¸°ì¡´ LangGraph ì´ˆê¸°í™” ì‹¤íŒ¨, ToTë¡œ í´ë°±: {e}")
                    self.use_langgraph = False
                    self.langgraph_service = None

            # ë°±ê·¸ë¼ìš´ë“œ ì›Œë°ì—… (ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆì„ ë•Œë§Œ)
            try:
                asyncio.create_task(self._warmup_handlers())
            except RuntimeError:
                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì›Œë°ì—…ì€ ë‚˜ì¤‘ì— ì‹¤í–‰ë¨
                logger.info("ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ì–´ ì›Œë°ì—…ì„ ë‚˜ì¤‘ì— ì‹¤í–‰í•©ë‹ˆë‹¤")

            # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
            if self.use_unified_langgraph:
                execution_mode = "Unified LangGraph"
            elif self.use_langgraph:
                execution_mode = "LangGraph"
            else:
                execution_mode = "Light (LLM Direct)"

            logger.info(f"âœ… HybridMentorService ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë“œ: {execution_mode})")

        except Exception as e:
            logger.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise AIMentorException(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}") from e

    async def _warmup_handlers(self):
        """í•¸ë“¤ëŸ¬ë“¤ ë°±ê·¸ë¼ìš´ë“œ ì›Œë°ì—…"""
        try:
            await asyncio.gather(
                self.sql_handler.warmup(),
                self.vector_handler.warmup(),
                self.mapping_handler.warmup(),
                self.curriculum_handler.warmup(),
                return_exceptions=True
            )
            logger.info("âœ… ëª¨ë“  í•¸ë“¤ëŸ¬ ì›Œë°ì—… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"í•¸ë“¤ëŸ¬ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {e}")



    async def run_agent(self, user_message: str, session_id: str = "default") -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ - í†µí•© LangGraph/ê¸°ì¡´ LangGraph/ToT ì„ íƒì  ì‹¤í–‰"""
        start_time = datetime.now()
        request_id = new_request_id()
        set_context({"request_id": request_id, "session_id": session_id})

        try:
            # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
            if self.use_unified_langgraph:
                execution_mode = "Unified LangGraph"
            elif self.use_langgraph:
                execution_mode = "LangGraph"
            else:
                execution_mode = "Light (LLM Direct)"

            logger.info(f"ğŸš€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ({execution_mode}): {user_message}")

            # í†µí•© LangGraph ëª¨ë“œ (ìµœìš°ì„ )
            if self.use_unified_langgraph and self.unified_langgraph_service:
                logger.info("ğŸŒŸ í†µí•© LangGraph ëª¨ë“œë¡œ ì‹¤í–‰ - ëª¨ë“  ë³µì¡ë„ ì§€ì›")

                try:
                    result = await self.unified_langgraph_service.run_agent(
                        user_message=user_message,
                        session_id=session_id
                    )

                    duration = (datetime.now() - start_time).total_seconds()
                    logger.info(f"âœ… í†µí•© LangGraph ì‹¤í–‰ ì™„ë£Œ: {duration:.2f}ì´ˆ")
                    return result

                except Exception as e:
                    logger.error(f"âŒ í†µí•© LangGraph ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    logger.error("ğŸ”„ ToT ëª¨ë“œë¡œ í´ë°±í•˜ì—¬ ì¬ì‹œë„")
                    self.use_unified_langgraph = False  # ì„ì‹œë¡œ ë¹„í™œì„±í™”

            # ê¸°ì¡´ LangGraph ëª¨ë“œ
            elif self.use_langgraph and self.langgraph_service:
                logger.info("âš¡ LangGraph ëª¨ë“œë¡œ ì‹¤í–‰ - ì„œë¹„ìŠ¤ ê²€ì¦ ì¤‘...")

                # ëŸ°íƒ€ì„ ì„œë¹„ìŠ¤ ê²€ì¦
                service_valid = self._validate_langgraph_service()
                if not service_valid:
                    logger.warning("âš ï¸ LangGraph ì„œë¹„ìŠ¤ ê²€ì¦ ì‹¤íŒ¨ - ToT ëª¨ë“œë¡œ í´ë°±")
                    self.use_langgraph = False  # ì„ì‹œë¡œ ë¹„í™œì„±í™”
                else:
                    logger.info("âœ… LangGraph ì„œë¹„ìŠ¤ ê²€ì¦ ì„±ê³µ")
                    try:
                        result = await self.langgraph_service.run_agent(
                            user_message=user_message,
                            session_id=session_id
                        )

                        duration = (datetime.now() - start_time).total_seconds()
                        logger.info(f"âœ… LangGraph ì‹¤í–‰ ì™„ë£Œ: {duration:.2f}ì´ˆ")
                        return result

                    except Exception as e:
                        logger.error(f"âŒ LangGraph ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        logger.error("ğŸ”„ ToT ëª¨ë“œë¡œ í´ë°±í•˜ì—¬ ì¬ì‹œë„")
                        self.use_langgraph = False  # ì„ì‹œë¡œ ë¹„í™œì„±í™”

            # ê¸°ì¡´ ToT ëª¨ë“œ (í´ë°±)
            logger.info("ğŸŒ³ ToT ëª¨ë“œë¡œ ì‹¤í–‰")

            # 1. ì¿¼ë¦¬ ë¶„ì„
            query_analysis = await self.query_analyzer.analyze_query_parallel(user_message, session_id)
            complexity = query_analysis.get('complexity', 'medium')
            logger.info(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ - ë³µì¡ë„: {complexity}")

            # 2. ì²´ì¸ ì…ë ¥ ì¤€ë¹„
            chain_input = {
                "user_message": user_message,
                "session_id": session_id,
                "analysis": query_analysis,
                "conversation_memory": self.conversation_memory
            }

            # 3. ì²´ì¸ ì²˜ë¦¬
            result = await self.chain_processor.process(chain_input)
            processing_type = result.get('processing_type', 'unknown')
            logger.info(f"ğŸ”§ ì²˜ë¦¬ íƒ€ì…: {processing_type}")

            # 4. ê²°ê³¼ í•©ì„± (ì‚¬ìš©ì ì¹œí™”ì  ì‘ë‹µ ìƒì„±)
            if self._should_synthesize(processing_type):
                result = await self._synthesize_result(user_message, result, query_analysis, session_id)
                logger.info("ğŸ”„ ê²°ê³¼ í•©ì„± ì™„ë£Œ")

            # 5. ëŒ€í™” ì €ì¥
            await self._save_conversation(session_id, user_message, result, query_analysis)

            # 6. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            processing_time = (datetime.now() - start_time).total_seconds()
            result['hybrid_info'] = {
                'session_id': session_id,
                'processing_time': processing_time,
                'query_analysis': query_analysis
            }

            logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return result

        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        finally:
            clear_context()

    def _should_synthesize(self, processing_type: str) -> bool:
        """í•©ì„±ì´ í•„ìš”í•œ ì²˜ë¦¬ íƒ€ì…ì¸ì§€ í™•ì¸"""
        # llm_onlyë„ í•©ì„± í•„ìš” (ì‚¬ìš©ì ì¹œí™”ì  ì‘ë‹µ ìƒì„±)
        return processing_type not in ['cache_only']

    async def _synthesize_result(self, user_message: str, result: Dict, query_analysis: Dict, session_id: str) -> Dict:
        """ê²°ê³¼ í•©ì„±"""
        try:
            # ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ ì‘ë‹µ ìœ„ì¹˜ í¬í•¨)
            content_sources = [
                result.get('choices', [{}])[0].get('message', {}).get('content'),  # ì‹¤ì œ ì‘ë‹µ ìœ„ì¹˜
                result.get('final_result'),
                result.get('contexts', {}).get('sql'),
                result.get('contexts', {}).get('vector'),
                result.get('contexts', {}).get('mapping')
            ]

            # ë””ë²„ê¹…: ì–´ë–¤ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ëŠ”ì§€ í™•ì¸
            logger.info(f"ğŸ” ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰ ê²°ê³¼:")
            for i, source in enumerate(content_sources):
                source_name = ['choices.content', 'final_result', 'contexts.sql', 'contexts.vector', 'contexts.mapping'][i]
                logger.info(f"  {source_name}: {str(source) if source else 'None'}")

            found_results = next((source for source in content_sources if source), "")

            if found_results:
                logger.info(f"ğŸ“ í•©ì„±í•  ì›ë³¸ ê²°ê³¼: {str(found_results)}")
                # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸
                conversation_history = self._get_conversation_history(session_id)

                # LLM í•©ì„±
                synthesized = await self.result_synthesizer.synthesize_with_llm(
                    user_message=user_message,
                    found_results=found_results,
                    processing_type=result.get('processing_type'),
                    query_analysis=query_analysis,
                    conversation_history=conversation_history
                )

                if synthesized:
                    result['choices'][0]['message']['content'] = synthesized
                    logger.info(f"ğŸ”„ ê²°ê³¼ í•©ì„± ì™„ë£Œ: {len(synthesized)}ì")
                    logger.info(f"ğŸ“„ í•©ì„±ëœ ë‚´ìš©: {synthesized}")
                else:
                    logger.warning("âš ï¸ í•©ì„± ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
            else:
                logger.warning("âš ï¸ í•©ì„±í•  ì›ë³¸ ê²°ê³¼ê°€ ì—†ìŒ")

            return result
        except Exception as e:
            logger.warning(f"ê²°ê³¼ í•©ì„± ì‹¤íŒ¨: {e}")
            return result

    def _get_conversation_history(self, session_id: str, limit: int = 5) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¬¸ìì—´ ìƒì„±"""
        try:
            exchanges = self.conversation_memory.get_recent_exchanges(session_id, limit=limit)
            if not exchanges:
                return ""

            history_parts = []
            for i, exchange in enumerate(exchanges[-limit:], 1):  # ìµœì‹  limitê°œë§Œ ì‚¬ìš©
                user_msg = exchange.get('user_message', '')
                ai_msg = exchange.get('ai_response', '')
                if user_msg and ai_msg:
                    # ë” ëª…í™•í•œ í„´ êµ¬ë¶„ê³¼ í•µì‹¬ ì •ë³´ ê°•ì¡°
                    history_parts.append(f"[ëŒ€í™” {i}í„´]")
                    history_parts.append(f"ğŸ‘¤ ì‚¬ìš©ì: {user_msg}")
                    # AI ë‹µë³€ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½)
                    summary_ai_msg = ai_msg[:200] + "..." if len(ai_msg) > 200 else ai_msg
                    history_parts.append(f"ğŸ¤– AI: {summary_ai_msg}")
                    history_parts.append("")  # í„´ êµ¬ë¶„ìš© ë¹ˆ ì¤„

            return "\n".join(history_parts)
        except Exception:
            return ""

    async def _save_conversation(self, session_id: str, user_message: str, result: Dict, query_analysis: Dict):
        """ëŒ€í™” ì €ì¥"""
        try:
            response = result['choices'][0]['message']['content']
            contexts = result.get('contexts', {})

            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            self.conversation_memory._append_message(session_id, 'user', user_message)
            # AI ì‘ë‹µ ì €ì¥
            self.conversation_memory._append_message(session_id, 'assistant', response)

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state = self.conversation_memory.get_state(session_id)
            state.update({
                "last_query_analysis": query_analysis,
                "last_sql_result": contexts.get('sql', ''),
                "last_vector_result": contexts.get('vector', ''),
                "updated_at": datetime.now()
            })

        except Exception as e:
            logger.warning(f"ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: {e}")


    def _create_error_response(self, message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": message
                },
                "finish_reason": "stop"
            }],
            "error": message,
            "timestamp": datetime.now().isoformat()
        }


    async def stream_response_with_messages(self, messages: List[Dict[str, str]], session_id: str = "default", ignore_history: bool = False) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

        Args:
            messages: ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬
            session_id: ì„¸ì…˜ ID
            ignore_history: íˆìŠ¤í† ë¦¬ ë¬´ì‹œ ì—¬ë¶€
        """
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = messages[-1]["content"] if messages else ""

        # LLM ëŒ€í™” ë¶„ì„
        query_analysis = await self._analyze_with_llm_context(messages, session_id)
        resolved_query = query_analysis.get('resolved_query', user_message)

        logger.info(f"ğŸ¯ ìŠ¤íŠ¸ë¦¬ë° - LLMì´ ì´í•´í•œ ì¿¼ë¦¬: {resolved_query}")

        # ê¸°ì¡´ ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ í™œìš©í•˜ë˜ resolved_query ì‚¬ìš©
        async for chunk in self._stream_with_resolved_query(resolved_query, query_analysis, messages, session_id, ignore_history):
            yield chunk

    async def _stream_with_resolved_query(
        self,
        resolved_query: str,
        query_analysis: Dict[str, Any],
        messages: List[Dict[str, str]],
        session_id: str,
        ignore_history: bool
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """í•´ê²°ëœ ì¿¼ë¦¬ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬"""
        # ê¸°ì¡´ stream_response ë¡œì§ í™œìš©
        async for chunk in self.stream_response(resolved_query, session_id, ignore_history):
            yield chunk

    async def stream_response(self, user_message: str, session_id: str = "default", ignore_history: bool = False) -> AsyncGenerator[Dict[str, Any], None]:
        """
        í†µí•© ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ - í†µí•© LangGraph/ê¸°ì¡´ LangGraph/ToT ì²´ì¸ ê¸°ë°˜

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            ignore_history: íˆìŠ¤í† ë¦¬ ë¬´ì‹œ ì—¬ë¶€

        Yields:
            Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
        """
        try:
            logger.info(f"ğŸ“º ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {user_message}")

            # í†µí•© LangGraph ëª¨ë“œ (ìµœìš°ì„ )
            if self.use_unified_langgraph and self.unified_langgraph_service:
                logger.info("ğŸŒŸ í†µí•© LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ")
                try:
                    async for chunk in self.unified_langgraph_service.run_agent_streaming(
                        user_message=user_message,
                        session_id=session_id
                    ):
                        # SSE í˜•ì‹ì˜ ì‘ë‹µì„ Dict í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        if chunk.startswith("data: "):
                            import json
                            try:
                                data = json.loads(chunk[6:])  # "data: " ì œê±°
                                if data.get("type") == "final":
                                    yield {"type": "content", "content": data.get("content", "")}
                                elif data.get("type") == "progress":
                                    yield {"type": "status", "content": data.get("message", "")}
                                elif data.get("type") == "error":
                                    yield {"type": "error", "content": data.get("message", "")}
                            except json.JSONDecodeError:
                                pass
                    return  # í†µí•© LangGraphë¡œ ì²˜ë¦¬ ì™„ë£Œ
                except Exception as e:
                    logger.error(f"âŒ í†µí•© LangGraph ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
                    logger.error("ğŸ”„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±")
                    # í´ë°±ìœ¼ë¡œ ê¸°ì¡´ ë¡œì§ ì‹¤í–‰

            # ê°„ë‹¨í•œ ë¶„ì„ ë° ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ìš”ì•½) - session_id ì „ë‹¬
            try:
                query_analysis = await self.query_analyzer.analyze_query_parallel(user_message, session_id)
            except Exception:
                query_analysis = {}

            # ContextBu ilderë¥¼ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            try:
                exp_ctx = (query_analysis.get('expansion_context') or '') if isinstance(query_analysis, dict) else ''
                if ignore_history:
                    combined_context = ContextBuilder.build_combined_context(
                        sql_context='',
                        vector_context='',
                        expansion_context=exp_ctx
                    )
                else:
                    prior = self.conversation_memory.get_last_contexts(session_id)
                    combined_context = ContextBuilder.build_combined_context(
                        sql_context=prior.get('sql', ''),
                        vector_context=prior.get('vector', ''),
                        expansion_context=exp_ctx
                    )
            except Exception:
                combined_context = ""

            # ìŠ¤íŠ¸ë¦¬ë° ë¡œì§: ChatModelë§Œ ì‚¬ìš© (LangGraphëŠ” ë©”ëª¨ë¦¬ ì „ìš©)
            async def chat_model_stream():
                if ChatModelHelper.validate_chat_model(self._chat_model):
                    # íˆìŠ¤í† ë¦¬ ì§€ì‹œ í”„ë¡¬í”„íŠ¸
                    directive = ContextBuilder.build_history_directive(user_message, mode=("ignore" if ignore_history else "auto"))
                    msgs = StreamingUtils.build_langchain_messages(
                        self.conversation_memory if not ignore_history else None,
                        session_id,
                        user_message,
                        f"{directive}\n\n{combined_context}" if combined_context else directive
                    )
                    async for chunk in self._chat_model.astream(msgs):
                        piece = getattr(chunk, "content", None)
                        if piece:
                            yield StreamingUtils.create_content_chunk(piece)
                else:
                    # ìµœì¢… í´ë°±: LLM í•¸ë“¤ëŸ¬ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
                    async for chunk_text in self.llm_handler.stream_response(user_message, agent_mode=False):
                        yield StreamingUtils.create_content_chunk(chunk_text)

            # ChatModel ìŠ¤íŠ¸ë¦¬ë°ë§Œ ì‚¬ìš©
            aggregated = []
            async for chunk in chat_model_stream():
                if chunk.get("type") == "content":
                    aggregated.append(chunk.get("content", ""))
                yield chunk

            # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ëŒ€í™” ì €ì¥ (ê°„ì†Œí™”)
            final_text = "".join(aggregated)
            if final_text:
                try:
                    prior = self.conversation_memory.get_last_contexts(session_id)
                    await self._save_conversation_safely(
                        session_id=session_id,
                        user_message=user_message,
                        assistant_response=final_text,
                        query_analysis=query_analysis,
                        sql_context=prior.get('sql', ''),
                        vector_context=prior.get('vector', '')
                    )
                except Exception as _e:
                    logger.warning(f"ìŠ¤íŠ¸ë¦¬ë° í›„ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: {_e}")

            logger.info("âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ LangGraph ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def get_session_info(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
        memory_stats = self.conversation_memory.get_session_stats(session_id)
        langchain_stats = self.llm_handler.get_performance_stats() if self.llm_handler else {}

        return {
            "session_info": memory_stats,
            "langchain_stats": langchain_stats,
            "hybrid_version": "2.0-chain-pipeline",
            "chain_info": {
                "pipeline_stages": ["analysis", "routing", "parallel_processing", "integration"],
                "parallel_support": True,
                "async_chains": True
            }
        }

    def clear_session_history(self, session_id: str):
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_memory.clear_session(session_id)
        self.llm_handler.clear_memory()
        logger.info(f"ì„¸ì…˜ {session_id} íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")

    def export_conversation_history(self, session_id: str) -> Dict[str, Any]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë‚´ë³´ë‚´ê¸°"""
        memory_export = self.conversation_memory.export_conversation(session_id)
        langchain_stats = self.llm_handler.get_performance_stats() if self.llm_handler else {}

        return {
            "memory_history": memory_export,
            "langchain_stats": langchain_stats,
            "export_timestamp": datetime.now().isoformat()
        }

    async def get_available_features(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ LangChain ê¸°ëŠ¥ë“¤ ì¡°íšŒ"""
        chains = await self.llm_handler.get_available_chains()
        tools = await self.llm_handler.get_available_tools()

        return {
            "available_chains": chains,
            "available_tools": tools,
            "streaming_support": True,
            "agent_support": True,
            "memory_support": True
        }

    def get_health_status(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬ë¥¼ ìœ„í•œ ìƒíƒœ ë°˜í™˜"""
        try:
            # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸ - í†µí•© LangGraph í¬í•¨
            if self.use_unified_langgraph:
                execution_mode = "Unified LangGraph"
            elif self.use_langgraph:
                execution_mode = "LangGraph"
            else:
                execution_mode = "ToT"
            status = {
                "status": "healthy",
                "service": "hybrid-mentor",
                "version": "3.0-langgraph",
                "execution_mode": execution_mode,
                "components": {
                    "llm_handler": hasattr(self, "llm_handler") and self.llm_handler is not None,
                    "query_analyzer": hasattr(self, "query_analyzer") and self.query_analyzer is not None,
                    "chain_processor": hasattr(self, "chain_processor") and self.chain_processor is not None,
                    "conversation_memory": hasattr(self, "conversation_memory") and self.conversation_memory is not None,
                    "langgraph_service": self._check_langgraph_service_health()
                },
                "langgraph_available": LANGGRAPH_AVAILABLE,
                "timestamp": datetime.now().isoformat()
            }

            # LangGraph ìƒíƒœ ì¶”ê°€
            if self.use_unified_langgraph and self.unified_langgraph_service:
                unified_health = self.unified_langgraph_service.get_health_status()
                status["unified_langgraph_status"] = unified_health
            elif self.use_langgraph and self.langgraph_service:
                langgraph_health = self.langgraph_service.get_health_status()
                status["langgraph_status"] = langgraph_health

            # ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì •ìƒì¸ì§€ í™•ì¸
            all_healthy = all(status["components"].values())
            if not all_healthy:
                status["status"] = "degraded"

            return status
        except Exception as e:
            return {
                "status": "unhealthy",
                "service": "hybrid-mentor",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # ==================== LangGraph ì „í™˜ ê´€ë¦¬ ë©”ì„œë“œë“¤ ====================

    def switch_to_langgraph(self) -> bool:
        """
        LangGraph ëª¨ë“œë¡œ ì „í™˜

        Returns:
            ì „í™˜ ì„±ê³µ ì—¬ë¶€
        """
        if not LANGGRAPH_AVAILABLE:
            logger.error("âŒ LangGraphê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì „í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False

        if self.use_langgraph:
            logger.info("â„¹ï¸ ì´ë¯¸ LangGraph ëª¨ë“œì…ë‹ˆë‹¤")
            return True

        try:
            # LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            if self.langgraph_service is None:
                self.langgraph_service = create_langgraph_service(self.conversation_memory)

            self.use_langgraph = True
            logger.info("âœ… LangGraph ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ LangGraph ëª¨ë“œ ì „í™˜ ì‹¤íŒ¨: {e}")
            return False

    def switch_to_tot(self) -> bool:
        """
        ToT ëª¨ë“œë¡œ ì „í™˜

        Returns:
            ì „í™˜ ì„±ê³µ ì—¬ë¶€
        """
        if not self.use_langgraph:
            logger.info("â„¹ï¸ ì´ë¯¸ ToT ëª¨ë“œì…ë‹ˆë‹¤")
            return True

        try:
            self.use_langgraph = False
            logger.info("âœ… ToT ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ToT ëª¨ë“œ ì „í™˜ ì‹¤íŒ¨: {e}")
            return False

    def get_execution_mode(self) -> str:
        """í˜„ì¬ ì‹¤í–‰ ëª¨ë“œ ë°˜í™˜"""
        return "LangGraph" if self.use_langgraph else "ToT"

    def is_langgraph_available(self) -> bool:
        """LangGraph ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return LANGGRAPH_AVAILABLE

    def enable_langgraph_mode(self) -> bool:
        """LangGraph ëª¨ë“œ í™œì„±í™” (ê°•í™”ëœ ê²€ì¦ ë¡œì§ í¬í•¨)"""
        if not LANGGRAPH_AVAILABLE:
            logger.warning("âš ï¸ LangGraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - LANGGRAPH_AVAILABLE=False")
            return False

        try:
            logger.info("ğŸ”§ LangGraph ëª¨ë“œ í™œì„±í™” ì‹œì‘")
            logger.info(f"   í˜„ì¬ langgraph_service ìƒíƒœ: {self.langgraph_service is not None}")
            logger.info(f"   conversation_memory ìƒíƒœ: {self.conversation_memory is not None}")

            # 1ë‹¨ê³„: ì„œë¹„ìŠ¤ ìƒì„±
            logger.info("ğŸ”„ 1ë‹¨ê³„: LangGraph ì„œë¹„ìŠ¤ ìƒì„± ì¤‘...")
            self.langgraph_service = create_langgraph_service(self.conversation_memory)
            logger.info("âœ… 1ë‹¨ê³„ ì™„ë£Œ: LangGraph ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„±ë¨")

            # 2ë‹¨ê³„: ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
            logger.info("ğŸ”„ 2ë‹¨ê³„: ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ ì‹¤í–‰ ì¤‘...")
            health_status = self.langgraph_service.get_health_status()
            service_status = health_status.get("status", "unknown")

            if service_status != "healthy":
                raise Exception(f"ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {service_status}")

            logger.info(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: ì„œë¹„ìŠ¤ ìƒíƒœ = {service_status}")
            logger.info(f"   ì„œë¹„ìŠ¤ íƒ€ì…: {health_status.get('service_type')}")
            logger.info(f"   ì»´í¬ë„ŒíŠ¸: {list(health_status.get('components', {}).keys())}")

            # 3ë‹¨ê³„: ê°„ë‹¨í•œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ”„ 3ë‹¨ê³„: ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                # ê°„ë‹¨í•œ ì„¸ì…˜ ì •ë³´ ì¡°íšŒë¡œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
                test_info = self.langgraph_service.get_session_info("test_validation")
                logger.info(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ - {test_info.get('service_type')}")
            except Exception as test_error:
                logger.warning(f"âš ï¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨í•˜ì§€ë§Œ ê³„ì† ì§„í–‰: {test_error}")

            # 4ë‹¨ê³„: ëª¨ë“œ í™œì„±í™”
            self.use_langgraph = True
            logger.info("ğŸš€ LangGraph ëª¨ë“œê°€ ì„±ê³µì ìœ¼ë¡œ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")

            return True

        except ImportError as e:
            logger.error(f"âŒ LangGraph import ì˜¤ë¥˜: {e}")
            logger.error("   LangGraph ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            self.use_langgraph = False
            self.langgraph_service = None
            return False

        except Exception as e:
            logger.error(f"âŒ LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error("ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            import traceback
            for line in traceback.format_exc().splitlines():
                logger.error(f"   {line}")

            # ì‹¤íŒ¨ì‹œ ìƒíƒœ ì´ˆê¸°í™”
            self.use_langgraph = False
            self.langgraph_service = None
            logger.error("ğŸ”„ LangGraph ëª¨ë“œ ë¹„í™œì„±í™”ë¨ - ToT ëª¨ë“œë¡œ í´ë°±")
            return False

    def disable_langgraph_mode(self) -> bool:
        """LangGraph ëª¨ë“œ ë¹„í™œì„±í™” (ToT ëª¨ë“œë¡œ ì „í™˜)"""
        self.use_langgraph = False
        logger.info("ğŸ”„ ToT ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤")
        return True

    def _validate_langgraph_service(self) -> bool:
        """LangGraph ì„œë¹„ìŠ¤ ê²€ì¦ (ëŸ°íƒ€ì„ ì²´í¬)"""
        try:
            if self.langgraph_service is None:
                logger.debug("ğŸ” ê²€ì¦ ì‹¤íŒ¨: langgraph_serviceê°€ None")
                return False

            # í—¬ìŠ¤ì²´í¬ í˜¸ì¶œ
            health = self.langgraph_service.get_health_status()
            status = health.get("status", "unknown")

            if status != "healthy":
                logger.warning(f"ğŸ” ê²€ì¦ ì‹¤íŒ¨: ì„œë¹„ìŠ¤ ìƒíƒœ = {status}")
                return False

            logger.debug("ğŸ” ê²€ì¦ ì„±ê³µ: LangGraph ì„œë¹„ìŠ¤ ì •ìƒ")
            return True

        except Exception as e:
            logger.warning(f"ğŸ” ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _check_langgraph_service_health(self) -> bool:
        """í—¬ìŠ¤ì²´í¬ìš© LangGraph ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ (ì‹¤ì œ ê¸°ëŠ¥ ê²€ì¦ í¬í•¨)"""
        try:
            # 1ë‹¨ê³„: ê°ì²´ ì¡´ì¬ í™•ì¸
            if not hasattr(self, "langgraph_service") or self.langgraph_service is None:
                return False

            # 2ë‹¨ê³„: ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ í˜¸ì¶œ
            health_status = self.langgraph_service.get_health_status()
            if health_status.get("status") != "healthy":
                return False

            # 3ë‹¨ê³„: ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ê²€ì¦)
            service_type = health_status.get("service_type")
            if service_type != "langgraph":
                return False

            # 4ë‹¨ê³„: ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
            components = health_status.get("components", {})
            langgraph_app_healthy = components.get("langgraph_app") == "healthy"

            return langgraph_app_healthy

        except Exception as e:
            # í—¬ìŠ¤ì²´í¬ì—ì„œëŠ” ë¡œê¹… ìµœì†Œí™”
            return False

    async def compare_execution_modes(
        self,
        user_message: str,
        session_id: str = "test_comparison"
    ) -> Dict[str, Any]:
        """
        ToTì™€ LangGraph ëª¨ë“œ ë¹„êµ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸/ê°œë°œìš©)

        Args:
            user_message: í…ŒìŠ¤íŠ¸í•  ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: í…ŒìŠ¤íŠ¸ìš© ì„¸ì…˜ ID

        Returns:
            ë‘ ëª¨ë“œì˜ ì‹¤í–‰ ê²°ê³¼ ë¹„êµ
        """
        if not LANGGRAPH_AVAILABLE:
            return {
                "error": "LangGraphê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "user_message": user_message
            }

        logger.info(f"ğŸ”¬ ì‹¤í–‰ ëª¨ë“œ ë¹„êµ í…ŒìŠ¤íŠ¸: {user_message}")
        results = {}

        # í˜„ì¬ ëª¨ë“œ ë°±ì—…
        original_mode = self.use_langgraph

        try:
            # 1. ToT ëª¨ë“œ ì‹¤í–‰
            start_time = datetime.now()
            self.use_langgraph = False
            tot_result = await self.run_agent(user_message, f"{session_id}_tot")
            tot_duration = (datetime.now() - start_time).total_seconds()

            results["tot"] = {
                "result": tot_result,
                "duration": tot_duration,
                "mode": "ToT"
            }

            # 2. LangGraph ëª¨ë“œ ì‹¤í–‰
            start_time = datetime.now()
            self.use_langgraph = True
            if self.langgraph_service is None:
                self.langgraph_service = create_langgraph_service(self.conversation_memory)

            langgraph_result = await self.run_agent(user_message, f"{session_id}_langgraph")
            langgraph_duration = (datetime.now() - start_time).total_seconds()

            results["langgraph"] = {
                "result": langgraph_result,
                "duration": langgraph_duration,
                "mode": "LangGraph"
            }

            # 3. ë¹„êµ ë¶„ì„
            results["comparison"] = {
                "speed_improvement": (tot_duration - langgraph_duration) / tot_duration * 100,
                "tot_faster": tot_duration < langgraph_duration,
                "user_message": user_message,
                "session_id": session_id
            }

            logger.info(f"ğŸ ë¹„êµ ì™„ë£Œ - ToT: {tot_duration:.2f}ì´ˆ, LangGraph: {langgraph_duration:.2f}ì´ˆ")

        except Exception as e:
            logger.error(f"âŒ ë¹„êµ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)

        finally:
            # ì›ë˜ ëª¨ë“œë¡œ ë³µì›
            self.use_langgraph = original_mode

        return results

    def get_langgraph_visualization(self) -> str:
        """LangGraph ì‹œê°í™” (ìˆëŠ” ê²½ìš°)"""
        if self.unified_langgraph_service:
            return self.unified_langgraph_service.get_graph_visualization()
        elif self.langgraph_service:
            return self.langgraph_service.get_graph_visualization()
        return "LangGraph ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # ==================== í†µí•© LangGraph ì „í™˜ ë©”ì„œë“œë“¤ ====================

    def enable_unified_langgraph_mode(self) -> bool:
        """í†µí•© LangGraph ëª¨ë“œ í™œì„±í™” (Light/Medium/Heavy í†µí•© ì²˜ë¦¬)"""
        if not UNIFIED_LANGGRAPH_AVAILABLE:
            logger.warning("âš ï¸ í†µí•© LangGraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - UNIFIED_LANGGRAPH_AVAILABLE=False")
            return False

        try:
            logger.info("ğŸ”§ í†µí•© LangGraph ëª¨ë“œ í™œì„±í™” ì‹œì‘")

            # 1ë‹¨ê³„: ê¸°ì¡´ ëª¨ë“œë“¤ ë¹„í™œì„±í™”
            logger.info("ğŸ”„ 1ë‹¨ê³„: ê¸°ì¡´ ëª¨ë“œë“¤ ë¹„í™œì„±í™”...")
            self.use_langgraph = False
            self.use_unified_langgraph = False

            # 2ë‹¨ê³„: í†µí•© ì„œë¹„ìŠ¤ ìƒì„±
            logger.info("ğŸ”„ 2ë‹¨ê³„: í†µí•© LangGraph ì„œë¹„ìŠ¤ ìƒì„± ì¤‘...")
            self.unified_langgraph_service = create_unified_langgraph_service(self.conversation_memory)

            # 3ë‹¨ê³„: ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
            logger.info("ğŸ”„ 3ë‹¨ê³„: í†µí•© ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ ì‹¤í–‰ ì¤‘...")
            health_status = self.unified_langgraph_service.get_health_status()
            if health_status.get("status") != "healthy":
                raise Exception(f"í†µí•© ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {health_status}")

            logger.info("âœ… í†µí•© ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ í†µê³¼")

            # 4ë‹¨ê³„: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ”„ 4ë‹¨ê³„: í†µí•© ì„œë¹„ìŠ¤ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
            test_visualization = self.unified_langgraph_service.get_graph_visualization()
            if not test_visualization or "error" in test_visualization.lower():
                logger.warning("âš ï¸ ê·¸ë˜í”„ ì‹œê°í™” í…ŒìŠ¤íŠ¸ì—ì„œ ê²½ê³ ê°€ ë°œìƒí–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")

            logger.info("âœ… í†µí•© ì„œë¹„ìŠ¤ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼")

            # 5ë‹¨ê³„: ëª¨ë“œ í™œì„±í™”
            self.use_unified_langgraph = True
            logger.info("ğŸš€ í†µí•© LangGraph ëª¨ë“œê°€ ì„±ê³µì ìœ¼ë¡œ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
            logger.info("   ğŸ“‹ ì§€ì› ê¸°ëŠ¥:")
            logger.info("     â€¢ Light: ê°„ë‹¨í•œ ëŒ€í™” (LLM ì§ì ‘ í˜¸ì¶œ)")
            logger.info("     â€¢ Medium: ì¤‘ê°„ ë³µì¡ë„ (ë‹¨ì¼ ì—ì´ì „íŠ¸)")
            logger.info("     â€¢ Heavy: ë†’ì€ ë³µì¡ë„ (ë³‘ë ¬ ë‹¤ì¤‘ ì—ì´ì „íŠ¸)")

            return True

        except ImportError as e:
            logger.error(f"âŒ í†µí•© LangGraph import ì˜¤ë¥˜: {e}")
            logger.error("   í†µí•© LangGraph ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            self.use_unified_langgraph = False
            self.unified_langgraph_service = None
            return False

        except Exception as e:
            logger.error(f"âŒ í†µí•© LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error("ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            import traceback
            for line in traceback.format_exc().splitlines():
                logger.error(f"   {line}")

            # ì‹¤íŒ¨ì‹œ ìƒíƒœ ì´ˆê¸°í™”
            self.use_unified_langgraph = False
            self.unified_langgraph_service = None
            logger.error("ğŸ”„ í†µí•© LangGraph ëª¨ë“œ ë¹„í™œì„±í™”ë¨ - ToT ëª¨ë“œë¡œ í´ë°±")
            return False

    def disable_unified_langgraph_mode(self) -> bool:
        """í†µí•© LangGraph ëª¨ë“œ ë¹„í™œì„±í™” (ToT ëª¨ë“œë¡œ ì „í™˜)"""
        self.use_unified_langgraph = False
        logger.info("ğŸ”„ í†µí•© LangGraphì—ì„œ ToT ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤")
        return True

    def get_execution_mode(self) -> str:
        """í˜„ì¬ ì‹¤í–‰ ëª¨ë“œ ë°˜í™˜"""
        if self.use_unified_langgraph:
            return "Unified LangGraph"
        elif self.use_langgraph:
            return "LangGraph"
        else:
            return "ToT"

    def is_unified_langgraph_available(self) -> bool:
        """í†µí•© LangGraph ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return UNIFIED_LANGGRAPH_AVAILABLE and self.unified_langgraph_service is not None

    def get_unified_complexity_stats(self) -> Dict[str, Any]:
        """í†µí•© LangGraphì˜ ë³µì¡ë„ë³„ í†µê³„"""
        if self.unified_langgraph_service:
            return self.unified_langgraph_service.get_complexity_stats()
        return {"message": "í†µí•© LangGraph ì„œë¹„ìŠ¤ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
