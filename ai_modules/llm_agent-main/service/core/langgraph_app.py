import logging
import asyncio
from typing import Dict, Any
from langgraph.graph import StateGraph, START, END

# í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë§Œ import
from ..handlers import (
    VectorSearchHandler,
    SqlQueryHandler,
    DepartmentMappingHandler,
    CurriculumHandler,
    QueryAnalyzer,
    ResultSynthesizer,
    LlmClient
)
from ..memory.memory import ConversationMemory
from ..memory.context_analyzer import ConversationContextAnalyzer
from .langgraph_state import GraphState, create_initial_state
from ..nodes import NodeManager


logger = logging.getLogger(__name__)

class LangGraphApp:
    """LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜"""

    def __init__(self, conversation_memory: ConversationMemory = None):
        logger.info("ğŸ—ï¸ í†µí•© LangGraph ì•„í‚¤í…ì²˜ ì´ˆê¸°í™” ì‹œì‘")

        # ë©”ëª¨ë¦¬ ì„¤ì •
        self.conversation_memory = conversation_memory

        # LLM í•¸ë“¤ëŸ¬ ìƒì„± (í†µí•© ì‚¬ìš©)
        self.llm_handler = LlmClient(max_tokens=10000)

        # íˆìŠ¤í† ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™” (llm_handler ì „ë‹¬)
        self.context_analyzer = ConversationContextAnalyzer(self.llm_handler)

        # NodeManager ì´ˆê¸°í™”
        self.node_manager = NodeManager(
            query_analyzer=QueryAnalyzer(conversation_memory=self.conversation_memory),
            llm_handler=self.llm_handler,  # ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            sql_handler=SqlQueryHandler(),
            vector_handler=VectorSearchHandler(),
            dept_handler=DepartmentMappingHandler(),
            curriculum_handler=CurriculumHandler(),
            result_synthesizer=ResultSynthesizer(self.llm_handler),  # ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            conversation_memory=self.conversation_memory,  # ë©”ëª¨ë¦¬ ì „ë‹¬
            llm_client=self.llm_handler  # Light ê²€ì¦ìš© LLM Client
        )

        # ê·¸ë˜í”„ ë¹Œë“œ
        self.graph = self.build_graph()

        logger.info("âœ… í†µí•© LangGraph ì•„í‚¤í…ì²˜ ì´ˆê¸°í™” ì™„ë£Œ")

    def build_graph(self) -> StateGraph:
        """ê·¸ë˜í”„ êµ¬ì¡° ìƒì„±"""
        logger.info("ğŸ”§ ê·¸ë˜í”„ êµ¬ì¡° ìƒì„±")

        graph = StateGraph(GraphState)

        # NodeManagerë¡œë¶€í„° ëª¨ë“  ë…¸ë“œë“¤ ê°€ì ¸ì˜¤ê¸°
        all_nodes = self.node_manager.get_all_nodes()
        conditional_edges = self.node_manager.get_conditional_edges()

        # ë…¸ë“œë“¤ ì¶”ê°€
        for node_name, node_func in all_nodes.items():
            graph.add_node(node_name, node_func)

        logger.info(f"âœ… {len(all_nodes)}ê°œ ë…¸ë“œ ì¶”ê°€ ì™„ë£Œ")

        # íë¦„ ì •ì˜
        graph.add_edge(START, "router")

        # ì¡°ê±´ë¶€ ì—£ì§€ë“¤ ì¶”ê°€
        for edge_name, edge_func in conditional_edges.items():
            if edge_name == "route_by_complexity":
                graph.add_conditional_edges(
                    "router",
                    edge_func,
                    {
                        "light": "light_validator",  # ğŸ”¥ lightëŠ” validatorë¥¼ ê±°ì¹¨
                        "medium_sql": "medium_sql",
                        "medium_vector": "medium_vector",
                        "medium_curriculum": "medium_curriculum",
                        "medium_department": "medium_department",
                        "heavy_sequential": "heavy_sequential"
                    }
                )

        # ğŸ”¥ light_validator â†’ light (ê²€ì¦ í†µê³¼) or rejection (ê²€ì¦ ì‹¤íŒ¨)
        def route_light_validation(state: Dict[str, Any]) -> str:
            """Light ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ë¼ìš°íŒ…"""
            validation_passed = state.get("light_validation_passed", True)
            if validation_passed:
                logger.info("âœ… Light ê²€ì¦ í†µê³¼ â†’ light ë…¸ë“œ")
                return "light"
            else:
                logger.warning("âŒ Light ê²€ì¦ ì‹¤íŒ¨ â†’ rejection ë…¸ë“œ")
                return "reject"

        graph.add_conditional_edges(
            "light_validator",
            route_light_validation,
            {
                "light": "light",
                "reject": "rejection"  # ğŸ”¥ ê²€ì¦ ì‹¤íŒ¨ ì‹œ rejection ë…¸ë“œë¡œ
            }
        )

        # rejection â†’ END (ê±°ì ˆ ë©”ì‹œì§€ ë°˜í™˜ í›„ ì¢…ë£Œ)
        graph.add_edge("rejection", END)

        # ëª¨ë“  ì²˜ë¦¬ ë…¸ë“œ â†’ synthesis â†’ finalize â†’ END
        for node in ["light", "medium_sql", "medium_vector",  "medium_department", "heavy_sequential"]:
            graph.add_edge(node, "synthesis")

        graph.add_edge("synthesis", "finalize")
        graph.add_edge("finalize", END)

        compiled_graph = graph.compile()
        logger.info("âœ… í†µí•© ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ")

        return compiled_graph

    async def process_query(self, user_message: str, session_id: str = "default") -> Dict[str, Any]:
        """í†µí•© ì¿¼ë¦¬ ì²˜ë¦¬ - íˆìŠ¤í† ë¦¬ ë¶„ì„ í¬í•¨"""
        logger.info(f"ğŸš€ í†µí•© ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: '{user_message}...'")

        # Follow-up ì§ˆë¬¸ ìƒì„± ìš”ì²­ ì°¨ë‹¨ (ê°•í™”ëœ ì•ˆì „ì¥ì¹˜)
        follow_up_patterns = [
            "### Task:",
            "follow-up questions",
            "follow-up prompts",
            "Suggest 3-5 relevant",
            "continue or deepen the discussion"
        ]

        if any(pattern in user_message for pattern in follow_up_patterns):
            logger.warning("ğŸš« Follow-up ì§ˆë¬¸ ìƒì„± ìš”ì²­ ì°¨ë‹¨ (LangGraph)")
            return {
                "response": "Follow-up ì§ˆë¬¸ ìƒì„± ìš”ì²­ì€ ì²˜ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "complexity": "blocked",
                "is_continuation": False,
                "history_usage": {}
            }

        # íˆìŠ¤í† ë¦¬ ë¶„ì„ ìˆ˜í–‰
        default_result = {
            "is_continuation": False,
            "query": user_message,
            "history_usage": {
                "reuse_previous": False,
                "relationship": "new_search"
            }
        }


        if not self.context_analyzer or not self.conversation_memory:
            logger.info("ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ê¸° ë˜ëŠ” ë©”ëª¨ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ë°˜í™˜")
            history_analysis = default_result
        else:
            history_analysis = await self.context_analyzer.analyze_session_context(
                user_message,
                self.conversation_memory,
                session_id
            )


        # ìƒíƒœ ì´ˆê¸°í™”
        initial_state = create_initial_state(user_message, session_id)
        initial_state["conversation_memory"] = self.conversation_memory
        initial_state["is_continuation"] = history_analysis.get("is_continuation", False)
        initial_state["history_usage"] = history_analysis.get("history_usage", {})

        # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì¶”ê°€ (ì¤‘ê°„ í”¼ë“œë°±ìš©) - process_queryìš© (ë¡œê·¸ë§Œ ì¶œë ¥)
        async def stream_callback_dummy(msg: str):
            logger.info(f"ğŸ“¤ í”¼ë“œë°± ë©”ì‹œì§€ (non-streaming): {msg}")

        initial_state["stream_callback"] = stream_callback_dummy

        # ì¬êµ¬ì„±ëœ ì¿¼ë¦¬ ì‚¬ìš© (ì—°ì†ëŒ€í™”ë“  ìƒˆ ì§ˆë¬¸ì´ë“  í•­ìƒ reconstructed_query ì‚¬ìš©)
        reconstructed_query = history_analysis.get("reconstructed_query", user_message)
        initial_state["query"] = reconstructed_query

        if history_analysis.get("is_continuation", False):
            logger.info(f"ğŸ”„ ì—°ì†ëŒ€í™” ê°ì§€: '{user_message}' â†’ '{reconstructed_query}'")
        else:
            logger.info(f"ğŸ†• ìƒˆë¡œìš´ ì§ˆë¬¸: '{reconstructed_query}'")

        # ê·¸ë˜í”„ ì‹¤í–‰
        result = await self.graph.ainvoke(initial_state)

        # ì‘ë‹µ ìƒì„±
        response = result.get("final_result", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
        if self.conversation_memory:
            self.conversation_memory.add_exchange(
                session_id=session_id,
                user_message=user_message,
                assistant_response=response
            )
            logger.info(f"ğŸ’¾ ëŒ€í™” ì €ì¥ ì™„ë£Œ: session_id={session_id}")

        return {
            "response": response,
            "complexity": result.get("complexity", "unknown"),
            "is_continuation": result.get("is_continuation", False),
            "history_usage": result.get("history_usage", {})
        }

    async def process_query_stream(self, user_message: str, session_id: str = "default"):
        """ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì¿¼ë¦¬ ì²˜ë¦¬"""
        logger.info(f"ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: '{user_message}...'")


        # Follow-up ì°¨ë‹¨
        follow_up_patterns = [
            "### Task:",
            "follow-up questions",
            "follow-up prompts",
            "Suggest 3-5 relevant",
            "continue or deepen the discussion"
        ]

        if any(pattern in user_message for pattern in follow_up_patterns):
            logger.warning("ğŸš« Follow-up ì§ˆë¬¸ ìƒì„± ìš”ì²­ ì°¨ë‹¨")
            return

        # íˆìŠ¤í† ë¦¬ ë¶„ì„
        default_result = {
            "is_continuation": False,
            "query": user_message,
            "history_usage": {
                "reuse_previous": False,
                "relationship": "new_search"
            }
        }

        if not self.context_analyzer or not self.conversation_memory:
            history_analysis = default_result
        else:
            history_analysis = await self.context_analyzer.analyze_session_context(
                user_message,
                self.conversation_memory,
                session_id
            )

        # ìƒíƒœ ì´ˆê¸°í™”
        initial_state = create_initial_state(user_message, session_id)
        initial_state["conversation_memory"] = self.conversation_memory
        initial_state["is_continuation"] = history_analysis.get("is_continuation", False)
        initial_state["history_usage"] = history_analysis.get("history_usage", {})

        # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì¶”ê°€ - ì¦‰ì‹œ yieldí•˜ëŠ” í ê¸°ë°˜
        feedback_queue = []

        async def stream_callback(msg: str):
            feedback_queue.append(msg)

        initial_state["stream_callback"] = stream_callback

        reconstructed_query = history_analysis.get("reconstructed_query", user_message)
        initial_state["query"] = reconstructed_query

        if history_analysis.get("is_continuation", False):
            logger.info(f"ğŸ”„ ì—°ì†ëŒ€í™” ê°ì§€: '{user_message}' â†’ '{reconstructed_query}'")
        else:
            logger.info(f"ğŸ†• ìƒˆë¡œìš´ ì§ˆë¬¸: '{reconstructed_query}'")

        # ê·¸ë˜í”„ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰í•˜ë©´ì„œ ê° ë…¸ë“œë§ˆë‹¤ í”¼ë“œë°± í™•ì¸
        final_state = None
        async for event in self.graph.astream(initial_state):
            # ë…¸ë“œ ì‹¤í–‰ í›„ ìƒˆë¡œìš´ í”¼ë“œë°±ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ yield (ë¬¸ë‹¨ êµ¬ë¶„)
            while feedback_queue:
                feedback_msg = feedback_queue.pop(0)
                yield feedback_msg + "\n\n"
                await asyncio.sleep(0.01)

            # ë§ˆì§€ë§‰ ìƒíƒœ ì €ì¥
            if event:
                for node_name, node_state in event.items():
                    if node_state:
                        final_state = node_state

        # ë‚¨ì€ í”¼ë“œë°± ë©”ì‹œì§€ ì²˜ë¦¬
        while feedback_queue:
            feedback_msg = feedback_queue.pop(0)
            yield feedback_msg + "\n\n"
            await asyncio.sleep(0.01)

        # final_resultê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        final_result = final_state.get("final_result", "") if final_state else ""

        if final_result:
            # ê²°ê³¼ë¥¼ í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë° (ì‹œë®¬ë ˆì´ì…˜)
            for char in final_result:
                yield char
                await asyncio.sleep(0.01)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼

            # ë©”ëª¨ë¦¬ì— ì €ì¥
            if self.conversation_memory:
                self.conversation_memory.add_exchange(
                    session_id=session_id,
                    user_message=user_message,
                    assistant_response=final_result
                )
                logger.info(f"ğŸ’¾ ëŒ€í™” ì €ì¥ ì™„ë£Œ: session_id={session_id}")
        else:
            yield "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."