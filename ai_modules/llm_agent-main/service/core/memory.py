"""
ì§„ì§œ LangGraph í˜•ì‹ ëŒ€í™” ìƒíƒœ ê´€ë¦¬ (ConversationMemory)
ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸(SQL/ë²¡í„°), ìºì‹œ, í†µê³„ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

LangGraph StateGraph ê¸°ë°˜:
- MessagesState: LangGraph í‘œì¤€ ìƒíƒœ ì •ì˜
- StateGraph: ë…¸ë“œì™€ ì—£ì§€ë¥¼ í†µí•œ ëŒ€í™” í”Œë¡œìš°
- Checkpointer: ì„¸ì…˜ë³„ ìƒíƒœ ì €ì¥/ë³µì›
"""

import logging
from typing import Dict, List, Any, Optional, TypedDict
try:
    from typing import Annotated
except ImportError:
    from typing_extensions import Annotated
from datetime import datetime
import re
import json
try:
    # LangGraph í•„ìˆ˜ imports
    from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
    from langchain_core.chat_history import BaseChatMessageHistory
    from langgraph.graph import StateGraph, MessagesState, START, END
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.graph.message import add_messages
    LANGGRAPH_AVAILABLE = True
except Exception:  # pragma: no cover
    # í´ë°± ì •ì˜
    BaseMessage = Any
    def add_messages(left: List[Any], right: List[Any]) -> List[Any]:
        """í´ë°± add_messages í•¨ìˆ˜"""
        return left + right

    class HumanMessage:
        def __init__(self, content: str):
            self.content = content
    class AIMessage:
        def __init__(self, content: str):
            self.content = content
    class SystemMessage:
        def __init__(self, content: str):
            self.content = content
    class BaseChatMessageHistory:
        @property
        def messages(self):
            return []
        def add_message(self, message: Any) -> None:
            pass
        def clear(self) -> None:
            pass
    class MessagesState(TypedDict):
        messages: Annotated[List[BaseMessage], add_messages]
    LANGGRAPH_AVAILABLE = False

# LangChain Memory backends (optional)
try:  # pragma: no cover
    from langchain_community.chat_message_histories import (
        SQLChatMessageHistory,
        ChatMessageHistory as LCInMemoryHistory,
    )
    LANGCHAIN_MEMORY_AVAILABLE = True
except Exception:  # pragma: no cover
    SQLChatMessageHistory = None  # type: ignore
    LCInMemoryHistory = None  # type: ignore
    LANGCHAIN_MEMORY_AVAILABLE = False

logger = logging.getLogger(__name__)

# LangGraph í™•ì¥ ìƒíƒœ ì •ì˜
class AIMentorState(MessagesState):
    """AI Mentorìš© í™•ì¥ ìƒíƒœ - LangGraph MessagesState ê¸°ë°˜"""
    session_id: str
    current_topic: Optional[str]
    last_sql_result: str
    last_vector_result: str
    query_analysis: Optional[Dict[str, Any]]
    search_results: Optional[List[Dict]]
    entity_cache: Dict[str, Dict[str, Any]]
    metadata: Dict[str, Any]

class ConversationState(TypedDict):
    """ëŒ€í™” ìƒíƒœ íƒ€ì… ì •ì˜"""
    session_id: str
    conversation_history: List[Dict[str, Any]]  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬
    last_search_results: Optional[List[Dict]]   # ìµœê·¼ ê²€ìƒ‰ ê²°ê³¼
    current_topic: Optional[str]                # í˜„ì¬ ì£¼ì œ
    last_query_analysis: Optional[Dict]         # ìµœê·¼ ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼
    metadata: Dict[str, Any]                    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    created_at: datetime
    updated_at: datetime
    last_sql_result: str                        # ìµœê·¼ SQL ê²°ê³¼(ì›ë¬¸)
    last_vector_result: str                     # ìµœê·¼ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼(ì›ë¬¸)
    entity_cache: Dict[str, Dict[str, Dict[str, Any]]]  # {'professors':{key:{data,updated_at}}, 'courses':{...}, 'departments':{...}}

class ConversationMemory:
    """ì§„ì§œ LangGraph ê¸°ë°˜ ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        max_history_length: int = 20,
        llm_handler: Any = None,
        lc_memory_backend: str = "memory",  # "memory" | "sqlite"
        lc_sqlite_path: Optional[str] = None,
    ):
        self.sessions: Dict[str, ConversationState] = {}
        self.max_history_length = max_history_length
        # ì™¸ë¶€ LLM í•¸ë“¤ëŸ¬ ì£¼ì…(ì„ íƒ)
        self.llm_handler = llm_handler

        # LangChain Memory ì„¤ì •
        self.lc_memory_backend = lc_memory_backend
        self.lc_sqlite_path = lc_sqlite_path or "./conversation_memory.sqlite"
        self._lc_histories: Dict[str, BaseChatMessageHistory] = {}

        # LangGraph StateGraph ì´ˆê¸°í™”
        if LANGGRAPH_AVAILABLE:
            self._init_langgraph()
        else:
            self.graph = None
            self.checkpointer = None

        logger.debug("ConversationMemory ì´ˆê¸°í™” ì™„ë£Œ (LangGraph ì§€ì›)")

    # ===== LangChain Memory helpers =====
    def _get_lc_history(self, session_id: str) -> Optional[BaseChatMessageHistory]:
        """ì„¸ì…˜ë³„ LangChain ChatMessageHistoryë¥¼ ë°˜í™˜ (ë°±ì—”ë“œì— ë”°ë¼ ìƒì„±/ìºì‹œ)."""
        if not LANGCHAIN_MEMORY_AVAILABLE:
            return None

        if session_id in self._lc_histories:
            return self._lc_histories[session_id]

        history: Optional[BaseChatMessageHistory] = None
        try:
            if self.lc_memory_backend == "sqlite" and SQLChatMessageHistory is not None:
                conn = f"sqlite:///{self.lc_sqlite_path}"
                history = SQLChatMessageHistory(session_id=session_id, connection_string=conn)
            elif LCInMemoryHistory is not None:
                history = LCInMemoryHistory()
        except Exception:
            history = None

        if history is not None:
            self._lc_histories[session_id] = history
        return history

    def _init_langgraph(self):
        """LangGraph StateGraph ì´ˆê¸°í™”"""
        try:
            logger.info("LangGraph ì´ˆê¸°í™” ì‹œì‘...")

            # StateGraph ìƒì„±
            self.checkpointer = MemorySaver()
            logger.debug("MemorySaver ìƒì„± ì™„ë£Œ")

            graph = StateGraph(AIMentorState)
            logger.debug("StateGraph ìƒì„± ì™„ë£Œ")

            # ë…¸ë“œ: ìµœì†Œ êµ¬ì„±(ë‹¨ì¼ ì‘ë‹µ ìƒì„± ë…¸ë“œ)
            graph.add_node("generate_response", self._generate_response_node)
            logger.debug("LangGraph ë…¸ë“œ ì¶”ê°€ ì™„ë£Œ (generate_response)")

            # ì—£ì§€: START â†’ generate_response â†’ END
            graph.add_edge(START, "generate_response")
            graph.add_edge("generate_response", END)
            logger.debug("LangGraph ì—£ì§€ ì¶”ê°€ ì™„ë£Œ (STARTâ†’generate_responseâ†’END)")

            # ê·¸ë˜í”„ ì»´íŒŒì¼
            self.graph = graph.compile(checkpointer=self.checkpointer)
            logger.info("âœ… LangGraph StateGraph ì»´íŒŒì¼ ì™„ë£Œ!")

        except Exception as e:
            logger.error(f"âŒ LangGraph ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            self.graph = None
            self.checkpointer = None

    # LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ (ìµœì†Œ êµ¬ì„±)
    def _build_system_context(self, state: AIMentorState) -> str:
        """ë¶„ì„/ìºì‹œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ë¡œ í•©ì„±"""
        try:
            from utils.context_builder import ContextBuilder
        except Exception:
            return ""

        qa = state.get("query_analysis") or {}
        expansion_ctx = qa.get("expansion_context", "") if isinstance(qa, dict) else ""
        return ContextBuilder.build_combined_context(
            sql_context=state.get("last_sql_result", ""),
            vector_context=state.get("last_vector_result", ""),
            expansion_context=expansion_ctx
        )

    async def _judge_and_refine(self, user_text: str, draft_answer: str) -> str:
        """LLM ê¸°ë°˜ ìµœì¢… ê²€ìˆ˜/ì •ì œ. ì‹¤íŒ¨ ì‹œ draft ë°˜í™˜"""
        if not self.llm_handler:
            return draft_answer

        prompt = f"""
            ë„ˆëŠ” ëŒ€í•™ AI ë©˜í† ì˜ ìµœì¢… ê²€ìˆ˜ ì—ì´ì „íŠ¸ì´ë‹¤. ì•„ë˜ ì§ˆì˜ì™€ ì„ì‹œ ë‹µë³€ì„ ê²€í† í•´ë¼.

            ê²€í†  ê¸°ì¤€:
            - ë‹µë³€ì´ ì§ˆì˜ ì˜ë„ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨
            - ì‚¬ì‹¤ê³¼ ë¬´ê´€í•œ ì¼ë°˜ì ì¸ ì¡ì„¤ì´ë‚˜ í™˜ê° ë¶€ë¶„ ì ë°œ
            - ë¶ˆí•„ìš”í•œ ì¥í™©í•¨ì„ ì¤„ì´ê³  í•µì‹¬ë§Œ ë‚¨ê²¨ ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ì¬ì •ë¦¬
            - ê°•ì˜/ê³¼ëª© ì¶”ì²œì´ë©´ ë²ˆí˜¸ ëª©ë¡ê³¼ í•™ê³¼/ìš”ì•½ì„ ê°„ê²°íˆ í¬í•¨
            - ì „ë¶ëŒ€í•™êµ ë§¥ë½ì— ë§ëŠ” ë‹µë³€ìœ¼ë¡œ ì •ì œ

            ì…ë ¥ ì§ˆì˜: "{user_text}"

            ì„ì‹œ ë‹µë³€:
            {draft_answer}

            ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥:
            {{
            "is_aligned": true/false,
            "quality_score": 0.0~1.0,
            "issues": ["ë°œê²¬ëœ ë¬¸ì œì ë“¤"],
            "final_answer": "í•œêµ­ì–´ë¡œ ì •ë¦¬ëœ ìµœì¢… ë‹µë³€",
            "notes": "ê°„ë‹¨ ë©”ëª¨"
            }}
            """

        try:
            raw = await self.llm_handler.run_chain("basic", prompt)
            # JSON ë¸”ë¡ ì¶”ì¶œ
            m = re.search(r"\{[\s\S]*\}", str(raw))
            if not m:
                return draft_answer
            data = json.loads(m.group())
            final = data.get("final_answer") or draft_answer
            return str(final)
        except Exception:
            return draft_answer

    async def _generate_response_node(self, state: AIMentorState) -> AIMentorState:
        """ì‘ë‹µ ìƒì„± ë…¸ë“œ - ì£¼ì…ëœ LLM í•¸ë“¤ëŸ¬ë¡œ ì‹¤ì œ ì‘ë‹µ ìƒì„± + ResponseJudge í’ˆì§ˆ ê²€ì¦"""
        logger.debug("LangGraph: ì‘ë‹µ ìƒì„± ë…¸ë“œ ì‹¤í–‰")
        last_message = state["messages"][-1] if state["messages"] else None
        if not last_message:
            logger.warning("LangGraph: ë©”ì‹œì§€ê°€ ì—†ì–´ ì‘ë‹µ ìƒì„± ë¶ˆê°€")
            return state

        user_text = str(getattr(last_message, 'content', ''))
        logger.info(f"ğŸ¤– LangGraph ì‘ë‹µ ìƒì„± ì‹œì‘: '{user_text[:50]}...'")

        # LLM í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ìµœì†Œ í´ë°±
        if self.llm_handler is None:
            logger.warning("LangGraph: LLM í•¸ë“¤ëŸ¬ ì—†ìŒ, ê¸°ë³¸ ì‘ë‹µ ìƒì„±")
            response_content = f"AI ë©˜í† : {user_text}ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            state["messages"].append(AIMessage(content=response_content))
            return state

        try:
            # ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            system_ctx = self._build_system_context(state)
            logger.debug(f"ğŸ“‹ ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {len(system_ctx)}ì")

            # LLMìœ¼ë¡œ ì´ˆì•ˆ ì‘ë‹µ ìƒì„±
            if state.get("last_sql_result") or state.get("last_vector_result"):
                logger.info("ğŸ” ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„± (SQL/Vector ê²°ê³¼ í¬í•¨)")
                draft = await self.llm_handler.run_chain("context_only", user_text, context=system_ctx)
            else:
                logger.info("ğŸ’­ ì¼ë°˜ ì»¨í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±")
                draft = await self.llm_handler.run_chain("context", user_text, context=system_ctx)

            logger.info(f"âœ… LLM ì´ˆì•ˆ ìƒì„± ì™„ë£Œ: {len(str(draft))}ì")

            # ResponseJudgeë¡œ í’ˆì§ˆ ê²€ì¦ ë° ì •ì œ
            logger.info("ğŸ” ResponseJudge í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
            final_text = await self._judge_and_refine(user_text, str(draft))

            if final_text != str(draft):
                logger.info("âœ¨ ResponseJudgeê°€ ì‘ë‹µì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤")
            else:
                logger.info("âœ… ResponseJudge ê²€ì¦ í†µê³¼ (ìˆ˜ì • ì—†ìŒ)")

            state["messages"].append(AIMessage(content=str(final_text)))
            logger.info(f"ğŸ¯ LangGraph ìµœì¢… ì‘ë‹µ ì™„ë£Œ: {len(final_text)}ì")
            return state

        except Exception as e:
            logger.error(f"âŒ LangGraph ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")

            # í´ë°± ì‘ë‹µ
            response_content = f"AI ë©˜í† : {user_text}ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            state["messages"].append(AIMessage(content=response_content))
            return state
    
    def get_state(self, session_id: str) -> ConversationState:
        """ì„¸ì…˜ ìƒíƒœ ì¡°íšŒ"""
        if session_id not in self.sessions:
            self.sessions[session_id] = self._create_new_state(session_id)
        
        return self.sessions[session_id]
    
    def _create_new_state(self, session_id: str) -> ConversationState:
        """ìƒˆ ì„¸ì…˜ ìƒíƒœ ìƒì„±"""
        now = datetime.now()
        return ConversationState(
            session_id=session_id,
            conversation_history=[],
            last_search_results=None,
            current_topic=None,
            last_query_analysis=None,
            metadata={},
            created_at=now,
            updated_at=now,
            last_sql_result="",
            last_vector_result="",
            entity_cache={
                'professors': {},
                'courses': {},
                'departments': {},
                'combos': {}
            }
        )
    
    def add_exchange(self, session_id: str, user_message: str, 
                    assistant_response: str, query_analysis: Dict[str, Any] = None,
                    search_results: List[Dict] = None,
                    sql_context: str = "",
                    vector_context: str = ""):
        """ëŒ€í™” êµí™˜ ì¶”ê°€"""
        state = self.get_state(session_id)
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        state["conversation_history"].append({
            "role": "user",
            "content": user_message,
            "timestamp": datetime.now().isoformat()
        })
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€
        state["conversation_history"].append({
            "role": "assistant", 
            "content": assistant_response,
            "timestamp": datetime.now().isoformat()
        })
        
        # LangChain Memoryì—ë„ ê¸°ë¡
        lc_hist = self._get_lc_history(session_id)
        try:
            if lc_hist is not None:
                from langchain_core.messages import HumanMessage, AIMessage
                lc_hist.add_message(HumanMessage(content=user_message))
                lc_hist.add_message(AIMessage(content=assistant_response))
        except Exception:
            pass

        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ
        if len(state["conversation_history"]) > self.max_history_length * 2:  # user + assistant ìŒ
            state["conversation_history"] = state["conversation_history"][-self.max_history_length * 2:]
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if query_analysis:
            state["last_query_analysis"] = query_analysis
            
            # í˜„ì¬ ì£¼ì œ ì—…ë°ì´íŠ¸
            category = query_analysis.get('category', '')
            owner_hint = query_analysis.get('owner_hint', '')
            if owner_hint:
                state["current_topic"] = f"{category}_{owner_hint}"
        
        # ê²€ìƒ‰ ê²°ê³¼ ì—…ë°ì´íŠ¸
        if search_results:
            state["last_search_results"] = search_results

        # ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì—…ë°ì´íŠ¸
        if sql_context:
            state["last_sql_result"] = sql_context
        if vector_context:
            state["last_vector_result"] = vector_context
        
        state["updated_at"] = datetime.now()
        
        logger.info(f"ì„¸ì…˜ {session_id}ì— ëŒ€í™” êµí™˜ ì¶”ê°€: {len(state['conversation_history'])}ê°œ ë©”ì‹œì§€")

    # ë‚´ë¶€ìš©: ë‹¨ì¼ ë©”ì‹œì§€ ì¶”ê°€ (ì—­í˜¸í™˜)
    def update_full_messages(self, session_id: str, messages: List[Dict[str, str]]) -> None:
        """
        ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸

        Args:
            session_id: ì„¸ì…˜ ID
            messages: ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/assistant", "content": "..."}]
        """
        if session_id not in self.sessions:
            self.sessions[session_id] = self._create_new_state(session_id)

        session = self.sessions[session_id]

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LangGraph ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        langchain_messages = []
        for msg in messages:
            if msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                langchain_messages.append(AIMessage(content=msg["content"]))
            elif msg["role"] == "system":
                langchain_messages.append(SystemMessage(content=msg["content"]))

        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        session['conversation_history'] = messages[-self.max_history_length:]
        session['updated_at'] = datetime.now()

        # LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ì—ì„œ í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ
        if self.llm_handler:
            self._extract_entities_with_llm(session_id, messages)

        logger.debug(f"ğŸ“ ì„¸ì…˜ {session_id}: {len(messages)} ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    def _extract_entities_with_llm(self, session_id: str, messages: List[Dict[str, str]]) -> None:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ë° ìºì‹±
        """
        try:
            # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
            recent_messages = messages[-5:] if len(messages) > 5 else messages

            extraction_prompt = {
                "role": "system",
                "content": """ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
                JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
                {
                    "professors": ["êµìˆ˜ëª…1", "êµìˆ˜ëª…2"],
                    "courses": ["ê³¼ëª©ëª…1", "ê³¼ëª©ëª…2"],
                    "departments": ["í•™ê³¼ëª…1", "í•™ê³¼ëª…2"]
                }"""
            }

            # ë™ê¸° í˜¸ì¶œì´ë¼ asyncio ì‚¬ìš©
            import asyncio
            loop = asyncio.new_event_loop()
            entities = loop.run_until_complete(
                self.llm_handler.chat_completion_json([extraction_prompt, *recent_messages])
            )
            loop.close()

            if isinstance(entities, str):
                entities = json.loads(entities)

            # ì—”í‹°í‹° ìºì‹œ ì—…ë°ì´íŠ¸
            session = self.sessions[session_id]
            session['entity_cache'].update({
                'professors': {name: {'updated_at': datetime.now()} for name in entities.get('professors', [])},
                'courses': {name: {'updated_at': datetime.now()} for name in entities.get('courses', [])},
                'departments': {name: {'updated_at': datetime.now()} for name in entities.get('departments', [])}
            })

            logger.debug(f"ğŸ“‹ ì—”í‹°í‹° ì¶”ì¶œ ì™„ë£Œ: {entities}")

        except Exception as e:
            logger.warning(f"ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")

    def _append_message(self, session_id: str, role: str, content: str) -> None:
        state = self.get_state(session_id)
        state["conversation_history"].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        # LangChain Memoryì—ë„ ê¸°ë¡
        lc_hist = self._get_lc_history(session_id)
        if lc_hist is not None:
            try:
                from langchain_core.messages import HumanMessage, AIMessage
                if role == 'user':
                    lc_hist.add_message(HumanMessage(content=content))
                else:
                    lc_hist.add_message(AIMessage(content=content))
            except Exception:
                pass
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ
        if len(state["conversation_history"]) > self.max_history_length * 2:
            state["conversation_history"] = state["conversation_history"][ - self.max_history_length * 2 : ]
        state["updated_at"] = datetime.now()
    
    def get_recent_context(self, session_id: str, max_turns: int = 5) -> List[Dict[str, str]]:
        """ìµœê·¼ Ní„´ì˜ ëŒ€í™” ë§¥ë½ ì¡°íšŒ"""
        state = self.get_state(session_id)
        history = state["conversation_history"]

        # ìµœê·¼ max_turns*2ê°œ ë©”ì‹œì§€ (user+assistant ìŒ)
        recent_messages = history[-(max_turns * 2):]
        return recent_messages

    def get_recent_exchanges(self, session_id: str, limit: int = 5) -> List[Dict[str, str]]:
        """ìµœê·¼ ëŒ€í™” êµí™˜ì„ ì‚¬ìš©ì-AI ìŒìœ¼ë¡œ ë°˜í™˜"""
        state = self.get_state(session_id)
        history = state["conversation_history"]

        if not history:
            return []

        exchanges = []
        i = 0
        while i < len(history) - 1 and len(exchanges) < limit:
            # ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ë‹¤ìŒ AI ë©”ì‹œì§€ë¥¼ ì°¾ê¸°
            if (history[i].get("role") == "user" and
                i + 1 < len(history) and
                history[i + 1].get("role") == "assistant"):

                exchange = {
                    "user_message": history[i].get("content", ""),
                    "ai_response": history[i + 1].get("content", ""),
                    "timestamp": history[i].get("timestamp", "")
                }
                exchanges.append(exchange)
                i += 2  # ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ ì´ë™
            else:
                i += 1

        return exchanges[-limit:] if exchanges else []

    # ===== LangGraph/LLM í˜¸í™˜ ë©”ì‹œì§€ í—¬í¼ =====
    def get_chat_messages(self, session_id: str, max_turns: int = 10) -> List[BaseMessage]:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ë¥¼ LangChain BaseMessage ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜

        Args:
            session_id: ì„¸ì…˜ ID
            max_turns: ìµœê·¼ ëª‡ í„´(user+assistant ìŒ ê¸°ì¤€)ì„ ì‚¬ìš©í• ì§€
        Returns:
            List[BaseMessage]: [HumanMessage/AIMessage ...]
        """
        # 1) LangChain Memoryì—ì„œ ì‹œë„
        lc_hist = self._get_lc_history(session_id)
        if lc_hist is not None:
            try:
                msgs = list(lc_hist.messages)
                if max_turns and max_turns > 0:
                    return msgs[-(max_turns * 2):]
                return msgs
            except Exception:
                pass

        # 2) í´ë°±: ë‚´ë¶€ ì„¸ì…˜ ì €ì¥ì†Œì—ì„œ ë³€í™˜
        recent = self.get_recent_context(session_id, max_turns=max_turns)
        messages: List[BaseMessage] = []
        for msg in recent:
            role = msg.get("role")
            content = msg.get("content", "")
            if role == "user":
                messages.append(HumanMessage(content=content))
            else:
                messages.append(AIMessage(content=content))
        return messages

    def append_chat_turn(self, session_id: str, user_text: str, ai_text: str) -> None:
        """(ì„ íƒ ì‚¬ìš©) í•œ í„´ì˜ ì‚¬ìš©ì/AI ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.add_exchange(
            session_id=session_id,
            user_message=user_text,
            assistant_response=ai_text,
            query_analysis=None,
            search_results=None,
            sql_context=self.get_last_contexts(session_id).get("sql", ""),
            vector_context=self.get_last_contexts(session_id).get("vector", "")
        )

    # LangChain RunnableWithMessageHistory í˜¸í™˜ ì–´ëŒ‘í„°
    class ConversationMessageHistory(BaseChatMessageHistory):  # type: ignore
        def __init__(self, memory: 'ConversationMemory', session_id: str, max_turns: int = 50):
            self._m = memory
            self._sid = session_id
            self._max = max_turns

        @property
        def messages(self) -> List[BaseMessage]:
            lc = self._m._get_lc_history(self._sid)
            if lc is not None:
                try:
                    msgs = list(lc.messages)
                    return msgs[-(self._max * 2):]
                except Exception:
                    pass
            return self._m.get_chat_messages(self._sid, max_turns=self._max)

        def add_message(self, message: BaseMessage) -> None:  # type: ignore[override]
            try:
                content = getattr(message, 'content', '')
                role = getattr(message, 'type', None) or getattr(message, 'role', '')

                # LangChain Memoryì—ë„ ê¸°ë¡
                lc = self._m._get_lc_history(self._sid)
                if lc is not None:
                    from langchain_core.messages import HumanMessage, AIMessage
                    if role in ('human', 'user'):
                        lc.add_message(HumanMessage(content=content))
                    else:
                        lc.add_message(AIMessage(content=content))

                # ë‚´ë¶€ ì„¸ì…˜ì—ë„ ê¸°ë¡
                if role in ('human', 'user'):
                    self._m._append_message(self._sid, 'user', content)
                else:
                    self._m._append_message(self._sid, 'assistant', content)
            except Exception:
                # ì•ˆì „í•˜ê²Œ ë¬´ì‹œ
                pass

        def add_messages(self, messages: List[BaseMessage]) -> None:
            """ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ í•œ ë²ˆì— ì¶”ê°€ (LangChain ìƒˆ ë²„ì „ í˜¸í™˜ì„±)"""
            try:
                for message in messages:
                    self.add_message(message)
            except Exception:
                # ì•ˆì „í•˜ê²Œ ë¬´ì‹œ
                pass

        def clear(self) -> None:  # type: ignore[override]
            # LangChain ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            lc = self._m._get_lc_history(self._sid)
            if lc is not None:
                try:
                    lc.clear()
                except Exception:
                    pass
            # ë‚´ë¶€ ì„¸ì…˜ ì´ˆê¸°í™”
            self._m.clear_session(self._sid)

    def get_history_adapter(self, session_id: str, max_turns: int = 50) -> 'ConversationMessageHistory':
        return ConversationMemory.ConversationMessageHistory(self, session_id, max_turns=max_turns)
    
    def get_contextual_prompt(self, session_id: str, current_query: str) -> str:
        """ë§¥ë½ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ContextBuilder ì‚¬ìš©)"""
        from utils.context_builder import ContextBuilder

        state = self.get_state(session_id)
        return ContextBuilder.build_contextual_prompt(
            conversation_history=state["conversation_history"],
            search_results=state["last_search_results"],
            current_topic=state["current_topic"],
            current_query=current_query,
            max_turns=3
        )
    
    def clear_session(self, session_id: str):
        """ì„¸ì…˜ ì´ˆê¸°í™”"""
        if session_id in self.sessions:
            del self.sessions[session_id]
            logger.info(f"ì„¸ì…˜ {session_id} ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_session_stats(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ í†µê³„ ì¡°íšŒ"""
        state = self.get_state(session_id)
        
        return {
            "session_id": session_id,
            "total_messages": len(state["conversation_history"]),
            "current_topic": state["current_topic"],
            "has_search_results": bool(state["last_search_results"]),
            "created_at": state["created_at"].isoformat() if state["created_at"] else None,
            "updated_at": state["updated_at"].isoformat() if state["updated_at"] else None,
            "has_last_sql": bool(state.get("last_sql_result")),
            "has_last_vector": bool(state.get("last_vector_result")),
        }

    def get_last_contexts(self, session_id: str) -> Dict[str, str]:
        state = self.get_state(session_id)
        return {
            "sql": state.get("last_sql_result", ""),
            "vector": state.get("last_vector_result", "")
        }

    def get_cache_summary(self, session_id: str) -> Dict[str, Any]:
        """ì—”í‹°í‹° ìºì‹œ ìš”ì•½ ë°˜í™˜(ê·¸ëŒ€ë¡œ ë°˜í™˜; í•„ìš”ì‹œ ì¶”í›„ ìš”ì•½/ìŠ¬ë¼ì´ìŠ¤)"""
        state = self.get_state(session_id)
        return state.get("entity_cache", {"professors": {}, "courses": {}, "departments": {}})

    # ===== LangGraph ì§„ì§œ ì‚¬ìš© ë©”ì„œë“œë“¤ =====
    async def process_with_langgraph(self, session_id: str, user_message: str, query_analysis: Optional[Dict[str, Any]] = None) -> Optional[str]:
        """LangGraph StateGraphë¥¼ ì‚¬ìš©í•œ ë©”ì‹œì§€ ì²˜ë¦¬

        Returns:
            str | None: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸(ì„±ê³µ ì‹œ) ë˜ëŠ” None(ì‹¤íŒ¨/ë¯¸ìƒì„±)
        """
        if not self.graph:
            logger.warning("LangGraph ë¯¸ì‚¬ìš©, ê¸°ë³¸ ì²˜ë¦¬ë¡œ í´ë°±")
            return None
        try:
            prior_msgs = self.get_chat_messages(session_id, max_turns=self.max_history_length)
            initial_state: AIMentorState = {
                "messages": [*prior_msgs, HumanMessage(content=user_message)],
                "session_id": session_id,
                "current_topic": None,
                "last_sql_result": self.get_last_contexts(session_id).get("sql", ""),
                "last_vector_result": self.get_last_contexts(session_id).get("vector", ""),
                "query_analysis": query_analysis or None,
                "search_results": None,
                "entity_cache": {},
                "metadata": {}
            }

            config = {"configurable": {"thread_id": session_id}}
            final_state = await self.graph.ainvoke(initial_state, config=config)

            ai_messages = [msg for msg in final_state["messages"] if isinstance(msg, AIMessage)]
            if ai_messages:
                response = str(ai_messages[-1].content)
                self._update_session_from_langgraph(session_id, final_state)
                return response
            return None
        except Exception as e:
            logger.error(f"LangGraph ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None



    def _update_session_from_langgraph(self, session_id: str, langgraph_state: AIMentorState):
        """LangGraph ìƒíƒœë¥¼ ê¸°ì¡´ ì„¸ì…˜ ìƒíƒœì— ë°˜ì˜"""
        state = self.get_state(session_id)

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        for msg in langgraph_state["messages"]:
            if isinstance(msg, HumanMessage):
                state["conversation_history"].append({
                    "role": "user",
                    "content": str(msg.content),
                    "timestamp": datetime.now().isoformat()
                })
            elif isinstance(msg, AIMessage):
                state["conversation_history"].append({
                    "role": "assistant",
                    "content": str(msg.content),
                    "timestamp": datetime.now().isoformat()
                })

        # ê¸°íƒ€ ìƒíƒœ ì—…ë°ì´íŠ¸
        if langgraph_state.get("current_topic"):
            state["current_topic"] = langgraph_state["current_topic"]
        if langgraph_state.get("last_sql_result"):
            state["last_sql_result"] = langgraph_state["last_sql_result"]
        if langgraph_state.get("last_vector_result"):
            state["last_vector_result"] = langgraph_state["last_vector_result"]

        state["updated_at"] = datetime.now()
        logger.debug(f"LangGraph ìƒíƒœë¥¼ ì„¸ì…˜ {session_id}ì— ë°˜ì˜ ì™„ë£Œ")

    def _fallback_processing(self, user_message: str) -> str:
        """LangGraph ì‹¤íŒ¨ì‹œ í´ë°± ì²˜ë¦¬"""
        return f"ê¸°ë³¸ ì‘ë‹µ: {user_message}ì— ëŒ€í•´ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤."

    async def stream_with_langgraph(self, session_id: str, user_message: str, query_analysis: Optional[Dict[str, Any]] = None):
        """LangGraphë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬"""
        if not self.graph:
            yield {"type": "content", "content": self._fallback_processing(user_message)}
            return

        try:
            prior_msgs = self.get_chat_messages(session_id, max_turns=self.max_history_length)
            initial_state: AIMentorState = {
                "messages": [*prior_msgs, HumanMessage(content=user_message)],
                "session_id": session_id,
                "current_topic": None,
                "last_sql_result": self.get_last_contexts(session_id).get("sql", ""),
                "last_vector_result": self.get_last_contexts(session_id).get("vector", ""),
                "query_analysis": query_analysis or None,
                "search_results": None,
                "entity_cache": {},
                "metadata": {}
            }

            config = {"configurable": {"thread_id": session_id}}

            # LangGraph ì‹¤í–‰ í›„, ìµœì¢… í•œ ë²ˆì— ìŠ¤íŠ¸ë¦¬ë°(ì •ì œëœ ê²°ê³¼)
            async for event in self.graph.astream(initial_state, config=config):
                if not (event and "generate_response" in event):
                    continue
                state = event["generate_response"]
                if not state.get("messages"):
                    continue
                last_msg = state["messages"][-1]
                if not isinstance(last_msg, AIMessage):
                    continue
                text = str(last_msg.content)
                # ì²­í¬ ìŠ¤íŠ¸ë¦¬ë°(ê°€ë²¼ìš´ ë¶„í• )
                for i in range(0, len(text), 200):
                    yield {"type": "content", "content": text[i:i+200]}
                break

        except Exception as e:
            logger.error(f"LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield {"type": "error", "content": f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"}

    # ToT í†µí•©ì„ ìœ„í•œ ì¶”ê°€ ë©”ì„œë“œë“¤
    def cache_action_result(self, session_id: str, action_type: str, query_key: str, result_data: Any, confidence: float = 0.8):
        """ì•¡ì…˜ ê²°ê³¼ ìºì‹œ (ToT í˜¸í™˜)"""
        state = self.get_state(session_id)

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"{action_type}_{hash(query_key) % 10000}"

        # ì—”í‹°í‹° ìºì‹œì— ì €ì¥
        if action_type.lower() in ['sql_query', 'sql']:
            state["last_sql_result"] = str(result_data)
        elif action_type.lower() in ['faiss_search', 'vector_search']:
            state["last_vector_result"] = str(result_data)

        # ë²”ìš© ìºì‹œì—ë„ ì €ì¥
        if "action_cache" not in state["metadata"]:
            state["metadata"]["action_cache"] = {}

        state["metadata"]["action_cache"][cache_key] = {
            "data": result_data,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat(),
            "action_type": action_type,
            "query_key": query_key
        }

        state["updated_at"] = datetime.now()
        logger.debug(f"ì•¡ì…˜ ê²°ê³¼ ìºì‹œ ì €ì¥: {action_type} -> {cache_key}")

    def get_cached_result(self, session_id: str, action_type: str, query_key: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ì•¡ì…˜ ê²°ê³¼ ì¡°íšŒ (ToT í˜¸í™˜)"""
        state = self.get_state(session_id)
        cache_key = f"{action_type}_{hash(query_key) % 10000}"

        action_cache = state["metadata"].get("action_cache", {})
        cached_item = action_cache.get(cache_key)

        if cached_item:
            # ìºì‹œ ë§Œë£Œ ì²´í¬ (24ì‹œê°„)
            cached_time = datetime.fromisoformat(cached_item["timestamp"])
            if (datetime.now() - cached_time).total_seconds() < 86400:  # 24ì‹œê°„
                logger.debug(f"ìºì‹œ íˆíŠ¸: {cache_key}")
                return cached_item
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del action_cache[cache_key]
                logger.debug(f"ë§Œë£Œëœ ìºì‹œ ì œê±°: {cache_key}")

        return None

    def add_decision_trace(self, session_id: str, decision: Dict[str, Any]):
        """ì˜ì‚¬ê²°ì • ì¶”ì  ê¸°ë¡ (ToT í˜¸í™˜)"""
        state = self.get_state(session_id)

        if "decision_trace" not in state["metadata"]:
            state["metadata"]["decision_trace"] = []

        decision_with_timestamp = {
            **decision,
            "timestamp": datetime.now().isoformat()
        }

        state["metadata"]["decision_trace"].append(decision_with_timestamp)

        # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
        if len(state["metadata"]["decision_trace"]) > 100:
            state["metadata"]["decision_trace"] = state["metadata"]["decision_trace"][-100:]

        state["updated_at"] = datetime.now()

    def get_session_statistics(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ í†µê³„ ì¡°íšŒ (ToT í˜¸í™˜)"""
        state = self.get_state(session_id)

        action_cache = state["metadata"].get("action_cache", {})
        decision_trace = state["metadata"].get("decision_trace", [])

        return {
            "session_id": session_id,
            "total_messages": len(state["conversation_history"]),
            "cached_actions": len(action_cache),
            "decisions_made": len(decision_trace),
            "session_duration_minutes": (datetime.now() - state["created_at"]).total_seconds() / 60,
            "last_activity": state["updated_at"].isoformat(),
            "current_topic": state["current_topic"],
            "has_sql_context": bool(state.get("last_sql_result")),
            "has_vector_context": bool(state.get("last_vector_result"))
        }

    def export_conversation(self, session_id: str) -> Dict[str, Any]:
        """ëŒ€í™” ì „ì²´ ë‚´ë³´ë‚´ê¸°"""
        state = self.get_state(session_id)

        return {
            "session_id": session_id,
            "conversation_history": state["conversation_history"],
            "metadata": state["metadata"],
            "statistics": self.get_session_statistics(session_id),
            "last_contexts": self.get_last_contexts(session_id),
            "exported_at": datetime.now().isoformat()
        }

    # --- Entity cache helpers ---
    def _norm_key(self, key: str) -> str:
        return (key or '').strip().lower()

    def cache_entity(self, session_id: str, kind: str, key: str, data: Any) -> None:
        state = self.get_state(session_id)
        ek = state["entity_cache"].setdefault(kind, {})
        ek[self._norm_key(key)] = {"data": data, "updated_at": datetime.now().isoformat()}
        state["updated_at"] = datetime.now()

    def get_entity(self, session_id: str, kind: str, key: str, ttl_sec: int = 600) -> Any:
        state = self.get_state(session_id)
        ek = state.get("entity_cache", {}).get(kind, {})
        entry = ek.get(self._norm_key(key))
        if not entry:
            return None
        try:
            ts = datetime.fromisoformat(entry.get("updated_at"))
            if (datetime.now() - ts).total_seconds() > ttl_sec:
                # expired
                del ek[self._norm_key(key)]
                return None
        except Exception:
            pass
        return entry.get("data")
    
