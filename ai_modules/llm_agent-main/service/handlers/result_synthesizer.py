import logging
from typing import Dict, Any
from utils.prompt_loader import load_prompt

logger = logging.getLogger(__name__)

class ResultSynthesizer:
    def __init__(self, llm_handler=None):
        self.llm_handler = llm_handler

    def set_llm_handler(self, llm_handler):
        """llm_handler ì„¤ì •"""
        self.llm_handler = llm_handler
        logger.info("âœ… ResultSynthesizerì— llm_handler ì„¤ì • ì™„ë£Œ")

    async def synthesize_with_llm(self, user_message: str, found_results: str,
                                 processing_type: str) -> str:
        """LLMì„ ì‚¬ìš©í•´ì„œ ì—ì´ì „íŠ¸ê°€ ì°¾ì€ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ìœ¼ë¡œ ì¢…í•© (ìŠ¤íŠ¸ë¦¬ë°)"""
        try:
            # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            synthesis_template = load_prompt("synthesis_prompt")
            synthesis_prompt = synthesis_template.format(
                user_message=user_message,
                found_results=found_results,
                processing_type=processing_type,
            )

            logger.info(f"ğŸš€ í•©ì„± ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë°): prompt_length={len(synthesis_prompt)}")

            # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ - LangChainì´ ìë™ìœ¼ë¡œ ì´ë²¤íŠ¸ ë°œìƒ
            if hasattr(self.llm_handler, 'chat_stream'):
                full_response = ""
                async for chunk in self.llm_handler.chat_stream(synthesis_prompt):
                    full_response += chunk

                if full_response.strip():
                    logger.info(f"âœ… í•©ì„± ì„±ê³µ: result_length={len(full_response)}")
                    return full_response.strip()
                else:
                    logger.warning("âš ï¸ í•©ì„± ê²°ê³¼ ë¹„ì–´ìˆìŒ")
                    return found_results
            else:
                logger.warning("âš ï¸ chat_stream ë¯¸ì§€ì›, ì¼ë°˜ í˜¸ì¶œ ì‚¬ìš©")
                synthesized = await self.llm_handler.chat(synthesis_prompt)
                return synthesized.strip() if synthesized.strip() else found_results

        except Exception as e:
            logger.error(f"âŒ í•©ì„± ì˜¤ë¥˜: {e}")
            return found_results

    async def synthesize_with_llm_stream(self, user_message: str, found_results: str,
                                    processing_type: str):
        """ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ LLM í•©ì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            synthesis_template = load_prompt("synthesis_prompt")
            synthesis_prompt = synthesis_template.format(
                user_message=user_message,
                found_results=found_results,
                processing_type=processing_type,
            )
            
            logger.info(f"ğŸš€ ìŠ¤íŠ¸ë¦¬ë° í•©ì„± ì‹œì‘: prompt_length={len(synthesis_prompt)}")
            
            # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ LLM í˜¸ì¶œ
            if hasattr(self.llm_handler, 'chat_stream'):
                async for chunk in self.llm_handler.chat_stream(synthesis_prompt):
                    yield chunk
            else:
                # fallback: ì¼ë°˜ í˜¸ì¶œ
                result = await self.llm_handler.chat(synthesis_prompt)
                yield result
                
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° í•©ì„± ì˜¤ë¥˜: {e}")
            yield found_results

