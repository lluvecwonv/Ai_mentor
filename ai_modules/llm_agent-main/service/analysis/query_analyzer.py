"""
Query ë³µì¡ë„ ë¶„ì„ Agent - LangChain ë²„ì „ (v3)
- LLM ë¦¬ì¦ˆë‹ + ê·œì¹™ ê²°í•© ë¼ìš°íŒ…
- ì—”í‹°í‹° ì¶”ì¶œ(ê³¼ëª©/í•™ê³¼/í–‰ë™ ì˜ë„), íŒŒì´í”„ë¼ì¸ í”Œëœ ìƒì„±
- ì¿¼ë¦¬ í™•ì¥(expansion_context / expansion_keywords / augmentation í•œ ì¤„)

ì£¼ì˜: ì´ ëª¨ë“ˆì€ LlmClientLangChain(chat_completion)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""
from __future__ import annotations

import logging
from typing import Dict, Any
import asyncio

# LLM í´ë¼ì´ì–¸íŠ¸ ë‹¨ìˆœí™”
from utils.llm_client_langchain import LlmClientLangChain as LlmClientLangChainAdvanced

# ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from utils.prompt_loader import load_prompt
from utils.json_utils import extract_json_block, to_router_decision


# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ì½˜ì†”/íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°)
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    try:
        from pathlib import Path
        logs_dir = Path('/home/dbs0510/AiMentor_edit/ai_modules/llm_agent-main/logs')
        logs_dir.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(str(logs_dir / 'llm-agent.log'), encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    except Exception as _e:
        logger.warning(f"íŒŒì¼ ë¡œê¹… ì„¤ì • ì‹¤íŒ¨: {_e}")

"""
ë°ì´í„° ëª¨ë¸(dataclass)ì€ ì œê±°í–ˆìŠµë‹ˆë‹¤. ë¼ìš°íŒ… ê²°ì • ì •ê·œí™”ëŠ” utils.json_utils.to_router_decision ì‚¬ìš©.
"""


class QueryAnalyzer:
    """Query ë³µì¡ë„ ë¶„ì„ ë° ë¶„ë¥˜ - LangChain LLM + ê·œì¹™ ê²°í•© (v3)"""

    def __init__(self, conversation_memory=None):
        logger.info("QueryAnalyzer ì´ˆê¸°í™” ì‹œì‘")
        self.llm_client = LlmClientLangChainAdvanced()
        self.conversation_memory = conversation_memory
        logger.debug("LangChain Query Analyzer(v3) ì´ˆê¸°í™” ì™„ë£Œ")

    async def analyze_query_parallel(self, query: str, session_id: str = "default", contextual_prompt: str = None) -> Dict[str, Any]:
        """ë³‘ë ¬ ì¿¼ë¦¬ ë¶„ì„ + í™•ì¥ - í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ì— í™œìš©"""
        _qprev = (query[:200] + '...') if len(query) > 200 else query
        logger.debug(f"ë³‘ë ¬ ì¿¼ë¦¬ ë¶„ì„ ì‹œì‘: '{_qprev}'")

        # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        history_context = self._get_history_context(session_id)
        if history_context:
            logger.info(f"íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ í™œìš©: {len(history_context)} ë¬¸ì")
        else:
            logger.debug("íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ")

        # 1ë‹¨ê³„: ë¨¼ì € ì¿¼ë¦¬ í™•ì¥ì„ ìˆ˜í–‰
        try:
            expansion_result = await self._expand_query_async(query, history_context)
            logger.info(f"ğŸ” ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ: context='{expansion_result.get('expansion_context', '')[:50]}...', keywords='{expansion_result.get('expansion_keywords', '')}'")

            # 2ë‹¨ê³„: í™•ì¥ëœ ì •ë³´ë¥¼ ì¡°í•©í•˜ì—¬ í–¥ìƒëœ ì¿¼ë¦¬ ìƒì„±
            enhanced_query = self._combine_expansion_with_query(query, expansion_result)
            logger.info(f"ğŸ”— í™•ì¥ ì •ë³´ê°€ ì¡°í•©ëœ í–¥ìƒëœ ì¿¼ë¦¬: '{enhanced_query}'")

            # 3ë‹¨ê³„: í–¥ìƒëœ ì¿¼ë¦¬ë¡œ ë¼ìš°íŒ… ë¶„ì„ ìˆ˜í–‰
            analysis_result = await self._analyze_routing_async(enhanced_query, contextual_prompt, history_context)

        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
            analysis_result = {
                "decision_question_type": "general",
                "decision_data_source": "llm",
                "complexity": "medium",
                "owner_hint": "llm-agent",
                "plan": ["ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬"],
                "reasoning": "ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì²˜ë¦¬"
            }
            expansion_result = {
                "expansion_context": "",
                "expansion_keywords": ""
            }

        # ë‘ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ë˜, ì›ë³¸ ì¿¼ë¦¬ë„ ë³´ì¡´
        combined_result = {
            **analysis_result,
            **expansion_result,
            "original_query": query,  # ì›ë³¸ ì¿¼ë¦¬ ë³´ì¡´
            "enhanced_query": enhanced_query if 'enhanced_query' in locals() else query,  # í–¥ìƒëœ ì¿¼ë¦¬
            "analysis_method": "parallel_v3_enhanced",
            "analyzer_type": "LangChain_Parallel",
            "has_context": bool(contextual_prompt),
        }

        logger.info(
            f"ğŸ“Š í–¥ìƒëœ ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {analysis_result.get('category')} ë³µì¡ë„, {analysis_result.get('owner_hint')} ë‹´ë‹¹"
        )
        return combined_result

    # ë™ê¸° ë¶„ì„/ì§ì ‘ ì‘ë‹µ ìƒì„± ë©”ì„œë“œëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì•„ ì œê±°í–ˆìŠµë‹ˆë‹¤.

    def get_performance_stats(self) -> dict:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ - ê°„ë‹¨í•œ ë²„ì „"""
        llm_stats = self.llm_client.get_performance_stats() if hasattr(self.llm_client, 'get_performance_stats') else {}

        # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„± ìœ ì§€
        legacy_stats = {
            "analyzer_type": "LangChain_v3",
            "total_analyses": 0,
            "total_analysis_time": 0.0,
            "avg_analysis_time": 0.0,
            "success_rate": 100.0,
            "llm_stats": llm_stats,
        }

        return legacy_stats


    # Parsing & Post-rulesëŠ” utils.json_utils.to_router_decision ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.

    # ë™ê¸° expand_queryëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì•„ ì œê±°í–ˆìŠµë‹ˆë‹¤. ë¹„ë™ê¸° ë²„ì „ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.

    # -------- ë³‘ë ¬ ì²˜ë¦¬ìš© ë¹„ë™ê¸° ë©”ì„œë“œë“¤ ---------
    async def _analyze_routing_async(self, query: str, contextual_prompt: str = None, history_context: str = None) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë¼ìš°íŒ… ë¶„ì„ (íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)"""
        _qprev = (query + '...') 
        logger.debug(f"ğŸ” ë¼ìš°íŒ… ë¶„ì„ ì‹œì‘: '{_qprev}'")


        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ ë° ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
        router_prompt = load_prompt('router_prompt').replace('{query}', query)

        # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if history_context:
            history_section = f"\n\n### ì´ì „ ëŒ€í™” ë§¥ë½:\n{history_context}\n\n### í˜„ì¬ ì§ˆë¬¸ ë¶„ì„:"
            router_prompt = router_prompt.replace('{query}', f"{history_section}\nì‚¬ìš©ì ì§ˆë¬¸: {query}")
            logger.info("ğŸ“š íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ê°€ ë¼ìš°íŒ… ë¶„ì„ì— ì¶”ê°€ë¨")

        if contextual_prompt:
            router_prompt = f"{contextual_prompt}\n\n{router_prompt}"
            logger.info("ğŸ“ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì ìš©ë¨")

        logger.debug("ğŸ¤– LLM ë¼ìš°íŒ… ë¶„ì„ ìš”ì²­ ì‹œì‘")
        # LLM í˜¸ì¶œ - ë™ê¸°/ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ í˜¸í™˜ì„± ì²˜ë¦¬
        try:
            import asyncio
            if asyncio.iscoroutinefunction(self.llm_client.chat_completion):
                response = await self.llm_client.chat_completion(
                    messages=[{"role": "user", "content": router_prompt}],
                    model="gpt-4o-mini"
                )
            else:
                # ë™ê¸° ë©”ì„œë“œì¸ ê²½ìš° ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                response = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.llm_client.chat_completion(
                        messages=[{"role": "user", "content": router_prompt}],
                        model="gpt-4o-mini"
                    )
                )
        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise
        logger.info(f"âœ… LLM ë¼ìš°íŒ… ì‘ë‹µ ë°›ìŒ: {len(response)}ì")
        logger.info(f"ğŸ” LLM ì›ë³¸ ì‘ë‹µ: {response}")

        # ê²°ê³¼ íŒŒì‹± ë° ë³€í™˜
        data = extract_json_block(response) or {}
        logger.info(f"ğŸ“Š ì›ë³¸ ë¶„ì„ ë°ì´í„°: {data}")

        decision = to_router_decision(data)
        logger.info(f"ğŸ¯ ë¼ìš°íŒ… ê²°ì •: complexity={decision.get('complexity')}, agent={decision.get('owner_hint')}")
        logger.info(f"ğŸ“‹ ê³„íš: {decision.get('plan', [])}")
        reason = (decision.get('reasoning', '') or '')
        if reason:
            logger.info(f"REASON: {reason}")

        return decision


    async def _expand_query_async(self, query: str, history_context: str = None) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì¿¼ë¦¬ í™•ì¥ (íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)"""
        _qprev = (query + '...') 
        logger.debug(f"ğŸ” ì¿¼ë¦¬ í™•ì¥ ì‹œì‘: '{_qprev}'")

        # ì¿¼ë¦¬ í™•ì¥ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        expansion_prompt = load_prompt('query_reasoning_prompt').replace('{query}', query)

        # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if history_context:
            history_section = f"\n\n### ì´ì „ ëŒ€í™” ë§¥ë½:\n{history_context}\n\n### í˜„ì¬ ì§ˆë¬¸ í™•ì¥:"
            expansion_prompt = expansion_prompt.replace('{query}', f"{history_section}\nì‚¬ìš©ì ì§ˆë¬¸: {query}")
            logger.info("ğŸ“š íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ê°€ ì¿¼ë¦¬ í™•ì¥ì— ì¶”ê°€ë¨")
        logger.debug("ğŸ“ ì¿¼ë¦¬ í™•ì¥ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

        logger.debug("ğŸ¤– LLM ì¿¼ë¦¬ í™•ì¥ ìš”ì²­ ì‹œì‘")
        # LLM í˜¸ì¶œ - ë™ê¸°/ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ í˜¸í™˜ì„± ì²˜ë¦¬
        try:
            import asyncio
            if asyncio.iscoroutinefunction(self.llm_client.chat_completion):
                response = await self.llm_client.chat_completion(
                    messages=[{"role": "user", "content": expansion_prompt}],
                    model="gpt-4o-mini"
                )
            else:
                # ë™ê¸° ë©”ì„œë“œì¸ ê²½ìš° ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                response = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.llm_client.chat_completion(
                        messages=[{"role": "user", "content": expansion_prompt}],
                        model="gpt-4o-mini"
                    )
                )
        except Exception as e:
            logger.error(f"LLM í™•ì¥ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise
        logger.debug(f"âœ… LLM í™•ì¥ ì‘ë‹µ ë°›ìŒ: {len(response)}ì")

        # ê²°ê³¼ íŒŒì‹±
        expansion_data = extract_json_block(response) or {}
        logger.debug(f"ğŸ“Š í™•ì¥ ë°ì´í„°: context='{expansion_data.get('expansion_context', '')[:50]}...', keywords='{expansion_data.get('expansion_keywords', '')}'")

        # expanded_queries ìƒì„± (ë²¡í„° ê²€ìƒ‰ìš©)
        expansion_keywords = expansion_data.get("expansion_keywords", "")
        expanded_queries = None
        if expansion_keywords:
            keywords_list = [k.strip() for k in expansion_keywords.split(',') if k.strip()]
            if keywords_list:
                expanded_queries = [
                    {"text": query, "kind": "base"}  # ì›ë³¸ ì¿¼ë¦¬
                ] + [
                    {"text": kw, "kind": "keyword"} for kw in keywords_list
                ]

        result = {
            "expansion_context": expansion_data.get("expansion_context", ""),
            "expansion_keywords": expansion_keywords,
            "expansion_augmentation": expansion_data.get("expansion_augmentation", ""),
            "expanded_queries": expanded_queries,  # ë²¡í„° ê²€ìƒ‰ìš© í™•ì¥ ì¿¼ë¦¬
            "constraints": expansion_data.get("constraints", {}),
            "decision_question_type": expansion_data.get("decision_question_type", ""),
            "decision_data_source": expansion_data.get("decision_data_source", "")
        }
        logger.info(f"ğŸ¯ í™•ì¥ ì™„ë£Œ: question_type={result.get('decision_question_type')}, data_source={result.get('decision_data_source')}")

        return result

    def _get_history_context(self, session_id: str) -> str:
        """ì„¸ì…˜ì˜ ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìš”ì•½í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        if not self.conversation_memory:
            return ""

        try:
            # ìµœê·¼ 5ê°œ ëŒ€í™” êµí™˜ ê°€ì ¸ì˜¤ê¸°
            recent_exchanges = self.conversation_memory.get_recent_exchanges(session_id, limit=5)
            if not recent_exchanges:
                return ""

            # íˆìŠ¤í† ë¦¬ë¥¼ ìš”ì•½ í˜•íƒœë¡œ ë³€í™˜
            context_parts = []
            for exchange in recent_exchanges[-3:]:  # ìµœê·¼ 3ê°œë§Œ ì‚¬ìš©
                user_msg = exchange.get('user_message', '')
                ai_response = exchange.get('ai_response', '')

                if user_msg and ai_response:
                    # ì‘ë‹µì„ ê°„ë‹¨íˆ ìš”ì•½ (ì²˜ìŒ 100ìë§Œ)
                    ai_summary = ai_response[:100] + "..." if len(ai_response) > 100 else ai_response
                    context_parts.append(f"ì‚¬ìš©ì: {user_msg}")
                    context_parts.append(f"AI: {ai_summary}")

            if context_parts:
                history_context = "\n".join(context_parts)
                logger.debug(f"íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {len(history_context)}ì")
                return history_context

        except Exception as e:
            logger.warning(f"íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return ""

    def _combine_expansion_with_query(self, original_query: str, expansion_result: Dict[str, Any]) -> str:
        """í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œë¥¼ ì›ë³¸ ì¿¼ë¦¬ì™€ ì¡°í•©í•˜ì—¬ í–¥ìƒëœ ì¿¼ë¦¬ ìƒì„±"""
        expansion_context = expansion_result.get("expansion_context", "").strip()
        expansion_keywords = expansion_result.get("expansion_keywords", "").strip()

        # ì¡°í•©í•  ìš”ì†Œë“¤ ìˆ˜ì§‘
        enhancement_parts = []

        # í™•ì¥ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if expansion_context:
            enhancement_parts.append(f"ë°°ê²½ì •ë³´: {expansion_context}")

        # í™•ì¥ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if expansion_keywords:
            # í‚¤ì›Œë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ë¦¬
            keywords_list = [kw.strip() for kw in expansion_keywords.split(',') if kw.strip()]
            if keywords_list:
                enhancement_parts.append(f"ê´€ë ¨í‚¤ì›Œë“œ: {', '.join(keywords_list)}")

        # í–¥ìƒëœ ì¿¼ë¦¬ êµ¬ì„±
        if enhancement_parts:
            enhanced_query = f"{original_query}\n\n[í™•ì¥ì •ë³´] {' | '.join(enhancement_parts)}"
            logger.info(f"ğŸ”— [ì¿¼ë¦¬ì¡°í•©] ì›ë³¸: '{original_query[:50]}...' + í™•ì¥ì •ë³´ {len(enhancement_parts)}ê°œ")
            return enhanced_query
        else:
            logger.info("ğŸ”— [ì¿¼ë¦¬ì¡°í•©] í™•ì¥ì •ë³´ ì—†ìŒ, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")
            return original_query
