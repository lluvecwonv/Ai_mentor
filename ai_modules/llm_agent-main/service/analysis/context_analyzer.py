"""
LLM ê¸°ë°˜ ëŒ€í™” ë§¥ë½ ë¶„ì„ê¸° - ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš© ë°©ì‹ íŒë‹¨
"""

import logging
import json
from typing import Dict, List, Any
from pathlib import Path
from openai import OpenAI

logger = logging.getLogger(__name__)

class ConversationContextAnalyzer:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ì–´ë–»ê²Œ ì—°ê´€ë˜ëŠ”ì§€ ë¶„ì„í•˜ëŠ” ë¶„ì„ê¸°
    """

    def __init__(self, openai_client: OpenAI):
        self.llm_client = openai_client
        self.prompt_path = Path(__file__).parent.parent / "prompts" / "history_aware_query_analyzer.txt"
        self._load_prompt()
        logger.info("ConversationContextAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (LLM ëª¨ë“œ)")

    def _load_prompt(self):
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.prompt_path, 'r', encoding='utf-8') as f:
                self.system_prompt = f.read()
            logger.info("âœ… íˆìŠ¤í† ë¦¬ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.system_prompt = "í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ì–´ë–»ê²Œ ì—°ê´€ë˜ëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”."

    async def analyze_context_relevance(self, current_query: str, history: List[Dict]) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ íˆìŠ¤í† ë¦¬ í™œìš© ë°©ì‹ ë¶„ì„"""
        # íˆìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë©´ ë…ë¦½ì  ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        if not history or len(history) == 0:
            return {
                "history_usage": {
                    "reuse_previous": False,
                    "relationship": "new_search",
                    "context_integration": "ì´ì „ ëŒ€í™”ê°€ ì—†ì–´ ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰"
                }
            }

        return await self._llm_analyze_history_usage(current_query, history)

    async def _llm_analyze_history_usage(self, current_query: str, history: List[Dict]) -> Dict[str, Any]:
        """LLMìœ¼ë¡œ íˆìŠ¤í† ë¦¬ í™œìš© ë°©ì‹ ë¶„ì„"""
        try:
            # íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
            history_context = self._format_history_for_prompt(history)

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self.system_prompt.format(
                history_context=history_context,
                current_query=current_query
            )

            logger.info(f"ğŸ” [ContextAnalyzer] LLM íˆìŠ¤í† ë¦¬ ë¶„ì„ ì‹œì‘: '{current_query}'")

            response = self.llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )

            result_text = response.choices[0].message.content.strip()
            logger.info(f"ğŸ“ [ContextAnalyzer] LLM ì‘ë‹µ: {result_text}")

            # ê°•í™”ëœ JSON íŒŒì‹±
            return self._robust_json_parse(result_text)

        except Exception as e:
            logger.error(f"âŒ [ContextAnalyzer] LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_fallback_result("LLM ë¶„ì„ ì‹¤íŒ¨")

    def _format_history_for_prompt(self, history: List[Dict]) -> str:
        """íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_history = []

        for entry in history[-3:]:  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ì‚¬ìš©
            user_query = entry.get('user_query', '')
            ai_response = entry.get('ai_response', '')

            formatted_history.append(f"ì‚¬ìš©ì: {user_query}")
            if ai_response:
                # AI ì‘ë‹µì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ (ì²« 100ì)
                response_summary = ai_response[:100] + "..." if len(ai_response) > 100 else ai_response
                formatted_history.append(f"AI: {response_summary}")

        return " | ".join(formatted_history)

    def _robust_json_parse(self, result_text: str) -> Dict[str, Any]:
        """ê°•í™”ëœ JSON íŒŒì‹± ë¡œì§ - ë‹¤ì–‘í•œ í˜•íƒœì˜ JSON ì‘ë‹µ ì²˜ë¦¬"""
        import re

        # 1ì°¨: ì§ì ‘ JSON íŒŒì‹± ì‹œë„
        try:
            result = json.loads(result_text)
            logger.info(f"âœ… [JSON Parser] ì§ì ‘ íŒŒì‹± ì„±ê³µ: {result}")
            return result
        except json.JSONDecodeError as e:
            logger.debug(f"âš ï¸ [JSON Parser] ì§ì ‘ íŒŒì‹± ì‹¤íŒ¨: {e}")

        # 2ì°¨: ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
        json_patterns = [
            r'```json\s*(\{.*?\})\s*```',  # ```json { ... } ```
            r'```\s*(\{.*?\})\s*```',      # ``` { ... } ```
            r'(\{[^{}]*"history_usage"[^{}]*\})',  # history_usageê°€ í¬í•¨ëœ JSON (ê°„ë‹¨/ì•ˆì „)
            r'(\{.*?\})',                  # ì¼ë°˜ JSON ê°ì²´
        ]

        for i, pattern in enumerate(json_patterns, 1):
            try:
                matches = re.findall(pattern, result_text, re.DOTALL | re.MULTILINE)
                if matches:
                    # ê°€ì¥ ê¸´ ë§¤ì¹˜ë¥¼ ì„ íƒ (ë” ì™„ì „í•œ JSONì¼ ê°€ëŠ¥ì„±)
                    json_text = max(matches, key=len).strip()
                    result = json.loads(json_text)
                    logger.debug(f"âœ… [JSON Parser] íŒ¨í„´ {i} íŒŒì‹± ì„±ê³µ")
                    return result
            except (json.JSONDecodeError, Exception) as e:
                logger.debug(f"âš ï¸ [JSON Parser] íŒ¨í„´ {i} ì‹¤íŒ¨: {e}")
                continue

        # 3ì°¨: ë¶€ë¶„ JSON ë³µêµ¬ ì‹œë„
        try:
            # "history_usage" í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ê¸°ë³¸ êµ¬ì¡° ìƒì„±
            if "history_usage" in result_text.lower():
                logger.debug("ğŸ”§ [JSON Parser] history_usage í‚¤ ë°œê²¬, ë¶€ë¶„ ë³µêµ¬ ì‹œë„")

                # ê¸°ë³¸ êµ¬ì¡°ë¡œ ì‹œì‘
                fallback_result = {
                    "needs_context": False,
                    "confidence": 0.5,
                    "history_usage": {
                        "reuse_previous": False,
                        "relationship": "parsing_error",
                        "context_integration": "JSON íŒŒì‹± ì˜¤ë¥˜ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"
                    }
                }

                # ê°„ë‹¨í•œ íŒ¨í„´ìœ¼ë¡œ ì¼ë¶€ ê°’ ì¶”ì¶œ ì‹œë„
                if "true" in result_text.lower():
                    fallback_result["needs_context"] = True
                    fallback_result["history_usage"]["reuse_previous"] = True

                logger.info("ğŸ”§ [JSON Parser] ë¶€ë¶„ ë³µêµ¬ ì™„ë£Œ")
                return fallback_result
        except Exception as e:
            logger.debug(f"âš ï¸ [JSON Parser] ë¶€ë¶„ ë³µêµ¬ ì‹¤íŒ¨: {e}")

        # 4ì°¨: ì™„ì „í•œ í´ë°±
        logger.warning("âŒ [JSON Parser] ëª¨ë“  íŒŒì‹± ì‹œë„ ì‹¤íŒ¨, ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜")
        return self._create_fallback_result(f"JSON íŒŒì‹± ì‹¤íŒ¨: {result_text[:100]}...")

    def _create_fallback_result(self, reason: str) -> Dict[str, Any]:
        """ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜"""
        return {
            "history_usage": {
                "reuse_previous": False,
                "relationship": "new_search",
                "context_integration": f"ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•œ ìƒˆë¡œìš´ ê²€ìƒ‰: {reason}"
            }
        }

    def should_include_history(self, analysis_result: Dict[str, Any]) -> bool:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ í¬í•¨ ì—¬ë¶€ ê²°ì •"""
        history_usage = analysis_result.get("history_usage", {})
        return history_usage.get("reuse_previous", False)
