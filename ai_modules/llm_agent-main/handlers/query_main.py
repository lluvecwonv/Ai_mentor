
from __future__ import annotations

import logging
from typing import Dict, Any

from .llm_client_langchain import LlmClientLangChain
from .query_analyzer.analyzer import (
    analyze_routing_async,
    expand_query_async,
    combine_expansion_with_query
)

logger = logging.getLogger(__name__)

class QueryAnalyzer:
    """Query ë³µì¡ë„ ë¶„ì„ ë° ë¶„ë¥˜ - LangChain LLM + ê·œì¹™ ê²°í•© (v3)"""

    def __init__(self, conversation_memory=None):
        self.llm_client = LlmClientLangChain()
        self.conversation_memory = conversation_memory

    async def analyze_query_parallel(self, query: str, session_id: str = "default", contextual_prompt: str = None) -> Dict[str, Any]:

        # ë©”ëª¨ë¦¬ì—ì„œ íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        history_context = ""
        if self.conversation_memory:
            history_context = self.conversation_memory.get_recent_context(session_id, max_turns=3)

        # ì¿¼ë¦¬ í™•ì¥
        expansion_result = await expand_query_async(self.llm_client, query, history_context)
        enhanced_query = combine_expansion_with_query(query, expansion_result)
        logger.info(f"ğŸ”— í™•ì¥ ì •ë³´ê°€ ì¡°í•©ëœ í–¥ìƒëœ ì¿¼ë¦¬: '{enhanced_query}'")

        # í–¥ìƒëœ ì¿¼ë¦¬ë¡œ ë¼ìš°íŒ… ë¶„ì„ ìˆ˜í–‰
        analysis_result = await analyze_routing_async(self.llm_client, enhanced_query, contextual_prompt, history_context)

        # ë‘ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ë˜, ì›ë³¸ ì¿¼ë¦¬ë„ ë³´ì¡´
        combined_result = {
            **analysis_result,
            **expansion_result,
            "original_query": query,  # ì›ë³¸ ì¿¼ë¦¬ ë³´ì¡´
            "enhanced_query": enhanced_query if 'enhanced_query' in locals() else query,  # í–¥ìƒëœ ì¿¼ë¦¬
            "analysis_method": "parallel_v3_enhanced",
            "analyzer_type": "LangChain_Parallel",
            "has_context": bool(contextual_prompt),
        }
        return combined_result
