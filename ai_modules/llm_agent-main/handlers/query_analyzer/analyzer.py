import logging
from typing import Dict, Any
from utils.prompt_loader import load_prompt
from utils.json_utils import extract_json_block, to_router_decision
    # LLM í˜¸ì¶œ
import asyncio
logger = logging.getLogger(__name__)



async def analyze_routing_async(llm_client, query: str, contextual_prompt: str = None, history_context: str = None) -> Dict[str, Any]:
    """ë¼ìš°íŒ… ë¶„ì„ - ì¬êµ¬ì„±ëœ ì§ˆë¬¸ë§Œ ì‚¬ìš© (íˆìŠ¤í† ë¦¬ ì œê±°)"""

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì¬êµ¬ì„±ëœ ì§ˆë¬¸ë§Œ ì‚¬ìš©
    router_prompt = load_prompt('router_prompt').replace('{query}', query)

    # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë§Œ ì ìš© (íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ëŠ” ë¼ìš°íŒ…ì—ì„œ ì œì™¸)
    if contextual_prompt:
        router_prompt = f"{contextual_prompt}\n\n{router_prompt}"

    response = await llm_client.chat(router_prompt)
    logger.info(f"ğŸ” [DEBUG] ë¼ìš°í„° ì‘ë‹µ: {response}")

    data = extract_json_block(response)

    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
    if not data:
        logger.warning("ğŸ”„ JSON íŒŒì‹± ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘...")
        retry_prompt = router_prompt + "\n\n**CRITICAL: Your previous response was not valid JSON. Please respond ONLY with JSON starting with { and ending with }.**"
        response = await llm_client.chat(retry_prompt, json_mode=True)
        logger.info(f"ğŸ” [RETRY] ë¼ìš°í„° ì¬ì‹œë„ ì‘ë‹µ: {response}")
        data = extract_json_block(response) or {}

    logger.info(f"ğŸ” [DEBUG] ì¶”ì¶œëœ JSON: {data}")
    decision = to_router_decision(data)
    logger.info(f"ğŸ” [DEBUG] ìµœì¢… ê²°ì •: {decision}")

    logger.info(f"ğŸ¯ ë¼ìš°íŒ… ê²°ì •: {decision.get('complexity')}")
    return decision


async def expand_query_async(llm_client, query: str, history_context: str = None) -> Dict[str, Any]:
    """ì¿¼ë¦¬ í™•ì¥ - ê°„ë‹¨ ë²„ì „"""

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    expansion_prompt = load_prompt('query_reasoning_prompt').replace('{query}', query)

    # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if history_context:
        expansion_prompt = expansion_prompt.replace('{query}',
            f"\n\n### ì´ì „ ëŒ€í™”:\n{history_context}\n\n### í˜„ì¬ ì§ˆë¬¸:\n{query}")

    response = await llm_client.chat(expansion_prompt)

    # ê²°ê³¼ íŒŒì‹±
    expansion_data = extract_json_block(response) or {}

    return {
        "expansion_context": expansion_data.get("expansion_context", ""),
        "expansion_keywords": expansion_data.get("expansion_keywords", ""),
        "expansion_augmentation": expansion_data.get("expansion_augmentation", ""),
        "decision_question_type": expansion_data.get("decision_question_type", ""),
        "decision_data_source": expansion_data.get("decision_data_source", "")
    }


def combine_expansion_with_query(original_query: str, expansion_result: Dict[str, Any]) -> str:
        """í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œë¥¼ ì›ë³¸ ì¿¼ë¦¬ì™€ ì¡°í•©í•˜ì—¬ í–¥ìƒëœ ì¿¼ë¦¬ ìƒì„±"""
        expansion_context = expansion_result.get("expansion_context", "").strip()
        expansion_keywords = expansion_result.get("expansion_keywords", "").strip()

        # ì¡°í•©í•  ìš”ì†Œë“¤ ìˆ˜ì§‘
        enhancement_parts = []

        # í™•ì¥ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if expansion_context:
            enhancement_parts.append(f"ë°°ê²½ì •ë³´: {expansion_context}")

        # í™•ì¥ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if expansion_keywords:
            # í‚¤ì›Œë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ë¦¬
            keywords_list = [kw.strip() for kw in expansion_keywords.split(',') if kw.strip()]
            if keywords_list:
                enhancement_parts.append(f"ê´€ë ¨í‚¤ì›Œë“œ: {', '.join(keywords_list)}")

        # í–¥ìƒëœ ì¿¼ë¦¬ êµ¬ì„±
        if enhancement_parts:
            enhanced_query = f"{original_query}\n\n[í™•ì¥ì •ë³´] {' | '.join(enhancement_parts)}"
            logger.info(f"ğŸ”— [ì¿¼ë¦¬ì¡°í•©] ì›ë³¸: '{original_query}'")
            return enhanced_query
        else:
            logger.info("ğŸ”— [ì¿¼ë¦¬ì¡°í•©] í™•ì¥ì •ë³´ ì—†ìŒ, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")
            return original_query
