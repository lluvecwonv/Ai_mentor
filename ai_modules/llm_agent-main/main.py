import uvicorn
import re
import logging
import logging.config
from pathlib import Path
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from config.settings import settings, LOGGING_CONFIG
from exceptions import AIMentorException, ValidationError
from controller.agentController import router as agent_router

# ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
log_dir = Path("/home/dbs0510/AiMentor_edit/ai_modules/llm_agent-main/logs")
log_dir.mkdir(exist_ok=True)

# ì»¤ìŠ¤í…€ ë¡œê¹… ì„¤ì • ì ìš©
logging.config.dictConfig(LOGGING_CONFIG)

logger = logging.getLogger(__name__)

# ì„œë²„ ì‹œì‘ ì‹œ ë¡œê·¸ ê¸°ë¡
logger.info("ğŸš€ AI Mentor Service ì‹œì‘ - ë¡œê·¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
logger.debug(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}")
logger.info("=" * 50)

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”)
global_mentor_service = None

@asynccontextmanager
async def lifespan(_: FastAPI):
    """ì„œë²„ ì‹œì‘ ë° ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    # Startup
    global global_mentor_service
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘ - í•¸ë“¤ëŸ¬ ì›Œë°ì—… ì‹œì‘")

    try:
        from service.core.mentor_service import HybridMentorService

        # ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í†µí•© LangGraph ëª¨ë“œë¡œ ì´ˆê¸°í™”)
        global_mentor_service = HybridMentorService(use_unified_langgraph=True)

        # ì›Œë°ì—… ì™„ë£Œ í™•ì¸
        warmup_status = global_mentor_service.get_health_status()

        if warmup_status.get("status") == "healthy":
            logger.info("âœ… ì„œë²„ ì‹œì‘ - ëª¨ë“  í•¸ë“¤ëŸ¬ ì›Œë°ì—… ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ ì›Œë°ì—… ë¶€ë¶„ ì‹¤íŒ¨: {warmup_status}")

    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
        global_mentor_service = None

    yield

    # Shutdown
    logger.info("ì„œë²„ ì¢…ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI Mentor Service",
    description="LangGraph ê¸°ë°˜ ì§€ëŠ¥í˜• AI ë©˜í† ë§ ì‹œìŠ¤í…œ",
    version="3.0-improved",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(AIMentorException)
async def ai_mentor_exception_handler(_: Request, exc: AIMentorException):
    logger.error(f"AI ë©˜í†  ì˜ˆì™¸: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": str(exc),
            "error_code": "AI_MENTOR_ERROR",
            "timestamp": "2024-01-01T00:00:00Z"
        }
    )

@app.exception_handler(ValidationError)
async def validation_exception_handler(_: Request, exc: ValidationError):
    logger.error(f"ê²€ì¦ ì˜ˆì™¸: {exc}")
    return JSONResponse(
        status_code=400,
        content={
            "error": str(exc),
            "error_code": "VALIDATION_ERROR",
            "timestamp": "2024-01-01T00:00:00Z"
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(_: Request, exc: Exception):
    logger.error(f"ì¼ë°˜ ì˜ˆì™¸: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "error_code": "INTERNAL_ERROR",
            "timestamp": "2024-01-01T00:00:00Z"
        }
    )

# ë¼ìš°í„° ë“±ë¡
app.include_router(agent_router, prefix="/api/v2", tags=["AI Mentor"])

# OpenAI API í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ (Open WebUIìš©)
@app.get("/v1/models")
async def get_models():
    """OpenAI API í˜¸í™˜ ëª¨ë¸ ëª©ë¡"""
    return {
        "object": "list",
        "data": [
            {
                "id": "ai-mentor",
                "object": "model",
                "created": 1672531200,
                "owned_by": "ai-mentor",
                "permission": [],
                "root": "ai-mentor",
                "parent": None
            }
        ]
    }

# Compatibility aliases for OpenWebUI which may call baseURL/models
@app.get("/models")
async def get_models_alias():
    return await get_models()

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """OpenAI API í˜¸í™˜ ì±„íŒ… ì™„ì„±"""
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = await request.json()
        messages = data.get("messages", [])
        
        # ë©”ì‹œì§€ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        user_message = ""
        for msg in messages:
            if msg.get("role") == "user":
                user_message += msg.get("content", "") + "\n"

        # OpenWebUIì˜ ë‚´ë¶€ íƒœìŠ¤í¬ ì²˜ë¦¬ (title, tags, follow-ups)
        text = (user_message or "").strip()
        if text:
            # íˆìŠ¤í† ë¦¬ ì¶”ì¶œ
            m = re.search(r"<chat_history>\s*(.*?)\s*</chat_history>", text, flags=re.DOTALL)
            chat_hist = m.group(1).strip() if m else text

            # Title ìƒì„± ìš”ì²­
            if "Generate a concise, 3-5 word title" in text or re.search(r'\{\s*"title"', text):
                title = "ğŸ¤– AI Topic" if any(k in chat_hist for k in ["ì¸ê³µì§€ëŠ¥", "AI"]) else "ğŸ’¬ Chat Summary"
                return {
                    "id": "chatcmpl-ai-mentor",
                    "object": "chat.completion",
                    "created": 1672531200,
                    "model": "ai-mentor",
                    "choices": [{"index": 0, "message": {"role": "assistant", "content": f'{{"title": "{title}"}}'}, "finish_reason": "stop"}],
                    "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                }

            # Tags ìƒì„± ìš”ì²­
            if "Generate 1-3 broad tags" in text or re.search(r'\{\s*"tags"', text):
                tags = ["Education", "Technology", "AI"] if "AI" in chat_hist or "ì¸ê³µì§€ëŠ¥" in chat_hist else ["General"]
                return {
                    "id": "chatcmpl-ai-mentor",
                    "object": "chat.completion",
                    "created": 1672531200,
                    "model": "ai-mentor",
                    "choices": [{"index": 0, "message": {"role": "assistant", "content": f'{{"tags": {tags}}}'.replace("'", '"')}, "finish_reason": "stop"}],
                    "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                }

            # Follow-ups ìƒì„± ìš”ì²­
            if "Suggest 3-5 relevant follow-up questions" in text or re.search(r'\{\s*"follow_ups"', text):
                followups = ["ì–´ë–¤ ê³¼ëª©ë¶€í„° ìˆ˜ê°•í•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œìš”?", "ì„ ìˆ˜ê³¼ëª©ì´ ìˆë‚˜ìš”?", "í”„ë¡œì íŠ¸ ê³¼ëª©ì´ ìˆë‚˜ìš”?"]
                return {
                    "id": "chatcmpl-ai-mentor",
                    "object": "chat.completion",
                    "created": 1672531200,
                    "model": "ai-mentor",
                    "choices": [{"index": 0, "message": {"role": "assistant", "content": f'{{"follow_ups": {followups}}}'.replace("'", '"')}, "finish_reason": "stop"}],
                    "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                }

        if not user_message.strip():
            return JSONResponse(
                status_code=400,
                content={"error": "No user message provided"}
            )
        
        # AI Mentor ì„œë¹„ìŠ¤ í˜¸ì¶œ (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìš°ì„  ì‚¬ìš©)
        global global_mentor_service
        
        if global_mentor_service is not None:
            # ì´ë¯¸ ì›Œë°ì—…ëœ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            service = global_mentor_service
        else:
            # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (fallback)
            logger.warning("ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
            from service.core.mentor_service import HybridMentorService
            service = HybridMentorService(use_unified_langgraph=True)
            global_mentor_service = service
        
        response = await service.run_agent(user_message.strip())
        
        # OpenAI í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ë³€í™˜
        if isinstance(response, dict) and "choices" in response:
            return response
        else:
            # ê¸°ë³¸ ì‘ë‹µ í˜•ì‹
            return {
                "id": "chatcmpl-ai-mentor",
                "object": "chat.completion",
                "created": 1672531200,
                "model": "ai-mentor",
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": str(response)
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                }
            }
            
    except Exception as e:
        logger.error(f"ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Internal server error: {str(e)}"}
        )

# Compatibility alias when base URL does not include /v1
@app.post("/chat/completions")
async def chat_completions_alias(request: Request):
    return await chat_completions(request)

# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "AI Mentor Service",
        "version": "3.0-improved",
        "status": "running",
        "docs": "/docs",
        "health": "/health",
        "main_api": "/api/v2/agent",
        "features": "/api/v2/langchain/features"
    }

# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    try:
        global global_mentor_service
        
        if global_mentor_service is not None:
            # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (ì´ë¯¸ ì›Œë°ì—…ë¨)
            health_status = global_mentor_service.get_health_status()
            # ì›Œë°ì—… ìƒíƒœ ì¶”ê°€
            health_status["prewarmed"] = True
        else:
            # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            from service.core.mentor_service import HybridMentorService
            service = HybridMentorService(use_unified_langgraph=True)
            health_status = service.get_health_status()
            health_status["prewarmed"] = False
            
        return health_status
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "prewarmed": False,
            "timestamp": "2024-01-01T00:00:00Z"
        }


if __name__ == "__main__":
    logger.info(f"AI Mentor Service v3.0-improved ì‹œì‘")
    logger.info(f"í¬íŠ¸: {settings.port}")
    logger.info(f"ë””ë²„ê·¸ ëª¨ë“œ: {settings.debug}")
    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘ ì‹œ ìë™ ì›Œë°ì—… í™œì„±í™”")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=settings.port,
        log_level=settings.log_level.lower(),
        reload=settings.debug,
        log_config=None,
        access_log=False
    )
