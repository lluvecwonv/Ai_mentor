# ğŸ›¡ï¸ LLM í´ë°± ì„œë¹„ìŠ¤ (tool_dumb-main)

AI ë©˜í†  ì‹œìŠ¤í…œì˜ **ì‹œìŠ¤í…œ ì•ˆì •ì„± ë³´ì¥**ì„ ìœ„í•œ í´ë°± ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì „ë¬¸ ì„œë¹„ìŠ¤(SQL, ë²¡í„° ê²€ìƒ‰, ë§¤í•‘ ë“±)ê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ **ê¸°ë³¸ì ì¸ LLM ì‘ë‹µ**ì„ ì œê³µí•˜ì—¬ ì‹œìŠ¤í…œ ì „ì²´ê°€ ë‹¤ìš´ë˜ì§€ ì•Šë„ë¡ í•˜ëŠ” **ì•ˆì „ë§ ì—­í• **ì„ í•©ë‹ˆë‹¤.

## ğŸ¯ ì„œë¹„ìŠ¤ ê°œìš”

### í•µì‹¬ ì—­í• 
- **í´ë°± ì„œë¹„ìŠ¤**: ì „ë¬¸ ì„œë¹„ìŠ¤ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ LLM ì‘ë‹µ ì œê³µ
- **ì‹œìŠ¤í…œ ì•ˆì •ì„±**: ì „ì²´ AI ë©˜í†  ì‹œìŠ¤í…œì˜ ìµœí›„ ë°©ì–´ì„ 
- **ê°„ë‹¨í•œ ì§ˆë¬¸ ì²˜ë¦¬**: ë³µì¡í•œ ì²˜ë¦¬ê°€ ë¶ˆí•„ìš”í•œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì‘ë‹µ
- **ì„œë¹„ìŠ¤ ê°€ìš©ì„±**: 24/7 ê¸°ë³¸ AI ë©˜í† ë§ ì„œë¹„ìŠ¤ ë³´ì¥

### ë™ì‘ ì›ë¦¬
```mermaid
graph TD
    A[ì‚¬ìš©ì ìš”ì²­] --> B[llm_agent-main]
    B --> C{ì „ë¬¸ ì„œë¹„ìŠ¤ ê°€ìš©?}
    C -->|Yes| D[SQL/ë²¡í„°/ë§¤í•‘ ì„œë¹„ìŠ¤]
    C -->|No| E[tool_dumb-main í´ë°±]
    E --> F[OpenAI GPT-4o-mini]
    F --> G[ê¸°ë³¸ AI ë©˜í†  ì‘ë‹µ]

    D -->|ì‹¤íŒ¨| E
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
tool_dumb-main/
â”œâ”€â”€ main.py                  # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 
â”œâ”€â”€ requirements.txt         # Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ .env                     # í™˜ê²½ ë³€ìˆ˜ (OpenAI API í‚¤ ë“±)
â”œâ”€â”€ Dockerfile              # Docker ì»¨í…Œì´ë„ˆ ì„¤ì •
â”œâ”€â”€
â”œâ”€â”€ controller/
â”‚   â””â”€â”€ controller.py        # FastAPI ë¼ìš°í„° ë° API ì—”ë“œí¬ì¸íŠ¸
â”œâ”€â”€
â”œâ”€â”€ service/
â”‚   â””â”€â”€ coreService.py       # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
â”œâ”€â”€
â””â”€â”€ util/
    â””â”€â”€ llmClient.py         # OpenAI API í´ë¼ì´ì–¸íŠ¸ ë˜í¼
```

## ğŸ”§ ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. í™˜ê²½ ì„¤ì •

#### í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ (.env íŒŒì¼)
```bash
# OpenAI API ì„¤ì •
OPENAI_API_KEY=your_openai_api_key_here

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
DB_HOST=your_db_host
DB_PASSWORD=your_db_password
```

#### íŒ¨í‚¤ì§€ ì˜ì¡´ì„± (requirements.txt)
```bash
fastapi        # ì›¹ í”„ë ˆì„ì›Œí¬
uvicorn        # ASGI ì„œë²„
pydantic       # ë°ì´í„° ê²€ì¦
openai         # OpenAI API í´ë¼ì´ì–¸íŠ¸
python-dotenv  # í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
```

### 2. ë¡œì»¬ ì‹¤í–‰

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ë˜ëŠ”
venv\Scripts\activate     # Windows

# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ì„¤ì •

# 4. ì„œë²„ ì‹¤í–‰
python main.py
```

### 3. Docker ì‹¤í–‰

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t tool-dumb-main .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name svc7998 \
  -p 7998:7998 \
  -e PORT=7998 \
  -e OPENAI_API_KEY=your_api_key \
  tool-dumb-main
```


### ì—”ë“œí¬ì¸íŠ¸

#### 1. í´ë°± ì—ì´ì „íŠ¸ API
```http
POST /agent
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "content": "ì „ë¶ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
    }
  ]
}
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
  "message": "ì „ë¶ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ëŠ” 1982ë…„ì— ì„¤ë¦½ë˜ì–´ í˜„ì¬ê¹Œì§€ ìš°ìˆ˜í•œ IT ì¸ì¬ë¥¼ ì–‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” êµìœ¡ ë¶„ì•¼ë¡œëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§, ì¸ê³µì§€ëŠ¥, ë°ì´í„°ë² ì´ìŠ¤, ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ë“±ì´ ìˆìŠµë‹ˆë‹¤..."
}
```

#### 2. í—¬ìŠ¤ ì²´í¬
```http
GET /health
```

**ì‘ë‹µ:**
```json
{
  "status": "healthy",
  "service": "llm-fallback",
  "port": 7998,
  "description": "LLM Fallback/General-Agent for system stability"
}
```

### ìš”ì²­/ì‘ë‹µ ëª¨ë¸

#### Message ëª¨ë¸
```python
class Message(BaseModel):
    role: str      # "user", "assistant", "system"
    content: str   # ë©”ì‹œì§€ ë‚´ìš©
```

#### RequestBody ëª¨ë¸
```python
class RequestBody(BaseModel):
    messages: List[Message]  # ëŒ€í™” íˆìŠ¤í† ë¦¬
```

## ğŸ—ï¸ ì½”ë“œ êµ¬ì¡° ìƒì„¸

### 1. main.py - ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 

```python
import uvicorn
from fastapi import FastAPI
from controller.controller import router as agent_router

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI Mentor Fallback Service",
    description="LLM Fallback/General-Agent for system stability",
    version="1.0"
)

# ë¼ìš°í„° ë“±ë¡
app.include_router(agent_router)

if __name__ == "__main__":
    # í¬íŠ¸ 7998ì—ì„œ ì„œë²„ ì‹¤í–‰
    uvicorn.run("main:app", host="0.0.0.0", port=7998)
```

**ì£¼ìš” ê¸°ëŠ¥:**
- FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
- ì»¨íŠ¸ë¡¤ëŸ¬ ë¼ìš°í„° ë“±ë¡
- í¬íŠ¸ 7998ì—ì„œ ì„œë²„ ì‹¤í–‰

### 2. controller/controller.py - API ë¼ìš°í„°

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List
import logging
from service.coreService import CoreService

router = APIRouter()
core_service = CoreService()

class Message(BaseModel):
    role: str
    content: str

class RequestBody(BaseModel):
    messages: List[Message]

@router.post("/agent")
async def agent_api(data: RequestBody):
    """í´ë°± ì—ì´ì „íŠ¸ API - ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•œ ê¸°ë³¸ LLM ì„œë¹„ìŠ¤"""
    try:
        logger.info(f"í´ë°± ì„œë¹„ìŠ¤ ìš”ì²­: {len(data.messages)}ê°œ ë©”ì‹œì§€")

        # ì„œë¹„ìŠ¤ ê³„ì¸µìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
        response = core_service.execute(data.messages)
        result = response.choices[0].message.content

        return {"message": result}
    except Exception as e:
        logger.error(f"í´ë°± ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í´ë°± ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {str(e)}")

@router.get("/health")
async def health_check():
    """í´ë°± ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "llm-fallback",
        "port": 7998,
        "description": "LLM Fallback/General-Agent for system stability"
    }
```

**ì£¼ìš” ê¸°ëŠ¥:**
- API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ (`/agent`, `/health`)
- ìš”ì²­ ë°ì´í„° ê²€ì¦ (Pydantic ëª¨ë¸)
- ì„œë¹„ìŠ¤ ê³„ì¸µ í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
- ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹…

**í•¨ìˆ˜ë³„ ìƒì„¸:**
- `agent_api()`: ë©”ì¸ í´ë°± ì²˜ë¦¬ API
  - **ì…ë ¥**: RequestBody (messages ë¦¬ìŠ¤íŠ¸)
  - **ì¶œë ¥**: {"message": "AI ì‘ë‹µ"}
  - **ì˜ˆì™¸**: HTTPException (500 ì˜¤ë¥˜)

- `health_check()`: ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
  - **ì…ë ¥**: ì—†ìŒ
  - **ì¶œë ¥**: ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´

### 3. service/coreService.py - í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§

```python
from util.llmClient import LlmClient
from typing import List
import logging

class CoreService:
    """í´ë°± ì—ì´ì „íŠ¸ ì½”ì–´ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.llmClient = LlmClient()
        logger.info("í´ë°± ì„œë¹„ìŠ¤ ì½”ì–´ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def execute(self, messages: List):
        """í´ë°± ì—ì´ì „íŠ¸ ì‹¤í–‰ - ê¸°ë³¸ì ì¸ LLM ì²˜ë¦¬"""
        logger.debug(f"í´ë°± ì„œë¹„ìŠ¤ ì‹¤í–‰: {len(messages)}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬")

        # ì „ë¶ëŒ€ AI ë©˜í†  ì‹œìŠ¤í…œ ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """
ë‹¹ì‹ ì€ ì „ë¶ëŒ€í•™êµ AI ë©˜í† ë§ ì‹œìŠ¤í…œì˜ í´ë°± ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ë¥¸ ì „ë¬¸ ì„œë¹„ìŠ¤ê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ ê¸°ë³¸ì ì¸ ë„ì›€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ëŒ€í™” ê¸°ë¡ ìš°ì„ : ì´ì „ ëŒ€í™”ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´ ê·¸ ì •ë³´ë¥¼ í™œìš©í•˜ì„¸ìš”.
2. ì¼ë°˜ ì§€ì‹ ë³´ì¡°: ëŒ€í™” ê¸°ë¡ì— ë‹µì´ ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ êµìœ¡ ì§€ì‹ìœ¼ë¡œ ë„ì›€ì„ ì œê³µí•˜ì„¸ìš”.
3. í•œêµ­ì–´ ì‘ë‹µ: ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.

ì „ë¬¸ ì„œë¹„ìŠ¤ê°€ ì •ìƒí™”ë˜ë©´ ë” ìƒì„¸í•œ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

        # Pydantic ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        message_dicts = [msg.dict() for msg in messages]

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        request_messages = [
            {"role": "system", "content": system_prompt},
            *message_dicts
        ]

        try:
            # OpenAI API í˜¸ì¶œ
            response = self.llmClient.call_llm(request_messages)
            logger.debug("í´ë°± ì„œë¹„ìŠ¤ LLM ì‘ë‹µ ìƒì„± ì„±ê³µ")
            return response
        except Exception as e:
            logger.error(f"í´ë°± ì„œë¹„ìŠ¤ LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            raise
```

**ì£¼ìš” ê¸°ëŠ¥:**
- LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ê´€ë¦¬
- ì „ë¶ëŒ€ AI ë©˜í†  ì‹œìŠ¤í…œ ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
- ë©”ì‹œì§€ í¬ë§· ë³€í™˜ (Pydantic â†’ Dict)
- OpenAI API í˜¸ì¶œ ë° ì‘ë‹µ ë°˜í™˜

**í•¨ìˆ˜ë³„ ìƒì„¸:**
- `__init__()`: ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
  - LlmClient ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  - ë¡œê¹… ì„¤ì •

- `execute(messages)`: ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
  - **ì…ë ¥**: List[Message] (Pydantic ëª¨ë¸)
  - **ì²˜ë¦¬**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€, í¬ë§· ë³€í™˜
  - **ì¶œë ¥**: OpenAI API ì‘ë‹µ ê°ì²´
  - **ì˜ˆì™¸**: LLM í˜¸ì¶œ ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ì „íŒŒ

### 4. util/llmClient.py - OpenAI API í´ë¼ì´ì–¸íŠ¸

```python
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

class LlmClient():
    def __init__(self):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o-mini"  # ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸

    def call_llm(self, messages: list):
        """OpenAI API í˜¸ì¶œ"""
        return self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.5  # ì ë‹¹í•œ ì°½ì˜ì„±
        )
```

**ì£¼ìš” ê¸°ëŠ¥:**
- OpenAI API í´ë¼ì´ì–¸íŠ¸ ë˜í¼
- í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë”©
- GPT-4o-mini ëª¨ë¸ ì‚¬ìš© (ë¹„ìš© íš¨ìœ¨ì„±)

**í•¨ìˆ˜ë³„ ìƒì„¸:**
- `__init__()`: í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
  - í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë”©
  - OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
  - ê¸°ë³¸ ëª¨ë¸ ì„¤ì • (gpt-4o-mini)

- `call_llm(messages)`: LLM API í˜¸ì¶œ
  - **ì…ë ¥**: List[Dict] (ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸)
  - **ì„¤ì •**: temperature=0.5 (ê· í˜• ì¡íŒ ì‘ë‹µ)
  - **ì¶œë ¥**: OpenAI ChatCompletion ê°ì²´

## ğŸ”„ ì‹œìŠ¤í…œ í†µí•©

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

#### restart_all.shì—ì„œ ì„¤ì •
```bash
export LLM_FALLBACK_URL="${LLM_FALLBACK_URL:-http://svc7998:7998/agent}"
```

#### llm_agent-mainì—ì„œ ì°¸ì¡°
```python
# config/settings.py
llm_fallback_url: str = "http://svc7998:7998/agent"
```

### 2. Docker ë„¤íŠ¸ì›Œí¬ í†µí•©

#### docker-compose.yml ì„¤ì •
```yaml
svc7998:
  build:
    context: ./ai_modules/tool_dumb-main
    dockerfile: Dockerfile
  container_name: svc7998
  environment:
    - PORT=7998
    - TZ=Asia/Seoul
  ports:
    - "7998:7998"
  restart: unless-stopped
```

### 3. Nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ

#### ë¼ìš°íŒ… ì„¤ì •
```nginx
location ^~ /api/fallback/ {
    proxy_pass http://svc7998:7998/;
}
```

### 4. ë©”ì¸ ì‹œìŠ¤í…œì—ì„œ í˜¸ì¶œ

#### í´ë°± í˜¸ì¶œ ì˜ˆì‹œ (llm_agent-main)
```python
# handlers/base_handler.pyì˜ get_fallback_message() êµ¬í˜„ì—ì„œ
async def call_fallback_service(user_message: str):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://svc7998:7998/agent",
            json={
                "messages": [
                    {"role": "user", "content": user_message}
                ]
            }
        )
        return response.json()["message"]
```

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### 1. ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€

#### ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
```python
# service/coreService.pyì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
system_prompt = """
ë‹¹ì‹ ì€ ì „ë¶ëŒ€í•™êµ AI ë©˜í† ë§ ì‹œìŠ¤í…œì˜ í´ë°± ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

# ìƒˆë¡œìš´ ì§€ì‹œì‚¬í•­ ì¶”ê°€
4. íŠ¹ì • ë„ë©”ì¸ ì§ˆë¬¸: ì»´í“¨í„°ê³µí•™ê³¼ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ë” ìƒì„¸íˆ ë‹µë³€í•˜ì„¸ìš”.
5. ì°¸ê³  ë§í¬ ì œê³µ: ê°€ëŠ¥í•œ ê²½ìš° ì „ë¶ëŒ€ ê³µì‹ ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ì œê³µí•˜ì„¸ìš”.
"""
```

#### ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
```python
# controller/controller.pyì— ì¶”ê°€
@router.get("/status")
async def get_status():
    """ì„œë¹„ìŠ¤ ìƒíƒœ ë° í†µê³„ ì •ë³´"""
    return {
        "uptime": get_uptime(),
        "requests_processed": request_counter,
        "model": "gpt-4o-mini",
        "last_request": last_request_time
    }
```

### 2. ëª¨ë¸ ë³€ê²½

#### GPT ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ
```python
# util/llmClient.pyì—ì„œ ëª¨ë¸ ë³€ê²½
class LlmClient():
    def __init__(self):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4"  # ë” ê°•ë ¥í•œ ëª¨ë¸ë¡œ ë³€ê²½
        # ë˜ëŠ”
        self.model = "gpt-3.5-turbo"  # ë” ê²½ì œì ì¸ ëª¨ë¸ë¡œ ë³€ê²½
```

### 3. ë¡œê¹… ê°•í™”

#### ìƒì„¸ ë¡œê¹… ì¶”ê°€
```python
# ê° íŒŒì¼ì— ë¡œê¹… ì„¤ì • ì¶”ê°€
import logging

# ë¡œê·¸ í¬ë§· ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/fallback.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

### 4. ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ 

#### ë” êµ¬ì²´ì ì¸ ì˜ˆì™¸ ì²˜ë¦¬
```python
# service/coreService.pyì—ì„œ
async def execute(self, messages: List):
    try:
        response = self.llmClient.call_llm(request_messages)
        return response
    except openai.APIError as e:
        logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=503, detail="AI ì„œë¹„ìŠ¤ ì¼ì‹œ ì´ìš© ë¶ˆê°€")
    except openai.RateLimitError as e:
        logger.error(f"API ìš”ì²­ í•œë„ ì´ˆê³¼: {e}")
        raise HTTPException(status_code=429, detail="ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜")
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### 1. ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
docker logs svc7998 -f

# íŠ¹ì • ë¡œê·¸ ë ˆë²¨ í™•ì¸
docker logs svc7998 2>&1 | grep "ERROR"

# ìµœê·¼ 100ì¤„ ë¡œê·¸
docker logs svc7998 --tail 100
```

### 2. í—¬ìŠ¤ ì²´í¬

```bash
# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
curl http://localhost:7998/health

# API í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:7998/agent \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
    ]
  }'
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### Docker ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
```bash
# CPU, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats svc7998

# ì»¨í…Œì´ë„ˆ ìƒì„¸ ì •ë³´
docker inspect svc7998
```

#### API ì‘ë‹µ ì‹œê°„ ì¸¡ì •
```bash
# curlë¡œ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
curl -w "ì‘ë‹µì‹œê°„: %{time_total}ì´ˆ\n" \
  -X POST http://localhost:7998/agent \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "í…ŒìŠ¤íŠ¸"}]}'
```

## ğŸš€ ë°°í¬ ë° ìš´ì˜

### 1. ìš´ì˜ í™˜ê²½ ì„¤ì •

#### í™˜ê²½ ë³€ìˆ˜ (ìš´ì˜)
```bash
# .env.production
OPENAI_API_KEY=prod_api_key_here
LOG_LEVEL=INFO
ENVIRONMENT=production
```

#### Docker ìš´ì˜ ë°°í¬
```bash
# ìš´ì˜ìš© ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t tool-dumb-main:prod .

# ìš´ì˜ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name svc7998-prod \
  --restart unless-stopped \
  -p 7998:7998 \
  -e PORT=7998 \
  -e OPENAI_API_KEY=${OPENAI_API_KEY} \
  -e LOG_LEVEL=INFO \
  tool-dumb-main:prod
```

### 2. ë°±ì—… ë° ë³µêµ¬

#### ì„¤ì • ë°±ì—…
```bash
# í™˜ê²½ ì„¤ì • ë°±ì—…
cp .env .env.backup.$(date +%Y%m%d)

# Docker ì´ë¯¸ì§€ ë°±ì—…
docker save tool-dumb-main:prod > tool-dumb-main-backup.tar
```
