import uvicorn
from fastapi import FastAPI
import random
import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse,HTMLResponse
from types import SimpleNamespace
from contextlib import asynccontextmanager
from pydantic import BaseModel

from dotenv import load_dotenv
import os
from open_ai import initialize_openai_client, query_expansion
from aov import build_prereq_postreq, visualize_and_sort_department_graphs
from dense_retriver import DenseRetriever, classRetriever
from utils import update_json_with_index, read_json
from db.db_search import DatabaseHandler
from pathlib import Path
import logging
import time
from debug_utils import (
    CurriculumDebugger,
    performance_monitor,
    validate_query_data,
    validate_retrieval_result
)

# ì „ì—­ ë³€ìˆ˜ (FastAPI startup ì´ë²¤íŠ¸ì—ì„œ ì´ˆê¸°í™”)
global_args = None
client = None
db_handler = None
debugger = None

HERE        = Path(__file__).resolve().parent
DATA_DIR    = HERE / "data"
PROMPT_DIR  = HERE / "prompt"
RESULT_DIR  = HERE / "result"
CHATBOT_DIR = HERE / "chatbot_result"

load_dotenv()  # .env íŒŒì¼ ìë™ ë¡œë“œ

db_host = os.getenv("DB_HOST")
db_password = os.getenv("DB_PASSWORD")
openai_api_key = os.getenv("OPENAI_API_KEY")

# ë¡œê·¸ë¥¼ íŒŒì¼ì— ì €ì¥
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='result.log',  # ë¡œê·¸ë¥¼ íŒŒì¼ì— ì €ì¥
    filemode='w'  # 'w': ë®ì–´ì“°ê¸°, 'a': ì´ì–´ì“°ê¸°
)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ï¿½ Seed ê³ ì • (ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥)
def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)

set_seed()

@asynccontextmanager
async def lifespan(app: FastAPI):
    global global_args, client, db_handler, debugger

    # ë””ë²„ê±° ì´ˆê¸°í™”
    debugger = CurriculumDebugger(log_level="INFO", enable_console=True, enable_file=True)
    debugger.logger.info("ğŸš€ Curriculum ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
    
    CHATBOT_DIR.mkdir(exist_ok=True)
    RESULT_DIR .mkdir(exist_ok=True)
    # ì„¤ì •ê°’ì„ argparse ëŒ€ì‹  SimpleNamespaceë¡œ ì„¤ì • (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    global_args = SimpleNamespace(
        openai_api_key   = openai_api_key,
        query_prompt_path= str(PROMPT_DIR / "query_exp_once.txt"),
        department_path  = str(DATA_DIR   / "depart_info.json"),
        save_path        = str(RESULT_DIR),
        save_path_txt    = str(CHATBOT_DIR),
        goal_index_path  = str(RESULT_DIR / "goal_Dataset.pkl"),
        class_index_path = str(RESULT_DIR / "class_Dataset.pkl"),
        top_k=1,
        reranker=False,
        query_exp=False,
        department_y=False,
    )
    
    # DB ì„¤ì • ë° ì—°ê²°
    debugger.logger.info("ğŸ”— OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
    client = initialize_openai_client(global_args.openai_api_key)
    debugger.logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    debugger.logger.info("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
    db_handler = DatabaseHandler(
        host=db_host,
        port=3311,
        user="root",
        password=db_password,
        database="nll",
        charset="utf8mb4"
    )
    db_handler.connect()
    debugger.logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")

    debugger.logger.info("ğŸ¯ FastAPI Startup: ëª¨ë“  ì´ˆê¸°í™” ì™„ë£Œ")

    yield

    debugger.logger.info("ğŸ›‘ ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")
    db_handler.close()
    debugger.logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ ì™„ë£Œ")

app = FastAPI(title="Curriculum Recommendation API", lifespan=lifespan)

# CORS for browser/ proxy preflight
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def recursive_top1_selection(client, db_handler, query_embedding, query, selected_dept_list,  
                             class_retriever, graph_path, required_dept_count, gt_department, 
                             already_selected_classes=None, depth=0):

    # ğŸ”¥ ì´ë¯¸ ì„ íƒëœ ê³¼ëª© ë¦¬ìŠ¤íŠ¸ê°€ Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
    if already_selected_classes is None or not isinstance(already_selected_classes, list):
        already_selected_classes = []
    
    # ğŸ”¥ ë°©ë¬¸í•œ ë…¸ë“œ ê°œìˆ˜ í™•ì¸
    visited_ids = {c.get("class_id") for c in already_selected_classes}
    visited_count = len(visited_ids)

    logger.info(f"í˜„ì¬ ë°©ë¬¸í•œ ë…¸ë“œ ê°œìˆ˜: {visited_count}")

    # âœ… ì¢…ë£Œ ì¡°ê±´ ì²´í¬
    if visited_count >= required_dept_count:
        print(f'current depth: {depth}')
        logger.info(f"ì¢…ë£Œ ì¡°ê±´ ë„ë‹¬: ì„ íƒëœ í›„ë³´ {visited_count}ê°œ (ëª©í‘œ: {required_dept_count})")
        
        # ğŸ”¥ ìµœì¢… ê·¸ë˜í”„ ìƒì„± (ì´ë¯¸ ë°©ë¬¸í•œ ë…¸ë“œë§Œ í¬í•¨)
        G, visited_ids = build_prereq_postreq(class_retriever, already_selected_classes, db_handler, 
                                              query_embedding, logger=logger, existing_visited_ids=visited_ids)
        return G

    # âœ… í›„ë³´ ê²€ìƒ‰
    candidate_dict = class_retriever.retrieve_by_department(query_embedding, selected_dept_list, top_k=1, visited_class_ids=visited_ids)
    logger.info(f"Candidate dict retrieved: {candidate_dict}")
    print("DEBUG â–¶ candidate_dict:", candidate_dict)

    candidate_list = []
    for dept_name, dept_results in candidate_dict.items():
        if isinstance(dept_results, list) and dept_results:
            candidate_list.extend(dept_results)

    # âœ… ë§Œì•½ í›„ë³´ê°€ ì—†ë‹¤ë©´ ì¢…ë£Œ
    if not candidate_list:
        logger.info("í›„ë³´ ì—†ìŒ. ì¬ê·€ ì¢…ë£Œ.")
        G, visited_ids = build_prereq_postreq(class_retriever, already_selected_classes, db_handler, 
                                              query_embedding, logger=logger, existing_visited_ids=visited_ids)
        return G

    # âœ… ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ê³¼ëª© ì„ íƒ (ì—¬ëŸ¬ ê°œ ê³ ë ¤)
    candidate_list = sorted(candidate_list, key=lambda x: x["score"], reverse=True)

    # âœ… ì—¬ëŸ¬ í•™ê³¼ì—ì„œ ê· í˜• ìˆê²Œ í›„ë³´ ì„ íƒ
    selected_candidates = []
    seen_departments = set()

    for candidate in candidate_list:
        if candidate["department_name"] not in seen_departments:
            selected_candidates.append(candidate)
            seen_departments.add(candidate["department_name"])

        # âœ… ì˜ˆ: ìµœëŒ€ 2ê°œì˜ í•™ê³¼ì—ì„œ ì„ íƒ
        if len(selected_candidates) >= 2:
            break

    print(f"ğŸ”¹ ì„ íƒëœ í›„ë³´ë“¤: {selected_candidates}")

    # âœ… ì„ íƒëœ ê°•ì˜ë“¤ì„ ì¶”ê°€
    for candidate in selected_candidates:
        logger.info(f"candidate: {candidate}")
        candidate_id = candidate.get("class_id")

        if candidate_id in visited_ids:
            logger.info(f"Candidate {candidate_id} ì´ë¯¸ ì„ íƒë¨. ê±´ë„ˆëœ€.")
            continue

        already_selected_classes.append(candidate)
        logger.info(f"í›„ë³´ ì¶”ê°€: {candidate_id} (ì´ {len(already_selected_classes)}ê°œ)")
        logger.info(f'alreay_selected_classes: {already_selected_classes}')

    # ğŸ”¥ **ë§¤ë²ˆ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸**
    G, visited_ids = build_prereq_postreq(class_retriever, already_selected_classes, db_handler, query_embedding, logger=logger, existing_visited_ids=visited_ids)

    # âœ… ì¬ê·€ í˜¸ì¶œ (ë¦¬ìŠ¤íŠ¸ ìœ ì§€)
    return recursive_top1_selection(client, db_handler, query_embedding, query, 
                                    selected_dept_list, class_retriever, graph_path, required_dept_count, gt_department,
                                    already_selected_classes, depth+1)

query_counter = 0

@performance_monitor("full_query_processing")
def process_query(query, args, client, db_handler, idx=0):
    global query_counter, debugger
    query_counter += 1  # ì¿¼ë¦¬ ìš”ì²­ë§ˆë‹¤ ì¹´ìš´í„° ì¦ê°€

    # ì¿¼ë¦¬ ì‹œì‘ ë¡œê¹…
    debugger.log_query_start(query, query_counter)

    # ì…ë ¥ ë°ì´í„° ê²€ì¦
    validation_issues = validate_query_data(query, getattr(args, 'required_dept_count', 30))
    if validation_issues:
        debugger.log_data_validation("ì…ë ¥ ì¿¼ë¦¬", query, validation_issues)
        # ê²½ê³ ë§Œ í•˜ê³  ê³„ì† ì§„í–‰
    else:
        debugger.log_data_validation("ì…ë ¥ ì¿¼ë¦¬", query, [])

    # 1) í•™ê³¼Â·ê°•ì¢Œ ì¸ë±ìŠ¤ ì¤€ë¹„
    debugger.logger.info("ğŸ“š í•™ê³¼Â·ê°•ì¢Œ ì¸ë±ìŠ¤ ì„ë² ë”© ì‹œì‘")
    dept_retriever = DenseRetriever(client, args)
    dept_retriever.doc_embedding()
    class_retriever = classRetriever(client, args)
    class_retriever.doc_embedding()
    debugger.logger.info("âœ… ê°•ì˜Â·í•™ê³¼ ì¸ë±ìŠ¤ ì„ë² ë”© ì™„ë£Œ")

    # 2) ì¿¼ë¦¬ í™•ì¥ & ì„ë² ë”©
    original_query = query
    debugger.logger.info("ğŸ”„ ì¿¼ë¦¬ í™•ì¥ ì‹œì‘")
    query_info = query_expansion(client, query, args.query_prompt_path)
    debugger.log_query_expansion(original_query, query_info)

    debugger.logger.info("ğŸ§  ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘")
    query_embedding = dept_retriever.query_embedding(query_info)
    debugger.logger.info("âœ… ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

    # 3) ê´€ë ¨ í•™ê³¼ top-k
    try:
        debugger.logger.info("ğŸ« ê´€ë ¨ í•™ê³¼ ê²€ìƒ‰ ì‹œì‘")
        selected_depart_list = dept_retriever.retrieve(query_embedding)

        # ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦
        dept_issues = validate_retrieval_result(selected_depart_list, "department")
        debugger.log_data_validation("í•™ê³¼ ê²€ìƒ‰ ê²°ê³¼", selected_depart_list, dept_issues)

        debugger.log_department_selection(selected_depart_list)
    except Exception as e:
        debugger.log_error(e, "í•™ê³¼ ê²€ìƒ‰")
        raise

    # 4) ì‹œê°í™” ì €ì¥ ê²½ë¡œ ì„¤ì • (args.save_path_txt ê¸°ë°˜)
    graph_dir  = args.save_path_txt
    graph_name = f"recommendations_query{'_exp' if args.query_exp else '_no_exp'}_similar_top{args.top_k}"
    graph_path = os.path.join(graph_dir, graph_name)
    os.makedirs(graph_path, exist_ok=True)
    gt_department = "result_department_top1"
    logger.info(f"ê·¸ë˜í”„ ì €ì¥ ê²½ë¡œ: {graph_path}")

    # 5) ì¬ê·€ì ìœ¼ë¡œ í›„ë³´ í´ë˜ìŠ¤ ì„ íƒ í›„ ì „ì œ/í›„ì œ ê·¸ë˜í”„ ìƒì„±
    debugger.logger.info("ğŸ”„ ì¬ê·€ì  ê°•ì˜ ì„ íƒ ë° ê·¸ë˜í”„ ìƒì„± ì‹œì‘")
    start_time = time.time()

    G = recursive_top1_selection(
        client,
        db_handler,
        query_embedding,
        query,
        selected_depart_list,
        class_retriever,
        graph_path,
        required_dept_count=args.required_dept_count,
        gt_department=gt_department
    )

    graph_time = time.time() - start_time
    debugger.log_performance("ê·¸ë˜í”„ ìƒì„±", graph_time)

    # 6) ê·¸ë˜í”„ ì‹œê°í™” ë° ì •ë ¬
    debugger.logger.info("ğŸ¨ ê·¸ë˜í”„ ì‹œê°í™” ë° ì •ë ¬ ì‹œì‘")
    start_time = time.time()

    all_results_json = visualize_and_sort_department_graphs(G, graph_path, idx, gt_department)

    viz_time = time.time() - start_time
    debugger.log_performance("ì‹œê°í™” ë° ì •ë ¬", viz_time)

    # 7) ìµœì¢… ì¶”ì²œ ê°•ì¢Œ ëª©ë¡ìœ¼ë¡œ ë³€í™˜
    flat_nodes = []
    for dept_name, dept_data in all_results_json.items():
        # dept_dataëŠ” {"nodes": [...], "edges": [...]}
        flat_nodes.extend(dept_data.get("nodes", []))

    recommended = [
        {
            "class_id": node["course_id"],      # all_results_json uses "course_id"
            "name":     node["course_name"],
            "department": node["department"],
            "score":    node.get("score", None)  # scoreëŠ” ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ get()
        }
        for node in flat_nodes
    ]

    # ìµœì¢… ê²°ê³¼ ë¡œê¹…
    total_time = time.time() - start_time if 'start_time' in locals() else 0
    debugger.log_final_result(all_results_json, total_time)

    return {
        "expanded_query": query_info,
        "all_results_json": all_results_json
    }

class QueryRequest(BaseModel):
    query: str
    required_dept_count: int = 30  # ê¸°ë³¸ê°’ 30, í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì›í•˜ëŠ” ê°’ì„ ì „ë‹¬í•  ìˆ˜ ìˆìŒ


@app.get("/chat")
def chat_get():
    return {"message": "ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” POST ë°©ì‹ìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. POST ìš”ì²­ì„ ë³´ë‚´ì„¸ìš”."}

@app.post("/chat")
def process_query_endpoint(request: QueryRequest):
    global global_args, client, db_handler, debugger

    try:
        debugger.logger.info(f"ğŸ“¨ API ìš”ì²­ ìˆ˜ì‹ : '{request.query[:100]}{'...' if len(request.query) > 100 else ''}' (ìš”êµ¬ ê³¼ëª© ìˆ˜: {request.required_dept_count})")

        global_args.required_dept_count = request.required_dept_count
        result = process_query(request.query, global_args, client, db_handler)
        # â”€â”€ ì—¬ê¸°ì„œ ì–¸íŒ¨í‚¹ â”€â”€
        expanded = result["expanded_query"]
        all_json = result["all_results_json"]
        
        lines = [f"{expanded}\n"]

        # ğŸ“ all_results_json í’€ì–´ì„œ
        for dept_name, dept_data in all_json.items():
            lines.append(f"=== {dept_name} ===")
            for node in dept_data.get("nodes", []):
                # ì›í•˜ëŠ” í•„ë“œë§Œ ë½‘ì•„ì„œ, ì¤‘ê´„í˜¸ ì—†ì´
                course = node.get("course_name", "")
                grade  = node.get("student_grade", "")
                sem    = node.get("semester", "")
                prereq = node.get("prerequisites", "")
                lines.append(
                    f"ê°•ì¢Œëª…: {course}\n"
                    f"{grade}í•™ë…„ {sem}í•™ê¸°\n"
                    f"ì„ ìˆ˜ê³¼ëª©: {prereq}"
                )
            lines.append("")  # í•œ ê³¼ëª©êµ° ëë‚˜ê³  ë¹ˆ ì¤„

    
        message_text = "\n".join(lines).strip()

        debugger.logger.info(f"âœ… API ì‘ë‹µ ì™„ë£Œ: {len(message_text)}ì ì‘ë‹µ ìƒì„±")

        return JSONResponse(
            status_code=200,
            content={"message": message_text}
        )

    except Exception as e:
        debugger.log_error(e, "API ìš”ì²­ ì²˜ë¦¬")
        return JSONResponse(
            status_code=500,
            content={"message": {"error": str(e)}}
        )


if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=7996)