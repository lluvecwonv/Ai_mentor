
# ğŸ¤– JBNU AI Mentor - ë¡œì»¬ ì‹¤í–‰ ê°€ì´ë“œ

docker logs --tail 15000 llm-agent | cat
docker logs llm-agent > ~/llm-agent.log 2>&1
docker logs --tail 400 svc7997 | cat
docker logs  --tail 400 svc7999 | cat
docker logs  --tail 400 department-mapping | cat


llm-agent (í¬íŠ¸ 8001): LLM ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°/ê²Œì´íŠ¸ì›¨ì´. í”„ë¡ íŠ¸ ìš”ì²­ì„ ë°›ì•„ ì ì ˆí•œ ì„œë¸Œ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…. ë””ë ‰í„°ë¦¬: ai_modules/llm_agent-main
svc7997 (í¬íŠ¸ 7997): Vector-Search-Agent (FAISS ìœ ì‚¬ë„ ê²€ìƒ‰). ê³¼ëª©/í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰. ì—”ë“œí¬ì¸íŠ¸: /search
svc7999 (í¬íŠ¸ 7999): DB-Agent (SQL ì§ˆì˜). êµ¬ì¡°í™”ëœ DB ì§ˆì˜ ì²˜ë¦¬. ì—”ë“œí¬ì¸íŠ¸: /agent
department-mapping (í¬íŠ¸ 8000): Department-Agent. í•™ê³¼/ì „ê³µ ëª…ì¹­ ë§¤í•‘Â·ì •ê·œí™”. ì—”ë“œí¬ì¸íŠ¸: /agent
ì¶”ê°€ë¡œ í”„ë¡œì íŠ¸ì— ìì£¼ ë³´ì´ëŠ” ê²ƒ:
svc7996: Curriculum-Agent (ì»¤ë¦¬í˜ëŸ¼ ìƒì„±/í”Œë˜ë‹, /chat)
svc7998: LLM Fallback/General-Agent (ì¼ë°˜ LLM ì²˜ë¦¬, /agent)



    - cd /home/dbs0510/
  AiMentor_edit
      - ./restart_all.sh --rebuild



## Frontend (port: 5173)


code --list-extensions | grep -i preview

curl -s http://localhost:3000 > /dev/null && echo "âœ… Frontend service is running on port 3000" || echo "âŒ Frontend service not responding"

### ì„¤ì¹˜
```bash
cd open-webui
npm install
```

### ì‹¤í–‰

```bash
cd open-webui
npm run dev
```

---

## Backend (port: 8080)

### ì„¤ì¹˜

```bash
conda create --name open-webui python=3.11
conda activate open-webui
cd open-webui/backend/
pip install -r requirements.txt -U
sh dev.sh
```

### ì‹¤í–‰

```bash
conda activate open-webui
cd open-webui/backend/
sh dev.sh
```

---

## Pipeline (port: 9099)

### ì„¤ì¹˜

```bash
conda activate open-webui
cd pipelines/
pip install -r requirements.txt
sudo apt update
sudo apt install dos2unix
sed -i 's/\r$//' start.sh
./start.sh
```x

### ì‹¤í–‰

```bash
conda activate open-webui
cd pipelines/
./start.sh
```

---

## Open WebUI â†” Pipelines ì—°ë™

1. **Admin Panel** > **Settings** > **Connections**
2. **OpenAI API ì—°ê²° ê´€ë¦¬** í´ë¦­

   * URL: `http://localhost:9099`
   * Key: `0p3n-w3bu!`

---

## Pipeline ì¶”ê°€

1. **Admin Panel** > **Settings** > **Pipeline**
2. **ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸** â†’ íŒŒì¼ ì„ íƒ â†’ ì˜†ì˜ ë²„íŠ¼ í´ë¦­ â†’ **ì €ì¥**

---

## llm-agent-main Uvicorn (port: 8001)

```bash
conda activate open-webui
cd ai_modules/llm_agent-main/
uvicorn main:app --host 0.0.0.0 --port 8001
```
## faiss_search-main (port: 7996)

```bash
conda activate open-webui
cd ai_modules/curriculum-main/
python main.py
```
## faiss_search-main (port: 7997)

```bash
conda activate open-webui
cd ai_modules/faiss_search-main/
python main.py
```
## tool_dumb-main (port: 7998)

```bash
conda activate open-webui
cd ai_modules/tool_dumb-main/
python main.py
```
## tool_sql-main (port: 7999)

```bash
conda activate open-webui
cd ai_modules/tool_sql-main/
python main.py
```

> âš ï¸ **ì£¼ì˜:** `ai_modules` ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ì€ `.env` íŒŒì¼ì„ í†µí•´ `OPENAI_API_KEY`, `DB_HOST`, `DB_PASSWORD` ë“±ì˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë°˜ë“œì‹œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.


---
## ì œëª© & íƒœê·¸ ìƒì„± ì„¤ì • External Task Model, Local Task Model
![alt text](image.png)
ê³µê°œ ì„¤ì •
![alt text](image-1.png)
External Task Model, Local Task Model: OpenAI Pipeline
ì•ˆí•˜ë©´ llm-agent-mainìœ¼ë¡œ ë³´ë‚´ì ¸ì„œ ì˜¤ë¥˜ ëœ¸
---
## PDF ì‘ë‹µ ì²˜ë¦¬

ì±„íŒ… ë©”ì‹œì§€ `content` ì— Markdown ë§í¬ë¡œ PDF ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì œê³µí•˜ì„¸ìš”:

```python
"content" : f"[PDF ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬]({pdf_url})"
```


---
## Stream=Falseì¼ ë•Œ

```jsonc
// stream=False ì‘ë‹µ ì˜ˆì‹œ
{
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ì•ˆë…•í•˜ì„¸ìš”! ìš”ì²­í•˜ì‹  ë‚´ìš©ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.\n\n[PDF ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ](https://example.com/report.pdf)",
        "attachments": [
          {
            "type": "image",
            "url": "https://example.com/image.png",
            "name": "image.png"
          },
          {
            "type": "file",
            "url": "https://example.com/report.pdf",
            "name": "report.pdf"
          }
        ]
      },
      "finish_reason": "stop"
    }
  ]
}
```

## Stream=Trueì¼ ë•Œ ì‘ë‹µ ì²˜ë¦¬ (Server-Sent Events í˜•ì‹)

* **í—¤ë”**

  ```
  HTTP/1.1 200 OK
  Content-Type: text/event-stream
  Connection: keep-alive
  ```

* **í† í° ë¸íƒ€ ì´ë²¤íŠ¸ ì˜ˆì‹œ**

  ```http
  data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk",
         "choices":[{"delta":{"content":"ë§›"},
                     "index":0,
                     "finish_reason":null}]}

  data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk",
         "choices":[{"delta":{"content":"ìˆ"},
                     "index":0,
                     "finish_reason":null}]}

  data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk",
         "choices":[{"delta":{"content":"ë‹¤"},
                     "index":0,
                     "finish_reason":null}]}

  data: [DONE]
  ```

* **FastAPI ì œë„ˆë ˆì´í„° ì˜ˆì‹œ**

  ```python
  async def event_generator():
      for token in ["ë§›", "ìˆ", "ë‹¤"]:
          chunk = {
              "id": "chatcmpl-xxx",
              "object": "chat.completion.chunk",
              "choices": [{
                  "delta": {"content": token},
                  "index": 0,
                  "finish_reason": None
              }]
          }
          yield f"data: {json.dumps(chunk)}\n\n"
          await asyncio.sleep(0.1)

      # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
      yield "data: [DONE]\n\n"
  ```

  ```python
  return StreamingResponse(
      event_generator(),
      media_type="text/event-stream"
  )
  ```





---

## ğŸ“„ ê¸°ëŠ¥ í•­ëª© (ì§„í–‰ ì¤‘)

* [x] PDF ì²˜ë¦¬
* [x] ëŒ€í™” stream ì²˜ë¦¬
* [ ] ëŒ€í™” ê¸°ë¡ ê´€ë¦¬

---

## ğŸ§¼ ì°¸ê³ ì‚¬í•­

* ëª¨ë“  ì„œë²„ëŠ” `.env` í™˜ê²½íŒŒì¼ì„ í†µí•´ ë¯¼ê°ì •ë³´(API KEY ë“±)ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
# Ai_mentor
